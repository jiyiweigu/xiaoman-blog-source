[{"title":"Bios与操作系统","url":"/2024/07/10/Bios/","content":"操作系统很难定义，[7]但有人称其为“为用户及其应用程序管理计算机资源的软件层”。[8]操作系统包括始终在运行的软件（称为内核），但也可以包含其他软件。[7] [9]可以在计算机上运行的另外两种程序是系统程序（与操作系统相关，但可能不是内核的一部分）和应用程序（所有其他软件）。[9]\n操作系统有三个主要目的：[10]\n操作系统在不同的应用程序之间分配资源，决定它们何时获得中央处理器(CPU) 时间或内存空间。[10]在现代个人计算机上，用户通常希望同时运行多个应用程序。为了确保一个程序不会独占计算机有限的硬件资源，操作系统会为每个应用程序提供一定比例的资源，无论是时间（CPU）还是空间（内存）。[11] [12]操作系统还必须将应用程序彼此隔离，以保护它们免受另一个应用程序代码的错误和安全漏洞的影响，但要允许不同应用程序之间进行通信。\n操作系统提供了一个接口，该接口抽象了访问硬件细节（例如物理内存）的细节，以使程序员的工作更加轻松。 虚拟化还使操作系统能够掩盖有限的硬件资源；例如，虚拟内存可以为程序提供几乎无限内存的错觉，而这超出了计算机的实际内存。[\n操作系统提供通用服务，例如用于访问网络和磁盘设备的接口。这使应用程序可以在不同的硬件上运行而无需重写。操作系统中包含的服务各不相同，而这些功能占大多数操作系统代码的绝大部分。\nBios\nBIOS 代表基本输入/输出系统，它是在计算机微处理器上运行的程序，用于在计算机开机时启动计算机。 BIOS 是在操作系统 (OS) 之前加载并在启动过程中执行初始化任务的固件。这些任务包括检查硬盘、RAM 和键盘等系统设备以及加载操作系统。 BIOS 还管理操作系统与打印机、鼠标、键盘和视频适配器等连接设备之间的数据流。\nBIOS是个程序，存储在BIOS芯片中，而现在的新式电脑用的基本都是UEFI启动，早期的过渡电脑用的都是EFI启动。其实EFI或UEFI的一部分也是存储在一个芯片中，由于它们在表面形式、基本功能上和BIOS差不多，所以习惯上我们也把存储EFI/UEFI的芯片叫做EFI/UEFI BIOS芯片，EFI/UEFI也叫做EFI/UEFI BIOS，但在实际上它们和BIOS根本是不一样的，所以最好还是把后面的“BIOS”尾巴去掉为好，下面就来具体谈一下BIOS、EFI和UEFI。\n!https://pic1.zhimg.com/80/v2-2031780e40860ff89222511597c900b4_720w.webp\nBIOS用于计算机硬件自检、CMOS设置、引导操作系统启动、提供硬件I/O、硬件中断等4项主要功能，因此BIOS程序可以分为若干模块，主要有Boot Block引导模块、CMOS设置模块、扩展配置数据（ESCD）模块、DMI收集硬件数据模块，其中引导模块直接负责执行BIOS程序本身入口、计算机基本硬件的检测和初始化，ESCD用于BIOS与OS交换硬件配置数据，DMI则充当了硬件管理工具和系统层之间接口的角色，通过DMI，用户可以直观地获得硬件的任何信息，CMOS设置模块就是实现对硬件信息进行设置，并保存在CMOS中，是除了启动初始化以外BIOS程序最常用的功能。\nBIOS本身是汇编语言代码，是在16位实模式下调用INT 13H中断执行的，由于x86-64是一个高度兼容的指令集，也为了迁就BIOS的16位实模式的运行环境，所以即使现在的CPU都已是64位，如果还是在BIOS启动（基本见于09年以前的主板），在开机时仍然都是在16位实模式下执行的。16位实模式直接能访问的内存只有1MB，就算你安了4G、8G或者16G还是32G内存，到了BIOS上一律只先认前1MB。在这1MB内存中，前640K称为基本内存，后面384K内存留给开机必要硬件和各类BIOS本身使用，了解了这些，下面谈一下BIOS启动计算机的具体过程。\n当按下电源开关时，电源就开始向主板和其他设备供电，这时电压还不稳定，在早期的南北桥主板上，由主板北桥向CPU发复位信号，对CPU初始化；稳定电压后复位信号便撤掉。而对于现在的单南桥主板，则由CPU自身调整稳定电压达到初始化的目的，当电压稳定后，CPU便在系统BIOS保留的内存地址处执行跳转BIOS起始处指令，开始执行POST自检。\n在POST自检中，BIOS只检查系统的必要核心硬件是否有问题，主要是CPU、640K基本内存、显卡是否正常，PS/2键盘控制器、系统时钟是否有错误等等。由于POST检查在显卡初始化以前，因此在这个阶段如发生错误，是无法在屏幕上显示的，不过主板上还有个报警扬声器，而且如果主板的8255外围可编程接口芯片没有损坏的话，POST报警声音一定是会出来的。可以根据报警声的不同大致判断错误所在，一般情况下，一声短“嘀”声基本代表正常启动，不同的错误则是不同的短“嘀”声和长“嘀”声组合。POST自检结束后，BIOS开始调用中断完成各种硬件初始化工作。\nEFI，是Extensible Firmware Interface的词头缩写，直译过来就是可扩展固件接口，它是用模块化、高级语言（主要是C语言）构建的一个小型化系统，它和BIOS一样，主要在启动过程中完成硬件初始化，但它是直接利用加载EFI驱动的方式，识别系统硬件并完成硬件初始化，彻底摒弃读各种中断执行。EFI驱动并不是直接面向CPU的代码，而是由EFI字节码编写成，EFI字节码是专用于EFI的虚拟机器指令，需要在EFI驱动运行环境DXE下解释运行，这样EFI既可以实现通配，又提供了良好的兼容。此外，EFI完全是32位或64位，摒弃16位实模式，在EFI中就可以实现处理器的最大寻址，因此可以在任何内存地址存放任何信息。另外，由于EFI的驱动开发非常简单，基于EFI的驱动模型原则上可以使EFI接触到所有硬件功能，在EFI上实现文件读写，网络浏览都是完全可能的。i，BIOS上的的CMOS设置程序在EFI上是作为一个个EFI程序来执行的，硬件设置是硬件设置程序、而启动管理则是另一个程序，保存CMOS又是另一个程序，虽然它们在形式的Shell上是在一起的。\nEFI在功能上完全等同于一个轻量化的OS（操作系统），但是EFI在制定时就定位到不足以成为专业OS的地位上，首先，它只是一个硬件和操作系统间的一个接口；其次，EFI不提供中断访问机制，EFI必须用轮询的方式检查并解释硬件，较OS下的驱动执行效率较低，最后，EFI只有简单的存储器管理机制，在段保护模式下只将存储器分段，所有程序都可以存取任何一段位置，不提供真实的保护服务。伴随着EFI，一种全新的GUID磁盘分区系统（GPT）被引入支持，传统MBR磁盘只能存在4个主分区，只有在创建主分区不足4个时，可以建立一个扩展分区，再在其上建立被系统识别的逻辑分区，逻辑分区也是有数量的，太多的逻辑分区会严重影响系统启动，MBR硬盘分区最大仅支持2T容量，对于现在的大容量硬盘来说也是浪费。GPT支持任意多的分区，每个分区大小原则上是无限制的，但实际上受到OS的规定限制不能做到无限，不过比MBR的2T限制是非常重要的进步。GPT的分区类型由GUID表唯一指定，基本不可能出现重复，其中的EFI系统分区可以被EFI存取，用来存取部分驱动和应用程序，虽然这原则上会使EFI系统分区变得不安全，但是一般这里放置的都是些“边缘”数据，即使其被破坏，一般也不会造成严重后果，而且也能够简单的恢复回来。\n当EFI发展到1.1的时候，英特尔决定把EFI公之于众，于是后续的2.0吸引了众多公司加入，EFI也不再属于英特尔，而是属于了Unified EFI Form的国际组织，EFI在2.0后也遂改称为UEFI，UEFI，其中的EFI和原来是一个意思，U则是Unified（一元化、统一）的缩写，所以UEFI的意思就是“统一的可扩展固件接口”，与前身EFI相比，UEFI主要有以下改进：\n首先，UEFI具有完整的图形驱动功能，之前的EFI虽然原则上加入了图形驱动，但为了保证EFI和BIOS的良好过渡，EFI多数还是一种类DOS界面（仍然是640480VGA分辨率），只支持PS/2键盘操作（极少数支持鼠标操作），不支持USB键盘和鼠标。到了UEFI，则是拥有了完整的图形驱动，无论是PS/2还是USB键盘和鼠标，UEFI一律是支持的，而且UEFI在显卡也支持GOP VBIOS的时候，显示的设置界面是显卡高分辨率按640480或1024*768显示，因此画面虽小但很清楚，但是这样会导致屏幕周围大片留黑，不过鱼和熊掌不可兼得，除非UEFI默认窗口大小也是最高分辨率。\n其次，UEFI具有一个独特的功能，安全启动，而EFI是没有安全启动的，安全启动（Secure Boot），实际上通俗的解释是叫做固件验证。开启UEFI的安全启动后，主板会根据TPM芯片（或者CPU内置的TPM）记录的硬件签名对各硬件判断，只有符合认证的硬件驱动才会被加载，而Win8以后的Windows则是在操作系统加载的过程中对硬件驱动继续查签名，符合Windows记录的硬件才能被Windows加载，这在一定程度上降低了启动型程序在操作系统启动前被预加载造成的风险，但是这也会造成系统安装变得垄断。\n无论EFI还是UEFI，都必须要有预加载环境、驱动执行环境、驱动程序等必要部分组成，为了支持部分旧设备（如在UEFI下挂载传统MBR硬盘，不支持UEFI启动的显卡在UEFI下仍然支持运行等），还需要一个CSM兼容性支持模块、EFI或UEFI都是仅支持GPT磁盘引导系统的，下面就具体谈一下EFI或UEFI启动计算机的过程。\n一般地，预加载环境和驱动执行环境是存储在UEFI（UEFI BIOS）芯片中的，当打开电源开关时，电脑的主要部件都开始有了供电，与BIOS不同的是，UEFI预加载环境首先开始执行，负责CPU和内存（是全部容量）的初始化工作，这里如出现重要问题，电脑即使有报警喇叭也不会响，因为UEFI没有去驱动8255发声，不过预加载环境只检查CPU和内存，如果这两个主要硬件出问题，屏幕没显示可以立即确定，另外一些主板会有提供LED提示，可根据CPU或内存亮灯大致判断故障。\nCPU和内存初始化成功后，驱动执行环境（DXE）载入，当DXE载入后，UEFI就具有了枚举并加载UEFI驱动程序的能力，在此阶段，UEFI会枚举搜索各个硬件的UEFI驱动并相继加载，完成硬件初始化工作，这相比BIOS的读中断加载速度会快的多，同样如加载显卡的UEFI驱动成功，电脑也会出现启动画面，硬件驱动全部加载完毕后，最后同BIOS一样，也得去启动操作系统。\n在启动操作系统的阶段，同样是根据启动记录的启动顺序，转到相应设备（仅限GPT设备，如果启动传统MBR设备，则需要打开CSM支持）的引导记录，引导操作系统并进入，这里需要注意的是，UEFI在检测到无任何操作系统启动设备时，会直接进入UEFI设置页面，而不是像BIOS那样黑屏显示相关信息。\n综上对BIOS和UEFI启动计算机过程的叙述，可以概括为：BIOS先要对CPU初始化，然后跳转到BIOS启动处进行POST自检，此过程如有严重错误，则电脑会用不同的报警声音提醒，接下来采用读中断的方式加载各种硬件，完成硬件初始化后进入操作系统启动过程；而UEFI则是运行预加载环境先直接初始化CPU和内存，CPU和内存若有问题则直接黑屏，其后启动PXE采用枚举方式搜索各种硬件并加载驱动，完成硬件初始化，之后同样进入操作系统启动过程。\n此外，BIOS是16位汇编语言程序，只能运行在16位实模式，可访问的内存只有1MB，而UEFI是32位或64位高级语言程序（C语言程序），突破实模式限制，可以达到要求的最大寻址。\n网吧：network booting 无盘系统\n无盘系统在设计时，负载均衡\n无盘系统靠着每次复位镜像的操作，每次开机都相当于做了一个系统还原，你qq号啥的都得重新输入，浏览记录啥的也不会保存，甚至一些游戏存档都不会留。流氓软件自然也没了。\n为啥更慢呢？为啥LOW帧更低呢？\n因为无硬盘，本来硬盘在电脑上，需要访问文件，CPU直接读取就行了。\n现在无盘系统变成了CPU访问文件——通过网卡发送请求，远端服务器接受请求-远端服务器发送数据。环节多了，延迟也变高了。自然就更慢了。\n可能觉得不到1秒，挺快的啊，但是把1毫秒当做一天来看，本来当天下午就能拿到的数据，现在要等几天，还觉得很快么？这种读取就在一些游戏上能造成细微卡顿。\n而且网吧的设备，有多少客户机会给上2.5G网口，大部分都是千兆网口，才100M/S的速度，2.5G最高也就250M/S的速度，而老一点的sata硬盘接口都是460M/S起步，M2接口配合现在的nvme固态，速度都是几千算的。不过这个不算主要原因，毕竟日常使用的时候，硬盘需要瞬间读取几百M的数据的情况并不是很多。主要还是延迟。尤其是几百几千个文件一起的时候，每个文件延迟一点，连起来那就多了。\n","categories":["计算机科学"],"tags":["计算机基础"]},{"title":"并发效率与多线程安全","url":"/2025/07/28/Concurrency/","content":"why you need to learn 并发与多线程?\n最直观的服务器处理多客户端访问的方式，就是为每个连接创建一个线程，或者更早期操作系统里甚至是一个进程。虽然线程比进程更轻量，切换成本也更低，但每连接一个线程在并发量大时仍会面临两个问题：\n线程创建/销毁有系统开销（malloc stack、context switch）。\n线程是稀缺资源，系统线程数有上限（如 Linux 默认 1024~65535）。\n所以，现代服务端架构更倾向于使用线程池来复用线程资源。线程池中每个线程从任务队列中取任务处理，避免了反复创建/销毁的开销。\n但线程池引入后面临另一个问题：I/O 阻塞会浪费线程资源。如果一个线程执行 read(socket) 被阻塞，而连接上没有数据，那么整个线程就“卡住”了，不能服务其他连接。\n为了避免这个问题，我们可以将 socket 设置为非阻塞模式。这样 read() 不会阻塞，而是立即返回，如果没有数据，就返回错误码 EAGAIN。线程就可以在用户空间“轮询”所有 socket。\n但这种方式会导致高 CPU 占用，尤其在连接数多时效率低。\n所以引入了 I/O 多路复用技术（如 select、poll、epoll），它允许一个线程通过一个系统调用（如 epoll_wait）同时监听多个连接的状态，一旦某些连接可读（或可写），内核就通知我们“这些连接准备好了”，我们再去 read()，避免了不必要的轮询。\n这就是像 Nginx、Redis、Node.js、netty 等高并发服务采用的基础模式：I/O 多路复用 + 非阻塞 I/O + 事件驱动模型。Reactor模式称为反应器模式或应答者模式，是基于事件驱动的设计模式，拥有一个或多个并发输入源，有一个服务处理器和多个请求处理器，服务处理器会同步的将输入的请求事件以多路复用的方式分发给相应的请求处理器。\n单 Reactor 单线程（Redis）\n所有 I/O + 业务逻辑都在一个线程中串行完成\n简单、性能好（但业务处理不能太重）\n单 Reactor 多线程（Nginx）\nReactor 只负责监听/调度，业务逻辑交给线程池处理\n主从 Reactor（Netty）\n主 Reactor 接收连接\n从 Reactor 负责具体数据读写\n分离 accept 和 IO 阶段，提高可扩展性\nProactor 是把任务交给操作系统 / runtime，让它完成后通知你处理结果\n当你需要让程序快速处理大量数据或高频率的任务时，并发和多线程能帮你充分发挥多核处理器的优势。这就像是给你的应用装上了多台引擎，使得任务处理更加迅速高效。掌握并发和多线程，可以让你更高效地使用系统资源，比如CPU和内存。这就像是你在分配资源时，能做到心中有数，不让系统资源闲置浪费。\n避免常见陷阱：竞争条件、死锁和资源匮乏等线程问题在并发编程中很常见。对这些问题的深入理解有助于开发人员编写避免此类陷阱的代码，从而开发出更强大、更可靠的软件。\n并发性（concurrency）是指程序、算法或问题的不同部分或单元可以在同一时间段内无序地在CPU上执行，但不会影响最终结果的能力。换句话说，并发性允许程序的不同部分同时进行，但它们的执行顺序可以是灵活的，只要最终的结果不受影响。\n并发Concurrent：无论上一个开始执行的任务是否完成，当前任务都可以开始执行 （也就是说，A B 顺序执行的话，A 一定会比 B 先完成，而并发执行则不一定。） 与可以一起执行的并行（parallel）相对的是不可以一起执行的串行（serial）\n从宏观方面来说，并发就是同时进行多种时间，实际上，这几种时间，并不是同时进行的，而是交替进行的，而由于CPU的运算速度非常的快，同一时间只有一个线程运行\n并行：则是真正意义上的同时进行多种事情。这种只可以在多核CPU的基础上完成。\n综上，并发与并行并不是互斥的概念，只是前者关注的是任务的抽象调度、后者关注的是任务的实际执行。而它们又是相关的\n在并发编程中，可见性、原子性和有序性是指导并发程序正确性的重要概念，与上述三大问题（竞态条件、死锁和内存同步）密切相关。\n可见性（Visibility）：\n可见性指的是一个线程对共享变量的修改是否能够被其他线程立即观察到。在多线程环境中，由于CPU缓存、编译器优化等因素，一个线程对共享数据的修改可能不会立即被其他线程所感知，造成不一致的结果。通过合适的同步机制（如互斥锁、原子操作、内存屏障等）可以确保对共享变量的修改对其他线程是可见的，从而保证了程序的正确性。\n原子性（Atomicity）：\n原子性是指一个操作要么完全执行，要么完全不执行，不存在中间状态。在并发环境中，多个线程可能同时访问同一个变量，当一个线程正在修改某个共享变量时，如果没有适当的同步机制，其他线程可能会读取到不一致的状态。使用原子操作或锁可以确保某个操作是原子的，即在执行过程中不会被中断或影响。\n原子性操作在并发编程中至关重要，它确保了一组操作的不可分割性和中间状态的不可见性。通过使用基本类型的原子操作、锁机制、原子变量和数据库事务等技术，我们可以有效地避免竞态条件，确保数据的一致性和正确性。\n有序性（Ordering）：\n有序性指的是程序中的操作在执行时保持其指令顺序，不会出现乱序执行的情况。在现代CPU中，由于指令重排优化，CPU 可能会重新排序指令以提高性能。对于并发编程来说，这可能会导致线程之间操作的执行顺序与代码编写的顺序不一致。使用内存屏障（Memory Barriers）或同步机制可以确保指令不会被重排，从而保持操作的有序性。\n关系：\n\n竞态条件通常与可见性和原子性有关。竞态条件可能发生在多个线程对共享资源的读写操作上，如果没有适当的同步机制来确保可见性和原子性，就可能导致不确定的结果。\n死锁可能与有序性有关，当多个线程按照不同的顺序获取锁或资源时，可能会产生死锁。合理地规划资源获取顺序可以避免死锁的发生。\n内存同步问题与可见性、原子性和有序性密切相关。解决内存同步问题需要确保对共享变量的修改对其他线程是可见的（可见性）、原子操作能够保证操作的完整性（原子性），以及指令执行的有序性。\n\n同步问题\n**线程同步（Thread Synchronization）**是指在多线程编程中，为了避免多个线程之间对共享资源的并发访问引发的问题（如竞争条件、死锁等），需要采取措施来协调线程的执行，以确保线程之间的操作按照期望的顺序进行。线程同步的目的是保证多个线程能够有序地访问共享资源，避免数据的不一致和错误。\n常见的线程同步机制包括：\n\n\n互斥锁（Mutex）： 互斥锁是一种最基本的线程同步机制，用于保护共享资源免受并发访问。一次只有一个线程能够持有互斥锁，其他线程必须等待直到锁被释放。这样可以防止多个线程同时修改同一资源。\n\n\n信号量（Semaphore）： 信号量是一种计数器，用于控制多个线程对共享资源的访问。它可以用来限制同时访问资源的线程数量，也可以用于线程间的通信。\n\n\n条件变量（Condition Variable）： 条件变量用于在线程之间传递信息，以及在某些条件满足时唤醒等待的线程。它通常和互斥锁一起使用，以实现更复杂的同步需求。\n\n\n读写锁（Read-Write Lock）： 读写锁允许多个线程同时读取共享资源，但只允许一个线程进行写操作。这可以提高读操作的并发性能。多个读线程可以同时获得锁，写线程会阻塞其他写线程和读线程。乐观锁\n\n\n原子操作（Atomic Operations）： 原子操作是不可中断的操作，可以在单个指令中完成。它们可以用于保护简单的共享数据结构，避免竞争条件。\n\n\n自旋锁： 自旋锁与互斥量类似，但它不使线程进入阻塞态；而是在获取锁之前一直占用CPU，处于忙等（自旋）状态。\n\n\n屏障（Barrier）： 屏障用于等待多个线程都达到某个点，然后再继续执行后续操作，常用于需要多个线程协同完成的任务。\n\n\n乐观锁：乐观锁是一种假设在大多数情况下不会发生冲突的锁。它不会在访问共享资源之前获取锁，而是在更新共享资源时进行检查。如果检测到其他线程已经更新了共享资源，那么当前线程可能需要重试或者采取其他措施来处理冲突。乐观锁通常用于多读的场景，可以提高吞吐量。\n\n\n悲观锁：悲观锁是一种假设在大多数情况下会发生冲突的锁。它在访问共享资源之前获取锁，并假定其他线程会干扰。悲观锁通常用于写入操作，以确保在写入共享资源时不会发生冲突。\n\n\n乐观锁的实现方式主要有两种：CAS机制和版本号机制。\n对于ABA问题，比较有效的方案是引入版本号，内存中的值每发生一次变化，版本号都+1；在进行CAS操作时，不仅比较内存中的值，也会比较版本号，只有当二者都没有变化时，CAS才能执行成功。Java中的AtomicStampedReference类便是使用版本号来解决ABA问题的。\n除了CAS，版本号机制也可以用来实现乐观锁。版本号机制的基本思路是在数据中增加一个字段version，表示该数据的版本号，每当数据被修改，版本号加1。当某个线程查询数据时，将该数据的版本号一起查出来；当该线程更新数据时，判断当前版本号与之前读取的版本号是否一致，如果一致才进行操作。\n临界区 对临界资源进行访问的那段代码称为临界区。\n为了互斥访问临界资源，每个进程在进入临界区之前，需要先进行检查。\n// entry section // critical section; // exit section\n\n同步与互斥 同步：多个进程因为合作产生的直接制约关系，使得进程有一定的先后执行关系。 互斥：多个进程在同一时刻只有一个进程能进入临界区。\n信号量 信号量（Semaphore）是一个整型变量，可以对其执行 down 和 up 操作，也就是常见的 P 和 V 操作。\n\ndown : 如果信号量大于 0 ，执行 -1 操作；如果信号量等于 0，进程睡眠，等待信号量大于 0； up ：对信号量执行 +1 操作，唤醒睡眠的进程让其完成 down 操作。 down 和 up 操作需要被设计成原语，不可分割，通常的做法是在执行这些操作的时候屏蔽中断。\n如果信号量的取值只能为 0 或者 1，那么就成为了 互斥量（Mutex） ，0 表示临界区已经加锁，1 表示临界区解锁。\n互斥锁\n1严格轮换法 加入锁变量 2 Peterson算法 进程0出临界区后进程1才会离开忙等\ntypedef int semaphore; semaphore mutex = 1; void P1() { down(&amp;mutex); // 临界区 up(&amp;mutex); }void P2() { down(&amp;mutex); // 临界区 up(&amp;mutex); } 使用信号量实现生产者-消费者\n问题描述：使用一个缓冲区来保存物品，只有缓冲区没有满，生产者才可以放入物品；只有缓冲区不为空，消费者才可以拿走物品。\n因为缓冲区属于临界资源，因此需要使用一个互斥量 mutex 来控制对缓冲区的互斥访问。\n为了同步生产者和消费者的行为，需要记录缓冲区中物品的数量。数量可以使用信号量来进行统计，这里需要使用两个信号量：empty 记录空缓冲区的数量，full 记录满缓冲区的数量。其中，empty 信号量是在生产者进程中使用，当 empty 不为 0 时，生产者才可以放入物品；full 信号量是在消费者进程中使用，当 full 信号量不为 0 时，消费者才可以取走物品。\n注意，不能先对缓冲区进行加锁，再测试信号量。也就是说，不能先执行 down(mutex) 再执行 down(empty)。如果这么做了，那么可能会出现这种情况：生产者对缓冲区加锁后，执行 down(empty) 操作，发现 empty = 0，此时生产者睡眠。消费者不能进入临界区，因为生产者对缓冲区加锁了，消费者就无法执行 up(empty) 操作，empty 永远都为 0，导致生产者永远等待下，不会释放锁，消费者因此也会永远等待下去。\n异步编程\n异步（Asynchronous）：异步编程是一种处理并发操作的方式，其中不需要等待一个操作完成才能执行下一个操作。在异步编程中，一个操作的启动不会阻塞程序的其他部分，而是允许程序继续执行其他操作，之后当异步操作完成时，程序会得到通知或回调。这种方式可以提高程序的响应性和并发性。但由于资源有限，进程的执行不是一贯到底的，这就是进程的异步性。 异步和同步是相对的，同步就是顺序执行，执行完一个再执行下一个，需要等待、协调运行\n“异步阻塞” 意味着可能存在某些情况下，虽然使用了异步编程的方式，但在某些时刻程序仍然被阻塞，等待某些操作的完成。这种情况可能发生在以下情形下：\n\n某些异步操作内部使用了同步操作，导致当前线程在等待这些同步操作完成时被阻塞。\n异步操作之间存在依赖关系，某个异步操作必须等待另一个异步操作完成后才能执行。\n程序中的某些部分仍然采用同步编程方式，而不是全面采用异步编程。\n\n要避免异步操作中的阻塞，通常需要使用异步编程框架和模式，以确保在异步操作中也不会阻塞程序的其他部分。这包括使用回调、Promise、异步/await等机制来管理异步操作的执行顺序和依赖关系，从而实现真正的非阻塞异步编程。\neg.       我们以经典的读取文件的模型举例。（对操作系统而言，所有的输入输出设备都被抽象成文件。）\n在发起读取文件的请求时，应用层会调用系统内核的 I/O 接口。\n如果应用层调用的是阻塞型 I/O，那么在调用之后，应用层即刻被挂起，一直出于等待数据返回的状态，直到系统内核从磁盘读取完数据并返回给应用层，应用层才用获得的数据进行接下来的其他操作。\n如果应用层调用的是非阻塞 I/O，那么调用后，系统内核会立即返回（虽然还没有文件内容的数据），应用层并不会被挂起，它可以做其他任意它想做的操作。（至于文件内容数据如何返回给应用层，这已经超出了阻塞和非阻塞的辨别范畴。）\n阻塞和非阻塞解决了应用层等待数据返回时的状态问题，那系统内核获取到的数据到底如何返回给应用层呢？是否是阻塞还是非阻塞，关注的是接口调用（发出请求）后等待数据返回时的状态。\n同步还是异步，关注的是任务完成时消息通知的方式。由调用方盲目主动问询的方式是同步调用，由被调用方主动通知调用方任务已完成的方式是异步调用。\nJAVA中的线程安全\nJava内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。\nJava提供了volatile关键字来保证可见性。当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。关闭缓存，（从Load到Store）是不安全的，中间如果其他的CPU修改了值将会丢失。下面的测试代码可以实际测试voaltile的自增没有原子性（原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。\n编译器层面 和CPU层面都提供了一套内存屏障来禁止重排序的指令，编码人员需要识别存在数据依赖的地方加上一个内存屏障指令，那么此时计算机将不会对其进行指令优化。\n在多线程环境下，确保代码正确同步和避免重排序reording是至关重要的。Java 提供了各种机制来帮助程序员在多线程场景下明确表达数据依赖和避免指令重排序：\n\nvolatile 关键字： 使用 volatile 关键字可以确保变量的可见性，并防止编译器和处理器对被 volatile 修饰的变量进行重排序。它适用于简单的变量操作，但不能保证原子性。\nsynchronized 关键字： 使用 synchronized 关键字可以确保代码块在同一时刻只能被一个线程执行，从而保证了线程安全性和避免了重排序。\nLock 接口及其实现类： Java 提供了 Lock 接口及其实现类，如 ReentrantLock，允许更灵活地进行锁定和解锁，并提供了比 synchronized 更多的特性。\nAtomic 类： Java 提供了一系列原子操作类，如 AtomicInteger、AtomicBoolean 等，用于在不使用锁的情况下执行原子操作。\n\n\n\nHappens-Before 规则： Java 内存模型定义了 Happens-Before 规则，规定了一系列操作的顺序，以保证多线程环境下的一致性和可预测性。程序员可以利用这个规则来确保代码正确性。\nJava 内存模型下一共有 8 条 happens-before 规则，如果线程间的操作无法从如下几个规则推导出来，那么它们的操作就没有顺序性保障，虚拟机或者操作系统就能随意地进行重排序，从而可能会发生并发安全问题。\n\n程序次序规则（Program Order Rule）：在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作。准确地说，应该是控制流顺序而不是程序代码顺序，因为要考虑分支、循环等结构。\n管程锁定规则（Monitor Lock Rule）：一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。这里必须强调的是同一个锁，而 “后面” 是指时间上的先后顺序。\nvolatile 变量规则（Volatile Variable Rule）：对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作，这里的 “后面” 同样是指时间上的先后顺序。\n线程启动规则（Thread Start Rule）：Thread 对象的 start () 方法先行发生于此线程的每一个动作。\n线程终止规则（Thread Termination Rule）：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过 Thread.join () 方法结束、Thread.isAlive () 的返回值等手段检测到线程已经终止执行。\n线程中断规则（Thread Interruption Rule）：对线程 interrupt () 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 Thread.interrupted () 方法检测到是否有中断发生。\n对象终结规则（Finalizer Rule）：一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize () 方法的开始。\n传递性（Transitivity）：如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那就可以得出操作 A 先行发生于操作 C 的结论。\n\n\n\n\n线程安全的集合类： Java 提供了一些线程安全的集合类，如 ConcurrentHashMap、CopyOnWriteArrayList 等，可以在多线程环境下安全地进行操作。\nvolatile 的双重检查锁定模式（Double-Checked Locking）： 在需要延迟初始化对象的情况下，使用双重检查锁定模式结合 volatile 关键字，可以确保线程安全性。\n\nScheduledExecutorService的主要作用就是可以将定时任务与线程池功能结合使用\n\n\nJava中存在两种锁机制：synchronized和Lock，Lock接口及其实现类是JDK5增加的内容\nsynchronized:  Java的关键字，在jvm层面上                 Lock: 是一个接口\n用法\nsynchronized: 在需要同步的对象中加入此控制，synchronized可以加在方法上，也可以加在特定代码块中，括号中表示需要锁的对象。\nLock: 一般使用ReentrantLock类做为锁。在加锁和解锁处需要通过lock()和unlock()显示指出。所以一定要在finally块中写unlock()以防死锁。\n锁的释放\nsynchronized: 1、以获取锁的线程执行完同步代码，释放锁 2、线程执行发生异常，jvm会让线程释放锁。在发生异常时候会自动释放占有的锁，因此不会出现死锁\nLock: 在finally中必须释放锁，不然容易造成线程死锁。必须手动unlock来释放锁，可能引起死锁的发生\n锁的获取\nsynchronized: 假设A线程获得锁，B线程等待。如果A线程阻塞，B线程会一直等待\nLock: 分情况而定，Lock有多个锁获取的方式，大致就是可以尝试获得锁，线程可以不用一直等待(可以通过tryLock判断有没有锁)\n锁的状态         synchronized: 无法判断                           Lock: 可以判断\n\nLock可以提高多个线程进行读操作的效率。（可以通过readwritelock实现读写分离）\n在资源竞争不是很激烈的情况下，Synchronized的性能要优于ReetrantLock，但是在资源竞争很激烈的情况下，Synchronized的性能会下降几十倍，但是ReetrantLock的性能能维持常态；\nReentrantLock提供了多样化的同步，比如有时间限制的同 步，可以被Interrupt的同步（synchronized的同步是不能Interrupt的）等。在资源竞争不激烈的情形下，性能稍微比synchronized差点点。但是当同步非常激烈的时候，synchronized的性能一下子能下降好几十倍。而ReentrantLock确还能维持常态。公平锁机制。什么叫公平锁呢？也就是在锁上等待时间最长的线程将获得锁的使用权。通俗的理解就是谁排队时间最长谁先执行获取锁。\n\n② 是否可手动释放\nsynchronized 不需要用户去手动释放锁，synchronized 代码执行完后系统会自动让线程释放对锁的占用； ReentrantLock则需要用户去手动释放锁，如果没有手动释放锁，就可能导致死锁现象。一般通过lock()和unlock()方法配合try/finally语句块来完成，使用释放更加灵活。\n③ 是否可中断\nsynchronized是不可中断类型的锁，除非加锁的代码中出现异常或正常执行完成； ReentrantLock则可以中断，可通过trylock(long timeout,TimeUnit unit)设置超时方法或者将lockInterruptibly()放到代码块中，调用interrupt方法进行中断。\n④ 是否公平锁\n所以公平和非公平的区别就是：线程执行同步代码块时，是否会去尝试获取锁。\n**如果会尝试获取锁，那就是非公平的。如果不会尝试获取锁，直接进队列，再等待唤醒，那就是公平的。**synchronized为非公平锁 ReentrantLock则即可以选公平锁也可以选非公平锁，通过构造方法new ReentrantLock时传入boolean值进行选择，为空默认false非公平锁，true为公平锁。\n可重入锁是指同一个线程如果首次获得了这把锁，那么因为它是这把锁的拥有者，因此 有权利再次获取这把锁\n公平锁是等待锁的线程不会饿死。缺点是整体吞吐效率相对非公平锁要低，等待队列中除第一个线程以外的所有线程都会阻塞，CPU唤醒阻塞线程的开销比非公平锁大。\n非公平锁的优点是可以减少唤起线程的开销，整体的吞吐效率高，因为线程有几率不阻塞直接获得锁，CPU不必唤醒所有线程。缺点是处于等待队列中的线程可能会饿死，或者等很久才会获得锁。\n调度\nsynchronized: 使用Object对象本身的wait 、notify、notifyAll调度机制\nLock: 可以使用Condition进行线程之间的调度\n底层实现\nsynchronized: 底层使用指令码方式来控制锁的，映射成字节码指令就是增加来两个指令：monitorenter和monitorexit。当线程执行遇到monitorenter指令时会尝试获取内置锁，如果获取锁则锁计数器+1，如果没有获取锁则阻塞；当遇到monitorexit指令时锁计数器-1，如果计数器为0则释放锁。\nLock: 底层是CAS乐观锁，依赖AbstractQueuedSynchronizer类，把所有的请求线程构成一个CLH队列。而对该队列的操作均通过Lock-Free（CAS）操作。\n关于ReentrantLock和CAS的关系：ReentrantLock是一个基于CAS的悲观锁，而不是乐观锁。ReentrantLock使用CAS操作来实现锁的获取和释放，但它的行为是悲观的，因为它要求线程在访问共享资源之前获得锁。CAS本身是一种乐观锁的实现方式，但ReentrantLock使用CAS来保证互斥性，因此它被归类为悲观锁。总的来说，乐观锁和悲观锁是两种不同的并发控制策略，它们在多线程环境下解决了竞争问题，但它们的使用场景和实现方式有所不同。 CAS是一种用于实现乐观锁的原子操作，而ReentrantLock是一种基于CAS的悲观锁。\n\n乐观锁：乐观锁是一种假设在大多数情况下不会发生冲突的锁。它不会在访问共享资源之前获取锁，而是在更新共享资源时进行检查。如果检测到其他线程已经更新了共享资源，那么当前线程可能需要重试或者采取其他措施来处理冲突。乐观锁通常用于多读的场景，可以提高吞吐量。\n悲观锁：悲观锁是一种假设在大多数情况下会发生冲突的锁。它在访问共享资源之前获取锁，并假定其他线程会干扰。悲观锁通常用于写入操作，以确保在写入共享资源时不会发生冲突。\n\n乐观锁的实现方式主要有两种：CAS机制和版本号机制。\n对于ABA问题，比较有效的方案是引入版本号，内存中的值每发生一次变化，版本号都+1；在进行CAS操作时，不仅比较内存中的值，也会比较版本号，只有当二者都没有变化时，CAS才能执行成功。Java中的AtomicStampedReference类便是使用版本号来解决ABA问题的。\n除了CAS，版本号机制也可以用来实现乐观锁。版本号机制的基本思路是在数据中增加一个字段version，表示该数据的版本号，每当数据被修改，版本号加1。当某个线程查询数据时，将该数据的版本号一起查出来；当该线程更新数据时，判断当前版本号与之前读取的版本号是否一致，如果一致才进行操作。\n\n\n最经典的分布式锁是可重入的公平锁，可重入锁（Reentrant Lock）是一种特殊类型的锁，允许同一个线程多次获取同一个锁而不会导致死锁。这种锁可以多次被同一线程获取，而不会因为同一线程的重复获取而阻塞自己。\n","categories":["计算机科学"],"tags":["计算机基础","多线程"]},{"title":"The Curse of Meritocracy","url":"/2025/04/13/Curse/","content":"From a young age, many of us are told: “If you work hard and prove yourself, you will be rewarded.”\nWe are steeped in the gospel of meritocracy from childhood. Study diligently, excel academically, collect the accolades – the path to a successful future, we are assured, is paved with quantifiable effort and talent. As students aiming for demanding fields like Computer Engineering, this ethos becomes an intense reality. School morphs into an arena for performance, knowledge a means to an end measured in grades and rankings, and the ultimate prize: admission into prestigious institutions.\nI bought into this promise wholeheartedly. I learned the rules, played the game, and poured years into mastering the metrics. And by the system’s own standards, I succeeded. Multiple Master’s offers in Computer Engineering from international universities felt like the culmination, the hard-earned validation.\n**The Twist\n**\nHowever, life doesn’t always go as expected. That’s it.\nI didn’t plan to take a gap year, but I saw it coming.\nSomewhere in this journey, I started questioning something deeper — not just why it’s hard to get a job in tech right now, but why it feels so personal.\nThis reflection led me to confront a much bigger issue: the curse of meritocracy.\nIn a true meritocracy, these things should translate into opportunities, right?\n\nGet good grades\nWin competitions\nFind a job\nSo when job offers don’t come in — or only come in under unfair terms — we’re left confused, maybe even ashamed. “Did I not work hard enough?” becomes an internal voice of blame.\n\nHere lies the curse: when success is framed as purely based on merit, failure feels entirely like a personal defect.\nBut the truth is more complex. The job market is cyclical. Recruiters often filter by brand names, not by capability. Many good roles are never posted publicly. Policies, market recessions, and insider referrals all distort the game. This isn’t a complaint — it’s a reality check.\nMerit is just one variable in a messy, nonlinear equation. The job market was still competitive, and societal expectations hadn’t vanished. But something fundamental had shifted within me. I used to think not getting into a FAANG company meant I was behind.\nBut Is it really matter that i solve the leetcode in time in a interview?\nFor a while, I thought the solution was simple: solve more Leetcode problems, and do it faster. I treated technical interviews as puzzles I just needed to crack.\nBut here’s what I learned: solving algorithms under a time limit is a skill, but it’s not the only one that matters.\nWhat it doesn’t prove in a person:\nThat you’re a good software engineer.\nThat you can maintain and refactor complex codebases.\nThat you can build user-facing systems with long-term impact.\nThat you think creatively or critically beyond known patterns.\nI think collaborating in hackathons gave me resilience, not just resume. Building tools and small games that someone actually use feels good.\nTrying, failing, and reflecting on user feedback have taught me far more than traditional coursework ever did. I learned to build before I felt ready, to listen more than explain, and to treat failure not as a signal to stop, but as a prompt to iterate more fast.\nTo truly understand feedback, you have to step into the user’s shoes. They are the ultimate arbiters of value. What seems intuitive to you might be completely confusing to them. What you thought was a killer feature might be ignored, while a minor element becomes unexpectedly popular.\nThat kind of growth doesn’t show up on a transcript — but it shapes the kind of engineer, teammate, and thinker I’ve become.\nMeritocracy told me to chase validation. Reality taught me to chase growth. It equates our human value with our output, fosters crippling anxiety, and can alienate us from our own genuine interests and sense of self. I didn’t magically have all the answers, nor was I suddenly immune to the pressures of the world.\n**To Others on a Similar Path\n**\nIf you’re always feel being left, or struggling to find your place — you’re not behind. You’re just not playing a zero-sum game anymore.\nGap Year was transformative for me because it existed outside the metrics of the standardize life I should live. I didn’t magically have all the answers, nor was I suddenly immune to the pressures of the world in the past 2 years. But something fundamental had shifted within me.\nI’m still try to find the “right” job — but I’m no longer hunting approval. I think the most significant 成长 (growth) came from stepping outside the achievement framework altogether and find your own way of success.\n","tags":["feelings","personal growth"]},{"title":"分布式系统中的一致性模型","url":"/2024/07/21/Distributed/","content":"一致性模型概述\n一致性模型是分布式系统中对数据读写操作顺序和结果的约定。不同的一致性模型定义了在不同情况下系统对数据的一致性保证程度。而其之所以这么重要正是因为其复杂而又直接关系到一个分布式系统的正确性，所以值得被反复讨论。\n在开发单机多线程程序的时候，不同线程如果不加任何措施同时对同一片内存进行操作会冲突造成不可预知的后果。分布式系统同样是个并发系统，这个问题依旧，不同的结点同时对同一个数据进行修改同样会造成不可预知的后果，造成数据一致性被破坏，同时加上分布式系统中可用性和性能等因素，使得这一问题更加棘手。\n对于并发操作来说，其实本质上是一个顺序的操作序列，只是不同进程的操作被交织在了一起，而一致性模型就是用于判断这个操作序列是否合法。因此，一致性模型其实就是一种约定，如果一个系统声称满足某个一致性模型，那么它就保证了在一定条件下，对于数据的读写操作都是可预期的。\n这边引用 Jepson 的一张关于一致性模型的图\n\n这张图中，从下往上，模型对于操作序列的要求更严格，称为更强（stronger）的一致性模型，反之要求则更宽松，称为更弱（weaker）。而箭头 A-&gt;B 则表示满足 B 则一定满足 A，也就是说满足 B 的操作是满足 A 的操作的一个子集。\n根据 CAP 理论，当追求 C 时势必需要放弃 A 或 P。如果发生了网络错误，图中红色的表示必须暂停服务，黄色的表示客户端仍可以继续在同一个未发生错误的结点上继续工作，蓝色的表示即使网络完全错误，仍可以切换至正常的结点上继续工作。\n在对每个模型进行介绍之前，还需先明确最理想的状态是怎么样的，也就是一致性模型所需要保证的目标。最强的一致性模型自然是图中位于最上方的 Strict Serializability。其实说来简单，就是单机执行的状态，所有事务的子操作不交错，根据实际发生的次序顺序执行。这其实正好对应了图中指向它的 Serializability 和 Linearizable，背后其实对应的是 隔离性（数据库事务 ACID 中的 I）和一致性（分布式系统 CAP 的 C）。\n隔离性\n隔离性指的是各个事务执行过程相互隔离，防止并发执行导致数据不一致。在解释各个隔离性的模型之前，让我们首先看看少了隔离性会遇到什么样的问题。\n潜在的问题\nP0 Dirty Write\nDirty Write 就是一个事务修改了另一个尚未提交的事务已经修改的值，如下\n+------+-------+-------+-------+-------+\n| T1   | Wx(1) |       |       | Wy(1) |\n+------+-------+-------+-------+-------+\n| T2   |       | Wx(2) | Wy(2) |       |\n+------+-------+-------+-------+-------+\n| x(0) | 1     | 2     | 2     | 2     |\n+------+-------+-------+-------+-------+\n| y(0) | 0     | 0     | 2     | 1     |\n+------+-------+-------+-------+-------+\n\nT2 的 Wx(2) 覆盖了 T1 的 Wx(1)，造成该修改的丢失。\nP1 Dirty Read\nDirty Read 指一个事务读取到了另一个执行到一半的事务中修改的值，如下\n+-------+--------+--------+--------+--------+\n| T1    | Wx(40) |        |        | Wy(60) |\n+-------+--------+--------+--------+--------+\n| T2    |        | Rx(10) | Ry(50) |        |\n+-------+--------+--------+--------+--------+\n| x(50) | 40     | 10     | 10     | 10     |\n+-------+--------+--------+--------+--------+\n| y(50) | 50     | 50     | 50     | 60     |\n+-------+--------+--------+--------+--------+\n\n假设原本 x 与 y 存款均为50，现在 T1 负责 x 向 y 转账 10，T2 负责读取 x 和 y 的存款。那么无论如何转账，两人存款之和必定是 100，Dirty Read 使得这一约束无法得到保证。\nP2 Non-Repeatable Read\nNon-Repeatable Read 也称 Fuzzy Read，指一个事务读取过程中读到了另一个事务更新后的结果，如下\n+-------+--------+--------+--------+--------+\n| T1    | Rx(50) |        |        | Ry(60) |\n+-------+--------+--------+--------+--------+\n| T2    |        | Wx(40) | Wy(60) |        |\n+-------+--------+--------+--------+--------+\n| x(50) | 50     | 40     | 40     | 40     |\n+-------+--------+--------+--------+--------+\n| y(50) | 50     | 50     | 60     | 60     |\n+-------+--------+--------+--------+--------+\n\n同样转账的例子，T1 读取到一半时插入 T2 的更新操作，导致约束条件再次被破坏。\nP3 Phantom\nPhantom 指的是某一事务 A 先挑选出了符合一定条件的数据，之后另一个事务 B 修改了符合该条件的数据，此时 A 再进行的操作都是基于旧的数据，从而产生不一致，例如\n+----------------+--------+-----------+-----------+-----------+\n| T1             | {a, b} |           |           | R(3)      |\n+----------------+--------+-----------+-----------+-----------+\n| T2             |        | W(c)      | W(3)      |           |\n+----------------+--------+-----------+-----------+-----------+\n| Employees      | {a, b} | {a, b, c} | {a, b, c} | {a, b, c} |\n+----------------+--------+-----------+-----------+-----------+\n| Employee Count | 2      | 2         | 3         | 3         |\n+----------------+--------+-----------+-----------+-----------+\n\nT1 前后两次读由于 T2 的存在打破了约束，使得两次读的结果不一致。\nP4 Lost Update\nLost Update 指的是更新被另一个事务覆盖，例如\n1\n2\n3\n4\n5\n6\n7\n+--------+-----+---------+---------+\n| T1     |     |         | Wx(110) |\n+--------+-----+---------+---------+\n| T2     |     | Wx(120) |         |\n+--------+-----+---------+---------+\n| x(100) | 100 | 120     | 110     |\n+--------+-----+---------+---------+\n\nT2 执行于 T1 执行过程中间，两个事务之间并没有发生任何 Dirty Read，但 T2 的更新被丢失了，相当于并没有被执行。\nP4C Cursor Lost Update\nCursor Lost Update 与前者类似，只是发生于 cursor 的操作过程之中，例如\n1\n2\n3\n4\n5\n6\n7\n+--------+----------+---------+----------+\n| T1     | RCx(100) |         | Wx(110)  |\n+--------+----------+---------+----------+\n| T2     |          | Wx(75)  |          |\n+--------+----------+---------+----------+\n| x(100) | 100      | 75      | 110      |\n+--------+----------+---------+----------+\n\nRCx 表示通过 cursor 读取，这样导致 T2 的更新操作被丢失了。\nA5A Read Skew\nRead Skew 由于事务的交叉导致读取到了不一致的数据，例如转账的例子\n+-------+--------+--------+--------+--------+\n| T1    | Rx(50) |        |        | Ry(75) |\n+-------+--------+--------+--------+--------+\n| T2    |        | Wx(25) | Wy(75) |        |\n+-------+--------+--------+--------+--------+\n| x(50) | 50     | 25     | 25     | 25     |\n+-------+--------+--------+--------+--------+\n| y(50) | 50     | 50     | 75     | 75     |\n+-------+--------+--------+--------+--------+\n\nA5B Write Skew\nWrite Skew 指两个事务同时读取到了一致的数据，然后分别进行了满足条件的修改，但最终结果破坏了一致性，例如\n+-------+--------+--------+--------+--------+\n| T1    | Rx(30) | Ry(10) | Wy(60) |        |\n+-------+--------+--------+--------+--------+\n| T2    | Rx(30) | Ry(10) |        | Wx(50) |\n+-------+--------+--------+--------+--------+\n| x(30) | 30     | 30     | 30     | 50     |\n+-------+--------+--------+--------+--------+\n| y(10) | 10     | 10     | 60     | 60     |\n+-------+--------+--------+--------+--------+\n\n要求 x+y &lt;= 100，T1 和 T2 都读取到了符合条件的数据，并在要求范围内修改了数据，但最后结果破坏了约束。\n隔离级别\n想要避免上述问题，付出的代价是不同的。为了确定上述情况是否会发生，同时能够根据场景选择付出怎么样的代价，因此产生了隔离级别这一概念，用于指明可能发生的问题，共有如下数种隔离级别：\n\nRead Uncommitted：事务执行过程中能够读到未提交的修改。\nRead Committed：事务执行过程中能够读到已提交的修改。\nCursor Stability：使用 cursor 读取某个数据时，这个不能被其他事务修改直至 cursor 释放或事务结束。\nMonotonic Atomic View：在 Read Committed 的基础上加上了原子性的约束，观测到其他事务的修改时会观察到完整的修改。\nRepeatable Read：即使其他事务修改了数据，重复读取都会读到一样的数据。\nSnapshot：每个事务在独立、一致的 snapshot 上进行操作，直至提交后其他事务才可见。\nSerializable：事务按照一定的次序顺序执行。\n\n对应的可能发生的问题如下\n\nP0\nP1\nP4C\nP4\nP2\nP3\nA5A\nA5B\nRead Uncommitted\nNP\nP\nP\nP\nP\nP\nP\nP\nRead Committed\nNP\nNP\nP\nP\nP\nP\nP\nP\nCursor Stability\nNP\nNP\nNP\nSP\nSP\nP\nP\nSP\nRepeatable Read\nNP\nNP\nNP\nNP\nNP\nP\nNP\nNP\nSnapshot\nNP\nNP\nNP\nNP\nNP\nSP\nNP\nP\nSerializable\nNP\nNP\nNP\nNP\nNP\nNP\nNP\nNP\n\nNP(Not Possible)：不可能发生\nSP(Sometimes Possible)：有时候可能发生\nP(Possible)：会发生\n\nMySQL 中只采用了 Read Uncommitted、Read Committed、Repeatable Read、Serializable 四种级别。\n一致性\n隔离性并不保证其他节点看到数据的顺序，这一点由一致性来进行保证。一致性指的是所有结点都能访问到最新的数据副本，并因此衍生出了不同级别的一致性，以表示所能看到的数据发生的顺序。\n强一致性（Strong Consistency）：\n严格串行化（Strict Serializability）：\n定义：所有操作都可以看作是按照某个全局的、线性的顺序执行的，且每个操作的效果立刻对后续操作可见。\n应用场景：强一致性要求很高的系统，如银行交易系统。\n线性化（Linearizability）：\n定义：每个操作在某个点上“原子的”发生，且这个点必须在操作开始和结束之间。\n应用场景：需要保证最新写入立刻对所有后续读可见的系统，如某些分布式数据库和文件系统。\n弱一致性（Weak Consistency）：\n最终一致性（Eventual Consistency）：\n定义：如果没有新的更新操作，系统最终会达到一致状态，即所有副本的数据最终会收敛到相同的状态。\n应用场景：高可用性和性能要求高的系统，如 DNS、CDN 等。\n因果一致性（Causal Consistency）：\n定义：如果一个操作影响了另一个操作，那么前一个操作的结果必须在后一个操作之前可见。\n应用场景：需要保证因果关系的系统，如社交网络中的动态更新。\n序列一致性（Sequential Consistency）：\n定义：所有操作按照某个顺序执行，但这个顺序不必与实际时间顺序一致，只需对所有进程可见的顺序一致。\n应用场景：需要保证全局操作顺序一致性的系统，如某些分布式存储系统。\n弱化的隔离级别：\n读已提交（Read Committed）：\n定义：只能读取已经提交的数据，但不同事务之间的操作顺序不一定一致。\n应用场景：数据库系统中，读已提交是一个常见的隔离级别。\n可重复读（Repeatable Read）：\n定义：在同一事务内多次读取同一数据项，结果必须一致。\n应用场景：需要避免“不可重复读”问题的系统，如某些事务处理系统\n\nWrites Follow Reads：如果一个进程读到了操作 w1 修改的值，并之后执行了操作 w2，那么 w2 只有在 w1 后可见，也就是说如果看到了 w2 那么一定看到了 w1。\nMonotonic Reads：如果一个进程进行了读取 r1，再进行 r2，那么 r2 不可能看到 r1 发生前的数据。\nMonotonic Writes：如果一个进程进行了写入 w1，再进行 w2，那么其他进程都会看到 w1 发生于 w2 之前。\nRead Your Writes：如果一个进程进行了写入 w，再进行读取 r，那么 r 一定能看到 w 所进行的修改。\nPRAM (Pipeline Random Access Memory)：单进程的写操作被观察到都是顺序的，不同进程间的写操作被观察到的顺序则可能不同。PRAM = Monotonic Writes + Monotonic Reads + Read Your Writes.\nCasual：存在因果关系的操作在任何地方被观察的顺序是一致的。\nSequential：保证操作一定按一定顺序发生，且任何地方观测到的都是一致的。\nLinearizable：所有操作都按操作发生的时间顺序原子地发生。\n\n参考\n\n一致性模型\nJepson\nA Critique of ANSI SQL Isolation Levels\n\n\nCATALOG\n\n隔离性\n潜在的问题\nP0 Dirty Write\nP1 Dirty Read\nP2 Non-Repeatable Read\nP3 Phantom\nP4 Lost Update\nP4C Cursor Lost Update\nA5A Read Skew\nA5B Write Skew\n隔离级别\n一致性\n参考\n\n","categories":["计算机科学"],"tags":["数据库","分布式系统"]},{"title":"从Docker容器探究Linux","url":"/2024/06/14/Dockerlinux/","content":"我最近一直在研究开源多语言应用服务器 NGINX Unit。在研究中，我注意到 Unit 同时支持 namespace 和 cgroup 这两项 进程隔离 的特性本文将介绍这两大构成 容器 基础的 Linux 技术。 容器及相关工具（例如 Docker 和 Kubernetes） 出现有一段时间了。它们改变了现代应用环境中软件的开发和交付方式。容器可支持软件在各自的隔离环境中快速部署和运行，而无需用户构建单独的虚拟机 (VM)。 大多数人可能很少考虑容器的工作基础，但我认为，了解底层技术很重要，因为这有助于我们制定决策。另外，就我个人而言，能够彻底弄清事物的工作原理令我心情舒畅！\nDocker由Solomon Hykes及其团队在DotCloud（后来更名为Docker, Inc.）公司内部开发和推出。Docker的初次发布是在2013年3月。Docker 在初期与 Warden 类似，使用的也是 LXC，之后才开始采用自己开发的 libcontainer 来替代 LXC，它是将应用程序及其依赖打包到几乎可以在任何服务器上运行的容器的工具。与其他只做容器的项目不同的是，Docker 引入了一整套管理容器的生态系统，这包括高效、分层的容器镜像模型、全局和本地的容器注册库、清晰的 REST API、命令行等等。\n发明Docker的背景和初衷\n问题背景\n开发环境与生产环境不一致：\n在传统的软件开发过程中，开发环境和生产环境之间往往存在差异，这会导致应用程序在开发人员的机器上运行正常，但在生产环境中出现问题。这种“在我的机器上能运行”的问题困扰了许多开发团队。\n依赖管理的复杂性：\n现代应用程序通常依赖于许多不同的库、框架和服务。管理这些依赖关系，并确保它们在不同环境中的一致性，是一个复杂且容易出错的过程。\n资源效率低下：\nNamespace 作为 Linux 内核的组成部分大约出现于 2002 年，随着时间的推移，Linux 内核添加了更多的工具和 namespace 类型。然而，直到 2013 年，Linux 内核才添加了真正的容器支持。\nDocker引入的解决方案\nDocker利用操作系统层的虚拟化技术，通过Linux内核的Cgroups和Namespaces实现轻量级的容器。与虚拟机不同，容器共享主机操作系统的内核，但仍提供进程级别的隔离。这使得容器比虚拟机更轻量、更高效。\n通过Docker，开发人员可以将应用程序及其所有依赖项打包成一个标准化的容器镜像。这个容器镜像可以在任何支持Docker的环境中运行，从而确保开发、测试和生产环境的一致性。Docker容器的快速启动和停止使得应用程序的部署和扩展变得更加简便。容器的不可变性也有助于实现更可靠的持续集成和持续交付（CI/CD）流程。Docker天然适合微服务架构，每个微服务可以运行在一个独立的容器中，互不干扰。这简化了微服务的开发、部署和管理。\nWSL\nWSL 提供的隔离比传统的虚拟机和容器要弱得多。它更注重与 Windows 的集成和互操作性，而不是隔离。WSL 更像是一个集成的开发环境，可以在 Windows 中无缝运行 Linux 工具和应用程序，但它没有提供强隔离性的需求，比如网络隔离或进程隔离。因此，WSL 更适合用于开发和测试场景，而不是对隔离性要求很高的生产环境。\n\nFile System: WSL interacts with the Windows file system, which means there might be differences in file I/O performance or behavior compared to a native Ubuntu installation where it interacts directly with Linux file systems like ext4.\nIntegration: WSL provides integration features allowing Windows and Linux to communicate and share resources. However, this integration might not be as seamless as in a native Ubuntu environment.\nHardware Access: Certain hardware functionalities and access might be different between WSL and native Ubuntu due to how WSL interacts with the underlying Windows system.\n\nLinux内核调用\nNamespaces和Cgroups是容器化技术的两个关键机制，它们分别用于实现进程隔离和资源管理。以下是对Namespaces和Cgroups的详细介绍：\nNamespace 作为 Linux 内核的组成部分大约出现于 2002 年，随着时间的推移，Linux 内核添加了更多的工具和 namespace 类型。然而，直到 2013 年，Linux 内核才添加了真正的容器支持。至此，namespace 开始大显身手，并得到了广泛应用。\nLinux 内核包含了不同类型的 namespace。每个 namespace 都有自己的独特属性。\n\nIPC：隔离System V IPC和POSIX消息队列。\nNetwork：隔离网络资源。\nMount：隔离文件系统挂载点。\nPID：隔离进程ID，使得每个Namespace内的进程拥有独立的PID空间。一个Namespace中的进程无法看到或干扰另一个Namespace中的进程。\nUTS：隔离主机名和域名。\nUser：隔离用户ID和组ID。\n\nNameSpace我们知道Linux中的PID、IPC、网络等资源是全局的，而NameSpace机制是一种资源隔离方案，在该机制下这些资源就不再是全局的了，而是属于某个特定的NameSpace，各个NameSpace下的资源互不干扰，这就使得每个NameSpace看上去就像一个独立的操作系统一样，但是只有NameSpace是不够。\nControl groups虽然有了NameSpace技术可以实现资源隔离，但进程还是可以不受控的访问系统资源，比如CPU、内存、磁盘、网络等，为了控制容器中进程对资源的访问，Docker采用control groups技术(也就是cgroup)，有了cgroup就可以控制容器中进程对系统资源的消耗了，比如你可以限制某个容器使用内存的上限、可以在哪些CPU上运行等等。\n有了这些理论，现在我们实际创建一个新的 namespace 以加深理解。Linux unshare 命令是一个很好的着手点。手册页显示它就是我们要找的：\nNAME\nunshare - run program in new name namespaces\n当前我以普通用户 svk 的身份登录，该用户拥有自己的用户 ID、组等，但没有 root 权限：\nsvk $ id uid=1000(svk) gid=1000(svk) groups=1000(svk) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c.1023\n现在，我运行以下 unshare 命令，使用自己的 user 和 PID namespace 创建一个新的 namespace。我将 root 用户映射到新的 namespace（换句话说，我在新的 namespace 中拥有 root 权限），挂载一个新的 proc 文件系统，并在新创建的 namespace 中进行进程（本例中为 bash）分支。\nsvk $ unshare --user --pid --map-root-user --mount-proc --fork bash （对于熟悉容器的人来说，这相当于在运行的容器中执行  exec -it  /bin/bash 命令。）\nps -ef 命令显示有两个进程正在运行（bash 和 ps 命令本身），并且 id 命令确认我在新的 namespace 中是 root 用户（这一点也可以从更改的命令提示符看出）：\nroot # ps -ef UID         PID     PPID  C STIME TTY        TIME CMD root          1        0  0 14:46 pts/0  00:00:00 bash root         15        1  0 14:46 pts/0  00:00:00 ps -ef root # id uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c.1023\n需要注意的是，我只能看到我的 namespace 中的两个进程，而看不到系统上运行的任何其他进程。我完全隔离在自己的 namespace 中。\n从外部角度看 Namespace\n虽然我从自己的 namespace 中看不到其他进程，但我可以使用 lsns (list namespace) 命令，从父 namespace（在新的 namespace 之外）的角度列出所有可用的 namespace 并显示其相关信息。\nDocker 文件系统和分层存储\nDocker镜像的工作原理涉及多个重要概念，包括rootfs、Union mount、image和layer。\n1. rootfs\n\n定义：rootfs（root filesystem）是一个操作系统在启动时所需要的最低限度文件系统，它包含所有必要的文件和目录。\n在Docker中的作用：在Docker中，rootfs通常指的是基础镜像层，它包含了操作系统的文件系统（如Linux的文件系统），这是容器运行所需的基本环境。例如，一个Ubuntu基础镜像的rootfs包含了Ubuntu系统的文件和目录。\n\n2. Union Mount\n\n定义：Union mount是一种文件系统技术，允许将多个不同的文件系统层合并成一个单一的文件系统视图。它实现了多个层的叠加，使得用户可以看到一个合并后的文件系统。\n在Docker中的作用：Docker利用Union mount技术将镜像的各层（只读层）和容器的可写层合并在一起，形成一个统一的文件系统视图。每个容器启动时，都会在镜像的基础上创建一个新的可写层，这个可写层是临时的，当容器删除时也会被删除。\n\n3. Image\n\n定义：在Docker中，image（镜像）是一个只读的模板，用于创建Docker容器。镜像包含了应用程序及其所有依赖项，以及运行应用程序所需的操作系统环境。\n在Docker中的作用：镜像是构建和运行容器的基础。每个镜像由多个层组成，每层都基于其下的层构建。镜像可以从Docker Hub等镜像仓库中拉取，也可以通过Dockerfile自定义构建。\n\n4. Layer\n\n定义：layer（层）是Docker镜像的基本组成单位。每个镜像是由多个只读层叠加而成的，每一层都是前一层的增量更新。\n在Docker中的作用：层是Docker镜像的核心特性之一。每个Dockerfile中的命令都会创建一个新的层。镜像的每一层都存储了该层的文件系统变更（添加、修改或删除的文件）。由于层的可重用性和缓存机制，Docker在构建镜像时可以显著提高效率，减少重复下载和存储的数据量。\n\n关系\n\nrootfs：作为基础层，为容器提供必要的操作系统文件和目录。\nUnion mount：将多个只读层和一个可写层合并成一个统一的文件系统视图，使得容器能够在不修改基础镜像层的情况下运行和保存数据。\nImage：由多个只读层组成，提供了创建和运行容器的模板。\nLayer：构成镜像的基本单位，每一层都是对前一层的增量更新，通过层的叠加和Union mount技术，实现了高效的镜像构建和管理。\n\n假设我们有一个简单的Dockerfile，用于构建一个包含Nginx的镜像：\ndockerfile复制代码`FROM ubuntu:20.04      # 基础镜像，提供rootfsRUN apt-get update &amp;&amp; apt-get install -y nginx  # 创建一个新层，安装NginxCOPY index.html /var/www/html/index.html  # 创建另一个新层，复制文件`\n\nrootfs：Ubuntu基础镜像提供了操作系统的文件系统。\nUnion mount：在运行容器时，Docker将基础镜像层（只读）和容器的可写层合并在一起，提供一个统一的文件系统视图。\nImage：这个Dockerfile构建出的镜像包含了Ubuntu操作系统和安装的Nginx软件，以及复制的文件。\nLayer：每个RUN、COPY指令都会创建一个新的层，这些层共同构成了最终的镜像。\n\n当 Docker Daemon 为 Docker Container 挂载 rootfs 的时候，与传统 Linux 内核类似，将其设定为只读（read-only）模式。在 rootfs 挂载完毕之后，和 Linux 内核不一样的是，Docker Daemon 没有将 Docker Container 的文件系统设为读写（read-write）模式，而是利用 Union mount 的技术，在这个只读的 rootfs 之上再挂载一个读写（read-write）的文件系统，挂载时该读写（read-write）文件系统内空无一物。\nCopy-On-Write (COW) 文件系统的核心理念是：在需要修改数据时，不直接修改原始数据，而是将其复制到新的可写层，然后在该新层上进行修改。这样做的好处是：\n\n原始数据保持不变，可以多次重复使用。\n数据修改在新的层上进行，保证了数据的可追溯性和可恢复性。\n\n具体到Docker，当容器试图修改一个只读文件时，例如 /etc/hosts，COW机制会将这个文件从只读层复制到可写层（容器存储层），然后对复制后的文件进行修改。这样，用户看到的文件系统就是修改后的版本，但原始的只读层数据并没有改变。\n前面讲过镜像使用的是分层存储，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为 容器存储层。容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。\n按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 数据卷（Volume）、或者 绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。\n容器技术可实现不同云计算之间应用程序的可移植性，以及提供了一个把应用程序拆分为分布式组件的方法。此外，用户还可以管理和扩展这些容器成为集群。\n分层存储\n因为镜像包含操作系统完整的 root 文件系统，其体积往往是庞大的，因此在 Docker 设计时，就充分利用 Union FS 的技术，将其设计为分层存储的架构。所以严格来说，镜像并非是像一个 ISO 那样的打包文件，镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。\n镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。\nCMD命令设置容器启动后默认执行的命令及其参数，当Dockerfile中存在多个CMD命令，只有最后一个会被执行，但CMD设置的命令能够被docker run后面的命令行参数覆盖替换。\nENTRYPOINT配置容器启动时的执行命令，当运行 docker run时指定了其他命令，docker run时指定的命令会追加到ENTRYPOINT配置命令行的参数一起执行。ENTRYPOINT 中的参数始终会被使用，与CMD不同，它不会被替换。\n","categories":["计算机科学"],"tags":["计算机基础"]},{"title":"理解数据库系统","url":"/2024/08/20/DB/","content":"Database\n数据库是一个存储数据的系统，它可以存储大量的数据，并且能够高效地检索这些数据。数据库中的数据被组织成表格的形式，这些表格称为表。表中的每一行代表一个记录，每一列代表一个字段。\n\n对于数据库而言，重要的不是数据量，而是当数据量增加时运算如何增加。时间复杂度用来检验某个算法处理一定量的数据要花多长时间，时间复杂度不会给出确切的运算次数，但是给出的是一种理念。\n\n根据数据模型分类：\n关系型数据库（RDBMS）：\n\n使用表格（表）和行列结构存储数据。\n数据以严格的结构（模式）存在，具有预定义的模式（Schema）。\n常见的关系型数据库包括 MySQL、PostgreSQL、Oracle 和 SQL Server 等。\n\n非关系型数据库（NoSQL）：\n键值（Key-Value）数据库\n概述：键值数据库就像在传统语言中使用的哈希表。你可以通过key来添加、查询或者删除数据，鉴于使用主键访问，所以会获得不错的性能及扩展性。\n产品：Riak、Redis、Memcached、Amazon’s Dynamo、Project Voldemort\n适用的场景：\n储存用户信息，比如会话、配置文件、参数、购物车等等。这些信息一般都和ID（键）挂钩，这种情景下键值数据库是个很好的选择。\n不适用场景\n需要通过值来查询。Key-Value数据库中根本没有通过值查询的途径\n需要储存数据之间的关系。在Key-Value数据库中不能通过两个或以上的键来关联数据。\n需要事务的支持。在Key-Value数据库中故障产生时不可以进行回滚。\n二、 面向文档（Document-Oriented）数据库\n概述：面向文档数据库会将数据以文档的形式储存。每个文档都是自包含的数据单元，是一系列数据项的集合。\n每个数据项都有一个名称与对应的值，值既可以是简单的数据类型，如字符串、数字和日期等；也可以是复杂的类型，如有序列表和关联对象。\n数据存储的最小单位是文档，同一个表中存储的文档属性可以是不同的，数据可以使用XML、JSON或者JSONB等多种形式存储。\n产品：MongoDB、CouchDB、RavenDB\n适用的场景\n日志。企业环境下，每个应用程序都有不同的日志信息。Document-Oriented数据库并没有固定的模式，所以我们可以使用它储存不同的信息。\n分析。鉴于它的弱模式结构，不改变模式下就可以储存不同的度量方法及添加新的度量。\n不适用场景\n在不同的文档上添加事务。Document-Oriented数据库并不支持文档间的事务，如果对这方面有需求则不应该选用这个解决方案。\n三、 列存储数据库\n概述：列存储数据库将数据储存在列族中，一个列族存储经常被一起查询的相关数据。\n举个例子，如果我们有一个Person类，我们通常会一起查询他们的姓名和年龄而不是薪资。这种情况下，姓名和年龄就会被放入一个列族中，而薪资则在另一个列族中。\n产品：Cassandra、HBase\n有谁在使用：Ebay （Cassandra）、Instagram （Cassandra）、NASA （Cassandra）、Twitter （Cassandra and HBase）、Facebook （HBase）、Yahoo!（HBase）\n适用的场景\n日志。因为我们可以将数据储存在不同的列中，每个应用程序可以将信息写入自己的列族中。\n博客平台。我们储存每个信息到不同的列族中。举个例子，标签可以储存在一个，类别可以在一个，而文章则在另一个。\n不适用场景\n如果我们需要ACID事务。Vassandra就不支持事务。\n原型设计。如果我们分析Cassandra的数据结构，我们就会发现结构是基于我们期望的数据查询方式而定。在模型设计之初，我们根本不可能去预测它的查询方式，而一旦查询方式改变，我们就必须重新设计列族。\n四、 图（Graph-Oriented）数据库\n概述：图数据库允许我们将数据以图的方式储存。实体会被作为顶点，而实体之间的关系则会被作为边。\n比如我们有三个实体，Steve Jobs、Apple和Next，则会有两个“Founded by”的边将Apple和Next连接到Steve Jobs。\n产品：Neo4J、Infinite Graph、OrientDB\nNoSQL 不适合处理多对多的场景，SQL 不适合节点的数据类型多变，且不支持递归的关系查找，关系也没有属性附着\n根据用途分类：\n事务处理数据库（OLTP）：\n\n用于处理日常的交易和操作，例如在线交易处理、电子商务等。\n需要快速读写、维护数据的一致性和完整性。\n常见的关系型数据库常用于 OLTP。\n\n在线分析处理数据库（OLAP）：\n\n用于分析大量数据，支持复杂的查询和数据分析。\n重视对数据的高效读取和汇总，用于决策支持和报告。\n数据仓库和一些大数据平台通常用于 OLAP。\n\nOLTP vs OLAP\nMysql\n\nMySQL 的架构共分为两层：Server 层和存储引擎层，\n\nServer 层负责建立连接、分析和执行 SQL。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。\n存储引擎层负责数据的存储和提取。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。\n\n执行一条 SQL 查询语句，期间发生了什么？\n\n连接器：建立连接，管理连接、校验用户身份；\n查询缓存：查询语句如果命中查询缓存则直接返回，否则继续往下执行。MySQL 8.0 已删除该模块；\n解析 SQL，通过解析器对 SQL 查询语句进行词法分析、语法分析，然后构建语法树，方便后续模块读取表名、字段、语句类型；\n执行 SQL：执行 SQL 共有三个阶段：\n\n预处理阶段：检查表或字段是否存在；将 select * 中的 `` 符号扩展为表上的所有列。\n优化阶段：基于查询成本的考虑， 选择查询成本最小的执行计划；\n执行阶段：根据执行计划执行 SQL 查询语句，从存储引擎读取记录，返回给客户端；\n\n\n\n连接的过程需要先经过 TCP 三次握手，因为 MySQL 是基于 TCP 协议进行传输的\n\n怎么解决长连接占用内存的问题？\n\n有两种解决方式。\n第一种，定期断开长连接。既然断开连接后就会释放连接占用的内存资源，那么我们可以定期断开长连接。\n第二种，客户端主动重置连接。MySQL 5.7 版本实现了 mysql_reset_connection() 函数的接口，注意这是接口函数不是命令，那么当客户端执行了一个很大的操作后，在代码里调用 mysql_reset_connection 函数来重置连接，达到释放内存的效果。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。\nInnoDB 的架构\n\n1. 入口点：Handler API\n\n这是应用程序与 InnoDB 存储引擎交互的接口。SQL 语句（如选择，插入，更新，删除) 通过此 API 进行处理。\n\n2.执行引擎：\n\nMTR1（阅读视图）： 管理事务的一致读取视图，确保每个事务都能看到数据库的一致快照。\n**MTR2（交易）：**处理事务管理，包括启动、提交和回滚事务。\n**命令：**表示要执行的已解析 SQL 语句（例如从用户中选择 *）。\n\n3.缓冲池（内存中）：\n\n**InnoDB的核心：**一个大型内存区域，用于缓存数据库文件中经常访问的数据页。这可最大限度地减少磁盘 I/O 并显著提高性能。\nLRU 列表： 实施最近最少使用 (LRU) 算法来管理缓存页面。经常使用的页面保留在内存中，而较少使用的页面则被逐出，为新数据腾出空间。\n**空闲列表：**跟踪缓冲池中可用于新数据的页面。\n冲洗清单： 保存需要写回磁盘的页面（由于修改）。\n自适应哈希索引： 通过在哈希表中缓存经常访问的索引条目来进一步加快索引查找速度。\n\n4. 更换缓冲液：\n\n优化写入： 内存中的一种特殊数据结构，用于临时存储对二级索引的更改。InnoDB 不会立即将这些更改写入磁盘，而是将它们缓冲在更改缓冲区中。这减少了磁盘 I/O，尤其是对于写入密集型工作负载。\n合并： 更改缓冲区会定期与磁盘上的二级索引合并，从而使更改持久化。\n\n5. 撤消缓冲区：\n\n**支持回滚：**存储有关以前版本数据的信息，允许 InnoDB 撤消事务并提供一致的读取视图。\n\n6.重做日志缓冲区：\n\n**预写日志：**内存缓冲区，用于记录对数据库所做的所有更改，在将其写入磁盘之前。这可确保数据持久性和崩溃恢复。\n\n7.磁盘存储：\n\n**InnoDB 表空间：**InnoDB 将数据和索引存储在表空间文件中。\n\n**系统表空间（.ibd）：**保存数据字典信息，如果配置的话可能包含用户表。\n每个表的文件表空间 (.ibd)： 每个表都有自己的.ibd 文件，提高了可管理性。\n**通用表空间（diy.ibd）：**允许您创建共享表空间来存储多个表。\n临时表空间（ibtmp1）： 用于临时表和内部操作。\n\n\n双写缓冲区： 一种有助于防止崩溃恢复期间数据损坏的机制。更改首先写入磁盘上的双写缓冲区，然后写入实际数据文件。\n撤消表空间 (.mother)： 将撤消信息存储在磁盘上。\n重做日志（ib_logfile[0/1]）： 重做日志的持久存储，对于崩溃恢复至关重要。\n\n数据流：\n\nSQL 语句： 应用程序通过 Handler API 发送 SQL 语句。\n解析和执行： 执行引擎处理该语句。\n缓冲池： InnoDB 检查所需数据是否已在缓冲池中。如果没有，它将数据从磁盘读入缓冲池。\n变化： 如果该语句修改了数据，则更改将应用于缓冲池，记录在重做日志缓冲区中，并可能缓冲在更改缓冲区中。\n重做日志： 重做日志缓冲区会定期刷新到磁盘上的重做日志文件。\n检查点： InnoDB 执行检查点，将修改后的页面从缓冲池写入磁盘。\n耐用性： 重做日志确保所有已提交的更改都安全地保存在磁盘上，即使发生崩溃。\n\n该架构突出了 InnoDB 的主要特性，如 ACID 属性（原子性、一致性、隔离性、持久性）、崩溃恢复以及通过缓冲和日志记录实现的性能优化。\n查询语句的那一套流程，更新语句也是同样会走一遍：\n\n客户端先通过连接器建立连接，连接器自会判断用户身份；\n因为这是一条 update 语句，所以不需要经过查询缓存，但是表上有更新语句，是会把整个表的查询缓存清空的，所以说查询缓存很鸡肋，在 MySQL 8.0 就被移除这个功能了；\n解析器会通过词法分析识别出关键字 update，表名等等，构建出语法树，接着还会做语法分析，判断输入的语句是否符合 MySQL 语法；\n预处理器会判断表和字段是否存在；\n优化器确定执行计划，因为 where 条件中的 id 是主键索引，所以决定要使用 id 这个索引；\nHandler API到执行器负责具体执行，找到这一行，然后更新。\n\n不过，更新语句的流程会涉及到 undo log（回滚日志）、redo log（重做日志） 、binlog （归档日志）这三种日志：\n\nundo log（回滚日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和 MVCC。\nredo log（重做日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的持久性，主要用于掉电等故障恢复；\nbinlog （归档日志）：是 Server 层生成的日志，主要用于数据备份和主从复制；\n\nundo log 和 redo log 用于记录事务对数据的修改，以保证原子性和持久性，而 binlog 用于记录数据库的整体变更，MVCC 则用于控制并发，确保读写之间的隔离性。这些机制相互协作，确保数据库在事务执行和并发访问时的一致性和可靠性。\nCREATE INDEX ,ALTER TABLE\n最基本的分页方式：\nSELECT ... FROM ... WHERE ... ORDER BY ... LIMIT ...\n在中小数据量的情况下，这样的 SQL 足够用了，唯一需要注意的问题就是确保使用了索引。\n举例来说，如果实际 SQL 类似下面语句，那么在 category_id, id 两列上建立复合索引比较好。\nSELECT * FROM articles WHERE category_id = 123 ORDER BY id LIMIT 50, 10\n\n分解大连接查询\n将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联，这样做的好处有：\n\n缓存之所以能够大幅提高系统的性能，关键在于数据的访问具有局部性，也就是二八定律：「百分之八十的数据访问是集中在 20% 的数据上」。这部分数据也被叫做热点数据。\n让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。\n分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。\n减少锁竞争；\n在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可伸缩。\n查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。\nOptimization\nhttps://developer.aliyun.com/article/1497219\n在实际场景中进行 MySQL 查询优化时，通常需要根据具体的业务需求和数据库环境采取针对性的措施。以下是一个系统化的优化步骤和方法：\n\n了解业务需求与数据特点\n\n明确查询的业务需求：了解查询的具体业务需求，包括查询的频率、重要性、响应时间要求等。\n数据分布与访问模式：分析数据的分布特点（如数据量、表结构、索引情况）和访问模式（读多写少还是读写并重，随机访问还是顺序访问）。\n\n分析现有查询性能\n\n使用 EXPLAIN 分析查询计划：通过 EXPLAIN 了解查询的执行计划，判断查询是否合理利用了索引，以及优化器whether正确连接表，是否存在全表扫描、文件排序等性能瓶颈。\n慢查询日志：启用 MySQL 慢查询日志，找出执行时间较长的查询，并集中优化这些查询。mysqldumpslow -s t /path/to/slow.log\n查询执行时间：使用 SHOW PROFILES 或 SHOW STATUS 查看查询的执行时间、CPU 使用率、磁盘 IO 等性能指标。\n\n索引优化\n\n创建适当的索引：为查询中的 WHERE、JOIN、ORDER BY、GROUP BY 等条件创建索引，特别是选择性高的列。\n避免冗余索引：检查并删除冗余或未被使用的索引，以减少维护开销。\n覆盖索引：通过覆盖索引（即查询字段都在同一索引中）来减少查询的回表操作。\n\n查询语句优化\n\n优化 JOIN 查询：避免在连接条件中使用函数或计算，尽量使用等值连接（INNER JOIN）而非笛卡尔积（CROSS JOIN）。\n减少返回的数据量：避免 SELECT *，只选择需要的列；使用 LIMIT 限制返回的行数。\n子查询优化：将复杂的子查询改写为 JOIN 或使用 EXISTS，避免嵌套子查询带来的性能问题。\n\n表结构优化\n\n规范化与反规范化：根据查询模式适当选择规范化或反规范化策略，减少表之间的连接次数或查询的复杂度。\n分区表：对大表进行分区，如按时间或范围分区，可以加快查询性能。\n优化表的数据类型：选择更合适的字段类型，避免使用过大或不必要的精度，如用 TINYINT 而不是 INT。\n\n利用缓存与存储过程\n\n缓存热点数据：将高频访问的数据缓存到内存中（如使用 Redis），减少数据库查询次数。\n预计算与存储过程：对于计算复杂且频繁的查询，可以通过预计算将结果缓存到表中，或者使用存储过程加快查询速度。\n\n事务与锁优化\n\n合理管理事务：避免长事务或在事务中执行大量查询，尽量缩短事务的执行时间。\n减少锁定范围：使用合适的锁级别（如行级锁而非表级锁），减少锁争用，提高并发性能。\n\n数据维护与归档\n\n定期归档历史数据：将不常用的历史数据迁移到冷存储或归档表，减小主表的体积，提升查询性能。\n碎片整理：定期执行 OPTIMIZE TABLE，整理表碎片，特别是对于频繁更新、删除的表。\n\n监控与持续优化\n\n监控查询性能：使用监控工具（如 MySQL Enterprise Monitor、Percona Toolkit、Prometheus 等）持续监控查询性能，发现并优化潜在的性能瓶颈。\n调整数据库参数：根据监控结果调整 MySQL 的配置参数（如 query_cache_size、innodb_buffer_pool_size），优化数据库整体性能。\n实际案例分析\n案例1：优化电商系统中的订单查询\n场景描述：某电商系统中的订单表非常大，用户经常查询自己最近一年的订单。\n优化方法：\n\n分区表：按年份或月份对订单表进行分区，查询最近一年订单时，只扫描相关分区。\n覆盖索引：在订单表的 user_id、order_date 上建立联合索引，避免回表。\n缓存：将最近一年的订单数据缓存到 Redis 中，减少数据库查询。\n\nCREATE TABLE orders (    order_id INT PRIMARY KEY,    user_id INT,    order_date DATE,    order_amount DECIMAL(10, 2),    product_id INT);你有一个查询需要经常获取特定用户在某个日期范围内的订单信息，例如：在没有索引的情况下，库需要对 orders 表进行**全表扫描**，查找符合条件的记录。这可能会导致查询速度变慢，尤其是在订单表数据量非常大的情况下。CREATE INDEX idx_user_order_date ON orders(user_id, order_date);\n案例2：优化社交平台中的用户动态查询\n场景描述：社交平台用户的动态查询经常发生，涉及多表连接，如用户信息表、动态表、评论表等。\n优化方法：\n\n拆分连接查询：将用户信息与动态内容的查询分开，先查询用户信息表，再通过 IN() 方式获取动态内容。\n水平拆分：将动态表按用户 ID 或时间进行水平拆分，减少单表数据量，提升查询效率。\n缓存用户信息：将用户信息缓存到 Redis 或内存中，减少数据库查询。\n\nCREATE TABLE comments (    comment_id INT PRIMARY KEY,    post_id INT,    user_id INT,    comment_content TEXT,    comment_date DATETIME);假设我们需要查询某个用户的所有动态及其对应的评论，通常会使用如下查询：SELECT u.username, p.post_content, c.comment_contentFROM users uJOIN posts p ON u.user_id = p.user_idLEFT JOIN comments c ON p.post_id = c.post_idWHERE u.user_id = 12345ORDER BY p.post_date DESC;为了优化上述查询，我们可以在相关表上创建合适的索引。1.在 posts 表上创建联合索引，优化 user_id 和 post_date 的查询和排序：CREATE INDEX idx_user_post_date ON posts(user_id, post_date DESC);在 comments 表上创建索引以优化 post_id 的连接查询：CREATE INDEX idx_post_id ON comments(post_id);2.覆盖索引CREATE INDEX idx_user_post_content ON posts(user_id, post_date DESC, post_content);\ncase 3:以%开头的 LIKE 查询会导致索引失效\n-- 原始查询SELECT * FROM products WHERE product_name LIKE 'abc%';-- 优化后的查询SELECT * FROM products WHERE product_name &gt;= 'abc' AND product_name &lt; 'abd'; or 为文本列添加全文索引\n通过以上的步骤和方法，你可以在实际场景中系统地优化 MySQL 查询，提升数据库的响应速度和系统的整体性能。\n索引\n数据库索引是对数据库表的一列或者多列的值进行排序一种结构，使用索引可以快速访问数据表中的特定信息。\n索引的优缺点？\n优点：\n\n大大加快数据检索的速度。\n将随机I/O变成顺序I/O(因为B+树的叶子节点是连接在一起的)\n\n缺点：\n\n从空间角度考虑，建立索引需要占用物理空间\n从时间角度 考虑，创建和维护索引都需要花费时间，例如对数据进行增删改的时候都需要维护索引。\n\n索引的实现通常使用Hash索引和B+树（MySQL常用的索引就是B+树）。除了数据之外，数据库系统还维护为满足特定查找算法的数据结构，这些数据结构以某种方式引用数据，这种数据结构就是索引。简言之，索引就类似于书本，字典的目录。以下是一些关键概念和要点与数据库索引相关：\n\n索引类型：\n\nB树索引：是最常见的索引类型，适用于大多数数据库系统。B树索引在树结构中进行数据划分，使得在平均情况下，检索时间复杂度为O(log n)。\n哈希索引：基于哈希算法，适用于等值查询。但是，哈希索引不适用于范围查询，且不适合于有序数据。\n全文索引：用于文本数据的高效搜索，支持关键词搜索、模糊查询等。\n空间索引：用于地理空间数据的索引，可以支持地理坐标查询和范围查询。\n\n\n\n什么时候不需要创建索引？\n\n只读或极少写的表（如配置表、代码表），很少被查询的信息（log\nWHERE 条件，GROUP BY，ORDER BY 里用不到的字段，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用物理空间的。\n字段中存在大量重复数据，不需要创建索引，比如性别字段，只有男女，如果数据库表中，男女的记录分布均匀，那么无论搜索哪个值都可能得到一半的数据。在这些情况下，还不如不要索引，因为 MySQL 还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描。\n表数据太少的时候，不需要创建索引；\n经常更新的字段不用创建索引，比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于要维护 B+Tree的有序性，那么就需要频繁的重建索引，这个过程是会影响数据库性能的。\n\nMySQL 索引\n索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现。InnoDB将磁盘数据建立索引并储存在B+树中\n\nB+Tree 索引\n是大多数 MySQL 存储引擎的默认索引类型。\n\nB树是一种多路平衡查找树\nB+ 树查询效率更稳定。因为 B+ 树每次只有访问到叶子节点才能找到对应的数据，而在 B 树中，非叶子节点也会存储数据，这样就会造成查询效率不稳定的情况，有时候访问到了非叶子节点就可以找到关键字，而有时需要访问到叶子节点才能找到关键字。\n其次，B+ 树的查询效率更高，这是因为通常 B+ 树比 B 树更矮胖（阶数更大，深度更低），查询所需要的磁盘 I/O 也会更少。同样的磁盘页大小，B+ 树可以存储更多的节点关键字。\n**B+Tree 只在叶子节点存储数据，而 B 树 的非叶子节点也要存储数据，**所以 B+Tree 的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点。 另外，B+Tree 叶子节点采用的是双链表连接，适合 MySQL 中常见的基于范围的顺序查找，而 B 树无法做到这一点。\n2、B+Tree vs Hash， Hash 在做等值查询的时候效率贼快，搜索复杂度为 O(1)。 但是 Hash 表不适合做范围查询，它更适合做等值的查询，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因。\n红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，这是因为使用 B+ 树访问磁盘数据有更高的性能。\nWhy innodb use b+ tree as index?\n\n适合范围查询和范围扫描：B+ 树索引适用于范围查询，例如 BETWEEN、&lt;、&gt; 等操作。B+ 树的特性使得在索引中查找范围更为高效。\n有序性：B+ 树是一种有序树结构，叶子节点形成了一个有序链表。这使得范围扫描变得更加高效，因为它们能够顺序访问索引页。\n减少磁盘 I/O：B+ 树索引的叶子节点通常构成一个链表，这有助于减少范围查询时需要读取的磁盘页数量。另外，由于索引节点存储了更多的键值对，相较于其他树结构（如 B 树），在内存中更有可能读取到所需的节点。\n支持聚集索引：在 InnoDB 中，主键索引（如果定义了主键）被称为聚集索引，它决定了数据的物理存储顺序。B+ 树作为主键索引提供了对表数据物理存储顺序的优化。\n范围分裂：B+ 树的分裂不会影响叶子节点，只有非叶子节点才会发生分裂。这有助于保持叶子节点的紧凑性，避免了频繁的平衡操作，提高了性能。\n支持多版本并发控制（MVCC）：InnoDB 使用了 MVCC（多版本并发控制）来处理事务隔离性，而 B+ 树索引能够有效地支持这种并发控制模型。\n\n二叉搜索树查询效率无疑是最高的，因为平均来说每次比较都能缩小一半的搜索范围  所以表面上来看我们使用 B、B+ 树没有 二叉查找树效率高，但是实际上由于 B、B+ 树降低了树高，减少了磁盘 IO 次数，反而大大提升了速度。\n\n聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据 聚簇索引的叶子节点就是实际的数据行\n非聚簇索引：一个表可以有多个非聚簇索引，这些索引在逻辑上只是指向数据行的指针。 将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因\n\n区别就在于叶子节点存放的是什么数据：\n\n聚簇索引的叶子节点存放的是实际数据，所有完整的用户记录都存放在聚簇索引的叶子节点；\n二级索引的叶子节点存放的是主键值，而不是实际数据。\n\n如果某个查询语句使用了二级索引，但是查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获得数据行，这个过程就叫作「回表」，也就是说要查两个 B+ 树才能查到数据。不过，当查询的数据是主键值时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作「索引覆盖」，也就是只需要查一个 B+ 树就能找到数据。\n什么是覆盖索引和索引下推？\n覆盖索引，也就是 covering index 。 指的是一个查询语句的执行只用从索引中就能获取到目标数据，不必从数据表中读取。\n\n覆盖索引: 当查询所需的全部列数据都能在索引中找到时，我们称之为覆盖索引。这意味着查询可以直接从索引中获取结果，无需回表查询主键索引。\n\nCREATE INDEX idx_name_age ON users (name, age);\n在这个例子中，我们创建了一个名为 idx_name_age 的覆盖索引，它包含了 name 和 age 两个列。如果查询需要根据 name 和 age 来查找用户，那么这个索引就可以覆盖查询，无需访问表本身。\n查询时使用覆盖索引:\n当查询条件能够通过覆盖索引找到所需数据时，MySQL 会自动使用覆盖索引。\n示例查询:\nSELECT name, age FROM users WHERE name = 'John' AND age = 30;\n在这个例子中，如果 idx_name_age 索引存在，并且包含了 name 和 age 的值，那么 MySQL 会使用覆盖索引来执行查询，直接返回结果，而无需访问表本身\nMysql5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。\n现在有下面这条查询语句：\nselect * from t_user  where age &gt; 20 and reward = 100000;\n联合索引当遇到范围查询 (&gt;、&lt;) 就会停止匹配，也就是 age 字段能用到联合索引，但是 reward 字段则无法利用到索引。具体原因这里可以看这篇：索引常见面试题(opens new window)\n那么，不使用索引下推（MySQL 5.6 之前的版本）时，执行器与存储引擎的执行流程是这样的：\n\nServer 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age &gt; 20 的第一条记录；\n存储引擎根据二级索引的 B+ 树快速定位到这条记录后，获取主键值，然后进行回表操作，将完整的记录返回给 Server 层；\nServer 层在判断该记录的 reward 是否等于 100000，如果成立则将其发送给客户端；否则跳过该记录；\n接着，继续向存储引擎索要下一条记录，存储引擎在二级索引定位到记录后，获取主键值，然后回表操作，将完整的记录返回给 Server 层；\n如此往复，直到存储引擎把表中的所有记录读完。\n\n可以看到，没有索引下推的时候，每查询到一条二级索引记录，都要进行回表操作，然后将记录返回给 Server，接着 Server 再判断该记录的 reward 是否等于 100000。而使用索引下推后，判断记录的 reward 是否等于 100000 的工作交给了存储引擎层，过程如下 ：\n\nServer 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age &gt; 20 的第一条记录；\n存储引擎定位到二级索引后，先不执行回表操作，而是先判断一下该索引中包含的列（reward列）的条件（reward 是否等于 100000）是否成立。如果条件不成立，则直接跳过该二级索引。如果成立，则执行回表操作，将完成记录返回给 Server 层。\nServer 层在判断其他的查询条件（本次查询没有其他条件）是否成立，如果成立则将其发送给客户端；否则跳过该记录，然后向存储引擎索要下一条记录。\n如此往复，直到存储引擎把表中的所有记录读完。\n\n可以看到，使用了索引下推后，虽然 reward 列无法使用到联合索引，但是因为它包含在联合索引（age，reward）里，所以直接在存储引擎过滤出满足 reward = 100000 的记录后，才去执行回表操作获取整个记录。相比于没有使用索引下推，节省了很多回表操作。\nInnoDB以页面（Page）为基本单位来组织和管理数据。这些页面是数据库中数据和索引的物理存储单元。\n以下是 InnoDB 页面的一些关键特点：\n\n固定大小的页：InnoDB 使用固定大小的数据页（通常是默认大小为 16KB）。每个页都有一个地址和唯一的页号，并用于存储数据、索引、事务日志等信息。\n页类型：InnoDB 中有不同类型的页，如数据页（Data Page）、索引页（Index Page）、Undo 页（Undo Page）、事务日志页（Log Page）等。每种类型的页用于存储不同的信息。\n数据页：主要用于存储表的数据。数据页中包含了行记录以及一些管理信息，例如行格式、行的版本信息（用于 MVCC）等。\n索引页：用于存储索引的节点信息，B+ 树结构中的分支节点和叶子节点。\n管理和缓存：InnoDB 通过缓冲池（Buffer Pool）来管理数据页的内存缓存。数据页在内存中进行读取和写入，以提高访问速度。LRU（Least Recently Used）算法用于管理缓冲池中的页。\n\nExplain查询创建的索引是否真正有效\n\nThe EXPLAIN statement in MySQL provides insight into how MySQL executes a query, specifically detailing how tables are joined. The type column in the EXPLAIN output describes the join method used, which indicates the efficiency of the query. Here’s a summary of the join types listed from best to worst:\n\nsystem\n\nDescription: The table has only one row, making it a “system table.” This is a special case of the const join type.\nEfficiency: Fastest, as there’s only one row to retrieve.\n\nconst\n\nDescription: The table has at most one matching row, which is read at the start of the query. The result is treated as a constant by the optimizer.\nUsage: Typically used when querying a table with a primary key or a unique index where the result is a single row.\nExample:\nSELECT * FROM tbl_name WHERE primary_key = 1;\n\neq_ref\n\nDescription: For each row from the previous tables, exactly one row is read from this table. This type is used when all parts of a primary key or unique index are used.\nUsage: Used in join queries where the indexed column is compared with a constant or an expression involving columns from previously read tables.\nExample:\nSELECT * FROM ref_table, other_table WHERE ref_table.key_column = other_table.column;\n\nref\n\nDescription: All rows with matching index values are read from this table for each combination of rows from the previous tables. This type is used when the join uses only a leftmost prefix of the key or if the key is not a primary key or unique index.\nUsage: Applicable when there are multiple rows that match the join condition.\nExample:\nSELECT * FROM ref_table WHERE key_column = expr;\n\nfulltext\n\nDescription: The join is performed using a FULLTEXT index.\nUsage: Used when a FULLTEXT index is involved in the query.\nExample:\nSELECT * FROM articles WHERE MATCH (title, body) AGAINST ('text');\n\nref_or_null\n\nDescription: Similar to ref, but includes an additional search for NULL values. Often used in queries that involve OR conditions with NULL.\nUsage: When the query checks for both specific values and NULL in the indexed column.\nExample:\nSELECT * FROM ref_table WHERE key_column = expr OR key_column IS NULL;\n\nindex_merge\n\nDescription: Indicates that the Index Merge optimization is used, where multiple indexes are used to satisfy the query.\nUsage: When the query can use more than one index.\nExample:\nSELECT * FROM tbl_name WHERE key1 = 10 OR key2 = 20;\n\nunique_subquery\n\nDescription: Replaces eq_ref for some IN subqueries. It functions as an index lookup and replaces the subquery for efficiency.\nUsage: For efficient IN subqueries.\nExample:\nSELECT * FROM tbl_name WHERE id IN (SELECT primary_key FROM single_table WHERE condition);\n\nindex_subquery\n\nDescription: Similar to unique_subquery, but works for non-unique indexes.\nUsage: For IN subqueries involving non-unique indexes.\nExample:\nSELECT * FROM tbl_name WHERE id IN (SELECT key_column FROM single_table WHERE condition);\n\nrange\n\nDescription: Only rows within a certain range are retrieved using an index. The index allows for range scans.\nUsage: When the query involves conditions that limit the result set to a specific range.\nExample:\nSELECT * FROM tbl_name WHERE key_column BETWEEN 10 AND 20;\n\nindex\n\nDescription: Similar to ALL, but instead of scanning the entire table, only the index is scanned. It’s used when the query can be satisfied by the index alone.\nUsage: When the index is covering (contains all needed columns).\nExample:\nSELECT indexed_column FROM tbl_name;\n\nALL\n\n\n\nDescription: A full table scan is performed for each combination of rows from the previous tables. This is generally the least efficient join type.\n\n\nUsage: When no indexes are available, or the query optimizer determines that a full table scan is faster.\n\n\nExample:\n  SELECT * FROM tbl_name;\n\n\nBest Types: system, const, eq_ref are the most efficient, meaning they minimize the number of rows scanned.\n\n\nGood Types: ref, range, index_merge are generally efficient, depending on the specific query and index design.\n\n\nLess Optimal Types: index, ALL are less efficient, as they may involve scanning large portions of the table.\n\n\nUnderstanding and optimizing these join types is key to improving query performance in MySQL.\nRedis\nRedis（Remote Dictionary Server )，即远程字典服务。C语言开发的一个开源的（遵从BSD协议）高性能键值对（key-value）的内存数据库，可以用作数据库、缓存、消息中间件等。它是一种NoSQL（not-only sql，泛指非关系型数据库）的数据库。\nRedis的基本概念和用法：\n\n高性能：Redis完全存储在内存中，这使得它能够以非常高的速度执行读写操作。\n多种数据结构：除了键值对，Redis还支持字符串、哈希表、列表、集合、有序集合等多种数据结构，允许更灵活的数据操作。\n持久化：Redis支持将数据持久化到硬盘，以便在重启后恢复数据。它提供了两种持久化方式：快照（Snapshotting）和AOF（Append-Only File）。\n发布/订阅：Redis提供了发布（publish）和订阅（subscribe）的功能，允许不同部分之间通过消息进行通信。\n事务：Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化，Redis 在事务失败时不进行回滚，而是继续执行余下的命令。Redis官方认为事务是原子性的：所有的命令，要么全部执行（但不一定成功），要么全部不执行。\n缓存：Redis常用于作为缓存层，将频繁访问的数据存储在内存中，以提高读取性能。\n计数器和排行榜：Redis适用于计数器和排行榜等需要快速处理增减操作的场景。\n分布式锁：Redis可以用来实现分布式锁，以确保在多个节点上的操作不会互相干扰。\n消息队列：使用Redis的列表数据结构，可以实现简单的消息队列，用于异步处理任务。\n\nRedis这么快？\n\n第一：Redis完全基于内存，绝大部分请求是纯粹的内存操作，非常迅速，数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度是O(1)。第二：数据结构简单，对数据操作也简单。第三：采用单线程，避免了不必要的上下文切换和竞争条件，不存在多线程导致的CPU切换，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有死锁问题导致的性能消耗。虽然 Redis 的主要工作（网络 I/O 和执行命令）一直是单线程模型，但是在 Redis 6.0 版本之后，也采用了多个 I/O 线程来处理网络请求，这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 I/O 的处理上。\n\n第四：使用多路复用IO模型，非阻塞IO。\nString、Hash、List、Set、SortedSet。\n\nHashes: Hashes are maps between string fields and string values. They are useful for representing objects with multiple fields, such as user profiles.\nString data up to 512 MB 。String类型是二进制安全的(其本质是将操作输入作为原始的、无任何特殊字符意义的数据流)，意思是 redis 的 string 可以包含任何数据。如数字，字符串，jpg图片或者序列化的对象。\nLists: 双向链表实现Lists are ordered collections of strings. You can add elements to the head (left) or tail (right) of a list. Lists can be used for task queues, message queues, and more.\nSets: Sets are unordered collections of unique strings. They are used for storing unique values. You can perform set operations like union, intersection, and difference.\nSorted Sets (ZSETs): Similar to sets, but each member has a score associated with it. Sorted sets are used for tasks that require ordering by score, such as leaderboards.\nBitmaps: Bitmaps are used for bit-level operations. They are often used to represent sets with a large number of unique items in a very memory-efficient way.\n\n\nRedis 的字符串表示为 sds(simple dynamic string) ，而不是 C 字符串（以 \\0 结尾的 char*）。\n对比 C 字符串， sds 有以下特性：\n\n可以高效地执行长度计算（strlen）；\n可以高效地执行追加操作（append）；\n二进制安全；\n\n\nsds 会为追加操作进行优化：加快追加操作的速度，并降低内存分配的次数，代价是多占用了一些内存，而且这些内存不会被主动释放。\n\nstruct sdshdr{\nint len;//buf数组中已经使用的字节的数量，也就是SDS字符串长度\nint  free;//buf数组中未使用的字节的数量\nchar buf[];//字节数组，字符串就保存在这里面\n};\nRedis需要对数据设置过期时间，并采用的是惰性删除+定期删除两种策略对过期键删除。Redis对过期键的策略+持久化\n\n数据过期清除策略：\n\n定期删除：Redis会定期（默认每隔100ms）随机抽取一些设置了过期时间TTL 的键，检查其是否过期，如果有过期的键就删除。这个策略确实是为了避免全局扫描而引入的，以减少CPU负载。\n惰性删除：当你尝试访问某个键时，Redis会检查该键是否已过期，如果已过期，则删除它。这个策略确保在获取键的时候处理过期数据，称为\"惰性\"，因为它不主动扫描键。\n\n\n内存淘汰策略：\n\nRedis是一个内存数据库，当内存用尽时，需要根据一定的策略来释放一些键以腾出内存空间。这个策略称为内存淘汰策略（Memory Eviction Policy）。\nRedis提供了不同的内存淘汰策略，常见的策略包括：\n\nLRU（Least Recently Used）：删除最近最少使用的键。\nLFU（Least Frequently Used）：删除最不频繁使用的键。\nRandom（随机淘汰）：随机选择一个键进行删除。\nTTL（Time To Live）：根据键的TTL来删除过期的键。\n\n\n你可以根据需要在Redis配置文件中选择适合你的内存淘汰策略，默认使用LRU策略。\n\n\n\n如果缓存数据设置的过期时间是相同的，并且Redis恰好将这部分数据全部删光了。这就会导致在这段时间内，这些缓存同时失效，全部请求到数据库中。\n跳表是可以实现二分查找的有序链表。\n\nMulti开始事务。\n命令入队。\nExec执行事务。\n\n持久化\nReids 是先执行写操作命令后，才将该命令记录到 AOF 日志里的，这么做其实有两个好处。\n\n避免额外的检查开销：因为如果先将写操作命令记录到 AOF 日志里，再执行该命令的话，如果当前的命令语法有问题，那么如果不进行命令语法检查，该错误的命令记录到 AOF 日志里后，Redis 在使用日志恢复数据时，就可能会出错。\n不会阻塞当前写操作命令的执行：因为当写操作命令执行成功后，才会将命令记录到 AOF 日志。\n\nRDB 快照：将某一时刻的内存数据，以二进制的方式写入磁盘\n所以，RDB 快照就是记录某一个瞬间的内存数据，记录的是实际数据   因此在 Redis 恢复数据时， RDB 恢复数据的效率会比 AOF 高些，因为直接将 RDB 文件读入内存就可以，不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据。\n\nRDB 做快照时会阻塞线程吗？\n\nRedis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，他们的区别就在于是否在「主线程」里执行：\n\n执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，会阻塞主线程；\n执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以避免主线程的阻塞；\n\nRedis 还可以通过配置文件的选项来实现每隔一段时间自动执行一次 bgsave 命令，默认会提供以下配置：\nsave 900 1 save 300 10 save 60 10000\n别看选项名叫 save，实际上执行的是 bgsave 命令，也就是会创建子进程来生成 RDB 快照文件。 只要满足上面条件的任意一个，就会执行 bgsave，它们的意思分别是：\n\n900 秒之内，对数据库进行了至少 1 次修改；\n300 秒之内，对数据库进行了至少 10 次修改；\n60 秒之内，对数据库进行了至少 10000 次修改。\n\n这里提一点，Redis 的快照是全量快照，也就是说每次执行快照，都是把内存中的「所有数据」都记录到磁盘中。所以执行快照是一个比较重的操作，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多。\nAOF 是一种持久化方式，它通过将 Redis 所有的写操作（包括写命令）以追加（append）的方式记录到一个文件中。AOF 文件包含了可以重放（replay）重建数据集的写命令序列，因此，它记录了数据库状态的完整变更历史。\n缺点：\n文件较大：由于记录了所有写操作，AOF 文件可能会比 RDB 文件更大。\n恢复速度相对较慢：在恢复时，可能会因为需要重新执行大量写操作而导致恢复速度较慢。\n缓存\n缓存雪崩 Cache avalanche\n\nRedis挂掉了，请求全部走数据库。\n对缓存数据设置相同的过期时间，导致某段时间内缓存失效，请求全部走数据库。\n\n缓存雪崩如果发生了，很可能就把我们的数据库搞垮，导致整个服务瘫痪！\n如何解决缓存雪崩？\n\n解决方法：考虑用加锁或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上；还有一个解决方案，原有的失效时间基础上增加一个随机值，这样就会大幅度的减少缓存在同一时间过期。\n\n对于“Redis挂掉了，请求全部走数据库”这种情况，我们可以有以下的思路：\n\n事发前：实现Redis的高可用(主从架构+Sentinel 或者Redis Cluster)，尽量避免Redis挂掉这种情况发生。\n事发中：万一Redis真的挂了，我们可以设置本地缓存(ehcache)+限流(hystrix)，尽量避免我们的数据库被干掉(起码能保证我们的服务还是能正常工作的)\n事发后：redis持久化，重启后自动从磁盘上加载数据，快速恢复缓存数据。\n\n缓存击穿   Hotspot data set is invalid\nHowever, for some hot data with very high requests, once the valid time has passed, there will be a large number of requests falling on the database at this moment, which may cause the database to crash.\n应对缓存击穿可以采取前面说到两种方案：\n\n互斥锁方案，保证同一时间只有一个业务线程更新缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。\n不给热点数据设置过期时间，由后台异步更新缓存，或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间；\n\n缓存穿透\n是指查询一个一定不存在的数据。由于缓存不命中，并且出于容错考虑，如果从数据库查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，失去了缓存的意义。\n缓存穿透也有两种方案：\n\n由于请求的参数是不合法的(每次都请求不存在的参数)，于是我们可以使用布隆过滤器(BloomFilter)或者压缩filter提前拦截，不合法就不让这个请求到数据库层！\n当我们从数据库找不到的时候，我们也将这个空对象设置到缓存里边去。下次再请求的时候，就可以从缓存里边获取了。\n这种情况我们一般会将空对象设置一个较短的过期时间。\n\n可以发现缓存击穿跟缓存雪崩很相似，如果缓存中的某个热点数据过期了，此时大量的请求直接访问数据库就是\n缓存与数据库双写一致\n只要我们设置了键的过期时间，我们就能保证缓存和数据库的数据最终是一致的。因为只要缓存数据过期了，就会被删除。随后读的时候，因为缓存里没有，就可以查数据库的数据，然后将数据库查出来的数据写入到缓存中。\n除了设置过期时间，我们还需要做更多的措施来尽量避免数据库与缓存处于不一致的情况发生。分布式环境下非常容易出现缓存和数据库间数据一致性问题，针对这一点，如果项目对缓存的要求是强一致性的，那么就不要使用缓存。我们只能采取合适的策略来降低缓存和数据库间数据不一致的概率，而无法保证两者间的强一致性。合适的策略包括合适的缓存更新策略，更新数据库后及时更新缓存、缓存失败时增加重试机制。\n两种策略各自有优缺点：\n\n先删除缓存，再更新数据库\n在高并发下表现不如意，在原子性被破坏时表现优异\n先更新数据库，再删除缓存(Cache Aside Pattern设计模式)\n\n缓存更新的策略\n\nCache Aside（旁路缓存）策略；\nRead/Write Through（读穿 / 写穿）策略；\nWrite Back（写回）策略；\n\n主要分为两类 Cache-Aside 和 Cache-As-SoR。 SoR 即「System Of Record，记录系统」，表示数据库。 实际开发中，Redis 和 MySQL 的更新策略用的是 Cache Aside，另外两种策略主要应用在计算机系统里\n\nCache Aside 策略：应用程序直接与数据库和缓存交互，并负责维护缓存的一致性。这种策略简单易用，但是需要维护缓存和数据库的一致性，可能出现缓存穿透或缓存雪崩的问题，一般采用延迟双删来保证最终一致性\n\n\n查询：先查询缓存，如果缓存中没有，则查询数据库，并将结果写入缓存\n更新：先更新数据库，然后删除缓存或者更新缓存\n\nCache-As-SoR **可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的Cache。**数据库由缓存代理，缓存未命中时由缓存加载数据库数据然后应用从缓存读，写数据时更新完缓存后同步写数据库。应用只感知缓存而不感知数据库。\n\nRead/Write Through 策略：应用程序只和缓存交互，使用缓存与数据库交互.  better integrity and consistency, less performance\n\n\n查询：先查询缓存，如果缓存中没有，则缓存从数据库中加载数据，并写入缓存\n更新：先更新缓存，再由缓存同步更新数据库\n\n\nWrite Behind 策略：应用程序只和缓存交互。回写式，数据写入缓存即可返回，缓存内部会异步的去更新数据源，这样好处是写操作特别快，因为只需要更新缓存。并且缓存内部可以合并对相同数据项的多次更新，但是带来的问题就是数据不一致，可能发生写丢失。\nRefresh-Ahead 策略：应用程序只和缓存交互，由后台服务与数据库交互\n\n\n查询：只查询缓存\n更新：由后台服务自动从数据库中查询最新的数据，并将数据写入缓存中，\n\n性能\nCache Aside 的性能较高，它只在缓存未命中时才访问数据库\nRead/Write Through 的性能较低，它在每次读写时都需要访问数据库\nWrite Behind Caching 的性能最高，它只在缓存未命中时才访问数据库，而写入操作是异步的\nRefresh-Ahead 的性能介于 Cache Aside 和 Write Behind Caching 之间，它只在即将过期时才访问数据库，并且写入操作也是异步的\n数据一致性\nCache Aside 的数据一致性较低，它只在缓存未命中时才更新缓存，而写入操作则是直接更新数据库，并将缓存中的数据删除或更新\nRead/Write Through 的数据一致性最高，它在每次读写时都更新数据库和缓存\nWrite Behind Caching 的数据一致性最低，它只在缓存未命中时才更新缓存，而写入操作则是先更新缓存，并在异步更新数据库，有较大的延迟。\nRefresh-Ahead 的数据一致性介于 Read/Write Through 和 Cache Aside 之间，它保证了缓存中的数据总是最新的，但是有一定的延迟\nRedis 持久化策略，其实就是为了减少服务宕机后数据丢失，以及快速恢复数据，也算是支持高可用的一种实现。 除此之外，Redis 还提供了其它几种方式来保证**系统高可用，**业务中最常用的莫过于主从同步（也称作主从复制）、Sentinel 哨兵机制以及 Cluster 集群。\n主从复制\nRedis 同时支持主从复制和读写分离：一个 Redis 实例作为主节点 Master，负责写操作。其它实例（可能有 1 或多个）作为从节点 Slave，负责复制主节点的数据。\n主节点 Master 数据更新：Master 负责处理所有的写操作，包括写入、更新和删除等。 数据同步：写操作在 Master 上执行，然后 Master 将写操作的结果同步到所有从节点 Slave 上。 从节点 Slave 数据读取：Slave 负责处理读操作，例如获取数据、查询等。 数据同步：Slave 从 Master 复制数据，并在本地保存一份与主节点相同的数据副本。\n2.2 为什么要读写分离 1）防止并发2）易于扩展 我们都知道，大部分使用 Redis 的业务都是读多写少的。所以，我们可以根据业务量的规模来确定挂载几个从节点 Slave，当缓存数据增大时，我们可以很方便的扩展从节点的数量，实现弹性扩展。 同时，读写分离还可以实现数据备份和负载均衡，从而提高可靠性和性能。 3）高可用保障 不仅如此，Redis 还可以手动切换主从节点，来做故障隔离和恢复。\n主从复制过程\n\n从服务器连接主服务器，发送SYNC命令；主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令；主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令；从服务器收到快照文件后丢弃所有旧数据，载入收到的快照；主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令；从服务器完成对快照的载入，开始接受命令请求，并执行来自主服务器缓冲区的写命令；（从服务器初始化完成）主服务器每执行一个写命令就会向从服务器发送相同的写命令，从服务器接收并执行收到的写命令（从服务器初始化完成后的操作）\n一般主从配置可以缓解请求压力，做读写分离，写服务器不开启持久化，从服务器开启，从服务器还负责读取的操作，而且从服务器可以是多个，可以有效缓解主服务器的压力；但是坏处在于，如果主服务器宕机，无法自动切换恢复；\n\nbgsave 过程中，Redis 依然可以继续处理操作命令的，也就是数据是能被修改的，关键的技术就在于**写时复制技术（Copy-On-Write, COW）。**执行 bgsave 命令的时候，会通过 fork() 创建子进程\n写入时复制（英语：Copy-on-write，简称COW）是一种计算机程序设计领域的优化策略。其核心思想是，如果有多个调用者（callers）同时请求相同资源（如内存或磁盘上的数据存储），他们会共同获取相同的指针指向相同的资源，直到某个调用者试图修改资源的内容时，系统才会真正复制一份专用副本（private copy）给该调用者，而其他调用者所见到的最初的资源仍然保持不变。这过程对其他的调用者都是透明的。此作法主要的优点是如果调用者没有修改该资源，就不会有副本（private copy）被建立，因此多个调用者只是读取操作时可以共享同一份资源。\nRedis的持久化策略\n有两种：1、RDB：快照形式是直接把内存中的数据保存到一个dump的文件中，定时保存，保存策略。2、AOF：把所有的对Redis的服务器进行修改的命令都存到一个文件里，命令的集合。Redis默认是快照RDB的持久化方式。当Redis重启的时候，它会优先使用AOF文件来还原数据集，因为AOF文件保存的数据集通常比RDB文件所保存的数据集更完整。你甚至可以关闭持久化功能，让数据只在服务器运行时存。\n集群原理\n在Redis集群中，所有Redis节点彼此互联，节点内部使用二进制协议优化传输速度和带宽。当一个节点宕机后，集群中超过半数节点检测失效才认为该节点失效。不同于Tomcat集群需要反向代理服务器，Redis集群中任意节点都可以直接和Java客户端连接。Redis集群上的数据分配采用哈希槽。Redis集群中内置了16384个哈希槽，当有数据要存储时，Redis会首先使用CRC16算法对key进行计算，将计算结果对16384取余，这样每个key都会对应一个取值在16384之间的哈希槽。开发者可根据每个Redis实例性能来调整每个Redis实例上哈希槽的分布范围。\nDELETE FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH);\n一般业务刚上线的时候，直接使用单机数据库就够了，但是随着用户量上来之后，系统就面临着大量的写操作和读操作，单机数据库处理能力有限，容易成为系统瓶颈。由于存在读写锁冲突，并且很多大型互联网业务往往读多写少，读操作会首先成为数据库瓶颈，我们希望消除读写锁冲突从而提升数据库整体的读写能力。\n那么就需要采用读写分离的数据库集群方式，一主多从，主库会同步数据到从库。写操作都到主库，读操作都去从库。\n读写分离的主要思想是将数据库的读和写操作分开处理，从而减轻主数据库的负担，提高整个系统的并发能力和性能。\n常见的读写分离架构包括：\n\n主从复制：通过在主数据库上进行写操作，并将写操作同步到多个从数据库（只读副本）。读操作则可以在从数据库上执行，分担了主数据库的读压力。\nSharding分片：按照某种规则（例如按照用户ID、地理位置等）将数据分散存储在不同的数据库节点上，每个节点负责一部分数据。这样可以水平扩展数据库系统，提高了系统整体的读写能力。\n\nMysql表格设计？\n平衡范式与冗余(效率优先；往往牺牲范式)拒绝3B(拒绝大sql语句：big sql、拒绝大事务：big transaction、拒绝大批量：big batch);\n第一范式（1NF）是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性。如果出现重复的属性，就可能需要定义一个新的实体，新的实体由重复的属性构成，新实体与原实体之间为一对多关系。在第一范式（1NF）中表的每一行只包含一个实例的信息。简而言之，第一范式就是无重复的列。\n说明：在任何一个关系数据库中，第一范式（1NF）是对关系模式的基本要求，不满足第一范式（1NF）的数据库就不是关系数据库。\n第一范式存在问题：冗余度大、会引起修改操作的不一致性、数据插入异常、数据删除异常。\n\n冗余度大：\n\n1NF要求每列都是原子性的，这可能导致数据的冗余。当数据重复存储在不同的表中，会增加存储空间，增加了数据不一致性的风险。\n\n\n修改操作的不一致性：\n\n如果某个信息在多个地方存储，当需要修改这个信息时，需要在所有存储位置进行同样的修改。如果有一个位置的信息修改了而其他位置未修改，数据就会不一致。\n\n\n数据插入异常：\n\n数据插入异常指的是由于表中的部分列被留空或无法插入新行，而导致无法插入需要的数据。在1NF中，如果表中的某列是必填项，而其他列不是，当插入新行时，可能会出现无法插入数据的情况。\n\n\n数据删除异常：\n\n当从表中删除数据时，可能会意外删除其他相关数据，从而造成数据丢失。这种情况在存在多个依赖于相同键的表时较为常见。如果删除某个主键相关的信息，可能会导致其他数据不完整或不一致。\n\n\n\n第二范式，强调记录的唯一性约束，数据表必须有一个主键，并且没有包含在主键中的列必须完全依赖于主键，而不能只依赖于主键的一部分。\n举例：\n学生信息（学号，身份证，姓名）学号-》姓名 ，学号，身份证-》姓名 所以姓名部分依赖于（学号，身份证）该表不是2NF。\n2.3 3NF 第三范式\n第三范式，强调数据属性冗余性的约束，也就是非主键列必须直接依赖于主键。也就是消除了非主属性对码的传递函数依赖的2NF。\nBCNF消除主键的某一列会依赖于主键的其他列\n4NF 第四范式\n非主属性不应该有多值。如果有多值就违反了第四范式。4NF是限制关系模式的属性间不允许有非平凡且非函数依赖的多值依赖。\n举例：用户联系方式表(用户id，固定电话，移动电话)，其中用户id是主键，这个满足了BCNF,但是一个用户有可能会有多个固定电话或者多个移动电话，那么这种设计就不合理，应该改为(用户id，联系方式类型，电话号码)。\n说明：如果只考虑函数依赖，关系模式规范化程度最高的范式是BCNF;如果考虑多值依赖则是4NF。\n先建立索引或者分区，然后再查询\n事务\n数据库环境中的事务有两个主要目的：\n\n提供可靠的工作单元，即使在系统发生故障的情况下，也可以从故障中正确恢复并保持数据库的一致性。例如：当执行过早且意外停止（完全或部分）时，在这种情况下，数据库上的许多操作仍未完成，状态不明确。\n\n","tags":["数据库"]},{"title":"Go生态常用库 中间件","url":"/2022/11/17/GoEco/","content":"GIN\n一个高性能的Web框架，具有内置的中间件支持。它提供了路由、中间件、JSON验证和绑定等功能，非常适合用于构建高性能的RESTful API。\nIt features a martini-like API with performance that is up to 40 times faster thanks to httprouter. If you need performance and good productivity, you will love Gin.\n只要你的路由带有参数，并且这个项目的API数目超过了10，就尽量不要使用net/http中默认的路由。在Go开源界应用最广泛的router是httpRouter，很多开源的router框架都是基于httpRouter进行一定程度的改造的成果。关于httpRouter路由的原理，会在本章节的router一节中进行详细的阐释。\nFeatures\nFast\nRadix tree based routing, small memory foot print. No reflection. Predictable API performance.\nMiddleware support\nAn incoming HTTP request can be handled by a chain of middlewares and the final action. For example: Logger, Authorization, GZIP and finally post a message in the DB.\nCrash-free\nGin can catch a panic occurred during a HTTP request and recover it. This way, your server will be always available. As an example - it’s also possible to report this panic to Sentry!\nJSON validation\nGin can parse and validate the JSON of a request - for example, checking the existence of required values.\nRoutes grouping\nOrganize your routes better. Authorization required vs non required, different API versions… In addition, the groups can be nested unlimitedly without degrading performance.\nError management\nGin provides a convenient way to collect all the errors occurred during a HTTP request. Eventually, a middleware can write them to a log file, to a database and send them through the network.\nRendering built-in\nGin provides an easy to use API for JSON, XML and HTML rendering.\n再来回顾一下文章开头说的，开源界有这么几种框架，第一种是对httpRouter进行简单的封装，然后提供定制的中间件和一些简单的小工具集成比如gin，主打轻量，易学，高性能。第二种是借鉴其它语言的编程风格的一些MVC类框架，例如beego，方便从其它语言迁移过来的程序员快速上手，快速开发。还有一些框架功能更为强大，除了数据库schema设计，大部分代码直接生成，例如goa。不管哪种框架，适合开发者背景的就是最好的\n\nroutes group是为了管理一些相同的URL\n\nGin提供了Any方法，可以一次性注册以上这些HTTP Method方法。如果你只想注册其中某两个、或者三个方法，Gin就没有这样的便捷方法了，不过Gin为我们提供了通用的Handle方法，我们可以包装一下使用。\nfuncHandle(r*gin.Engine, httpMethods []string, relativePathstring, handlers...gin.HandlerFunc) gin.IRoutes {var routes gin.IRoutesfor _, httpMethod:=range httpMethods {    routes = r.Handle(httpMethod, relativePath, handlers...)  }return routes}\n如果对于这些请求的URL我们一个个去注册，比如张三用户和李四用户，分别注册一个对应的GET方法，是很繁琐的，所以Gin为我们提供了URL路由的模糊匹配，比如URL路径中的参数\n/users/*id 表示模糊匹配id\n/users/:id这种匹配模式是精确匹配的，只能匹配一个k\n重定向的根本原因在于/users没有匹配的路由，但是有匹配/users/的路由，所以就会被重定向到/users/。得益于gin.RedirectTrailingSlash\n 等于true\nURL获取查询参数\nGetQuery来代替Query方法。\nGetQuery方法的底层实现其实是c.Request.URL.Query().Get(key)，通过url.URL.Query()来获取所有的参数键值对。\nunc (c*Context)GetQueryArray(keystring) ([]string,bool) {  c.getQueryCache()//缓存所有的键值对if values, ok:= c.queryCache[key]; ok&amp;&amp; len(values) &gt; 0 {return values,true  }return []string{},false}func (c*Context)getQueryCache() {if c.queryCache==nil {    c.queryCache = c.Request.URL.Query()  }}\n从以上的实现代码中，可以看到最终的实现都在GetQueryArray方法中，找到对应的key就返回对应的[]string，返回就返回空数组。\n这里Gin进行了优化，通过缓存所有的键值对，提升代码的查询效率。这里缓存的queryCache本质上是url.Values，也是一个map[string][]string。提高了GIN的性能\nGetQuery方法的底层实现其实是c.Request.URL.Query().Get(key)，通过url.URL.Query()\n来获取所有的参数键值对\n!https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ab28a5ec-65a2-4ed5-bfc7-f4c972dc4c8a/Untitled.png\n通过gin.Default()生成的gin.Engine其实包含一个RouterGroup(嵌套组合),所以它可以用RouterGroup的方法。\nGroup方法又生成了一个*RouterGroup，这里最重要的就是basePath,它的值是group.calculateAbsolutePath(relativePath)\n1.gin 数据解析和绑定\n客户端传参，后端接收并解析到结构体。\nLifecycle\nGin-Context 实现了对request和response的封装，是Gin的核心实现之一，学习使用gin框架就是学习使用Context包的过程。内部封装了request 和response 过程中的数据。\n框架启动过程\n1. New 一个Engine实例\napp **:=** gin.**Default**()\n2. 注册添加路由、中间件\n你可能会疑惑，为什么这里的路由处理函数要接受一个 gin.Context 类型的参数，是在何时传入的？\nEngine结构体本身发挥的核心功能就是路由处理。\napp.**GET**(\"/ping\", **func**(c *****gin.Context) {         c.**JSON**(200, gin.H{             \"message\": \"pong\",         })     })\n3. 启动Gin框架\nRun 本质就是将 注册的路由信息engine 绑定到一个 http.Server，然后开始开始监听并处理请求\n`app.Run()\nfunc (engine *****Engine) Run(addr …string) (err error) {\ndefer func() { debugPrintError(err) }()\naddress := resolveAddress(addr)\ndebugPrint(“Listening and serving HTTP on %s\\n”, address)\nerr = http.ListenAndServe(address, engine)\nreturn}`\n进入到 golang net/http包 （ListenandServe) ，Hanlder 是一个实现了ServeHTTP方法的类型\nAPI调用过程\n当监听到请求时，gin框架就会衍生一个Context，为它添加上请求相关参数。开一个go程进行处理。\n1. net\\http 创建连接，握手\n2. gin 框架处理请求核心：ServeHTTP\nfunc (engine*Engine)ServeHTTP(w http.ResponseWriter, req*http.Request) {    c:= engine.pool.Get().(*Context)    c.writermem.reset(w)    c.Request = req    c.reset()    engine.handleHTTPRequest(c)    engine.pool.Put(c)}\nSEO\n\nSearch engine optimization: the process of making your site better for search engines.\n\n如果您的网站不在 Google 的索引中\n虽然 Google 可抓取数十亿个网页，但难免也会遗漏部分网站。造成抓取工具遗漏网站的常见原因如下：\n\n此网站未与网络上的其他网站紧密关联\n您刚刚推出新的网站，Google 还没来得及抓取\n网站的设计致使 Google 难以有效抓取其中的内容\nGoogle 在尝试抓取网站时收到了错误消息\n您的政策阻止 Google 抓取网站\n\n元描述标记很重要，因为 Google 可能会在搜索结果中将其用作网页的摘要。请注意，我们说的是“可能”，因为如果网页中有一段可见文本能很好地匹配用户查询，那么 Google 也可能会选择使用这段文本。最好为每个网页添加元描述标记，以防 Google 找不到要在摘要中使用的恰当文本。\n&lt;meta\n网络搜索引擎通常使用网站具有的反向链接数作为确定该网站的搜索引擎排名，受欢迎程度和重要性的最重要因素之一。例如Google的PageRank系统[1]。垃圾索引（linkspam），即公司试图在其站点上放置尽可能多的入站链接backlinks，而不管源站点的上下文如何。搜索引擎排名的重要性非常高，它被视为在线业务和任何网站的访问者转化率（尤其是在线购物时）的关键参数。博客评论、文章提交、新闻稿发布，社交媒体参与和论坛发布可用于增加反向链接。\nDNS 将域名转换为 IP 地址，以便浏览器能够加载互联网资源。\n要解决跨域问题的办法有CORS、代理和JSONP\n\nCORS\n\n出于安全原因，浏览器限制从脚本发起的跨域 HTTP 请求。例如，XMLHttpRequest 和 Fetch API遵循同源策略。这意味着使用这些 API 的 Web 应用程序只能从加载应用程序的同一来源请求资源，除非来自其他来源的响应包含正确的 CORS 标头。(XMLHTTP是一组API函数集，可被JavaScript、JScript、VBScript以及其它web浏览器内嵌的脚本语言调用，通过HTTP在浏览器和web服务器之间收发XML或其它数据。XMLHTTP最大的好处在于可以动态地更新网页，它无需重新从服务器读取整个网页，也不需要安装额外的插件。该技术被许多网站使用，以实现快速响应的动态网页应用。例如：Google的Gmail服务、Google Suggest动态查找界面以及Google Map地理信息服务。\n在HTTP协议中，OPTIONS方法是一个预检请求（preflight request），用于在发送实际请求之前确定服务器是否愿意接受该请求。这种方法特别重要在跨源资源共享（CORS, Cross-Origin Resource Sharing）中，因为它帮助浏览器决定是否安全地进行跨域请求。\nOPTIONS 请求的作用包括：\n\n检查服务器支持的HTTP方法：通过发送一个OPTIONS请求到服务器，客户端可以知道服务器支持哪些HTTP方法（如GET, POST, DELETE等）。\nCORS预检请求：\n\n在进行跨域请求时，如果请求满足某些条件（例如，使用了除GET/HEAD/POST以外的方法，或者POST请求的Content-Type不是application/x-www-form-urlencoded、multipart/form-data或text/plain，或者请求包含了额外的头信息），浏览器会自动先发送一个OPTIONS请求到服务器，询问服务器是否允许跨域请求，并且询问可以使用哪些HTTP方法和头信息。\n这个预检请求的响应中，服务器会返回**Access-Control-Allow-Origin、Access-Control-Allow-Methods和Access-Control-Allow-Headers**等CORS相关的响应头，指示哪些源、哪些HTTP方法和哪些头信息是被允许的。\nOPTIONS 请求的响应头：\n\n\n\n\nAccess-Control-Allow-Origin：指示哪些源可以访问资源。\nAccess-Control-Allow-Methods：指示允许的HTTP方法。\nAccess-Control-Allow-Headers：在实际请求中允许携带的自定义请求头。\nAccess-Control-Max-Age：指示浏览器应该缓存预检请求的结果多长时间。\n\n2.JSONP （动态创建script标签）\nJSONP跨域-前端适配，后端配合\n前后端同时改造\njsonp原理：img、srcipt，link标签的src或href属性不受同源策略限制，可以用来作为请求，后端接受请求后返回一个回调函数callback，调用前端已经定义好的函数，从而实现跨域请求，如：\n$(‘#btn’).click(function(){\nvar frame = document.createElement(‘script’);\nframe.src = ‘http://localhost:3000/article-listname=leo&amp;age=30&amp;callback=func’;\n$(‘body’).append(frame);\n});\n// 此为回调函数，其中res为后端返回的数据\nfunction func(res){\nalert(res.message+res.name+‘你已经’+res.age+‘岁了’);\n}\n其中， func 这个回调函数命名，需要前后端沟通一致\n3.接口代理\n通过修改nginx服务器配置实现代理转发\n前端修改，后端不用\n前端请求 a 地址，设置nginx服务，将 a 地址代理到 b 地址。\n如vue项目中可以在 vue.config.js 中设置\nEcho is a Web pkg of GO,\nGORM\nWEB开发中的ORM\n实际上大量的工作都是简单的select，和update delete等，复杂的sql并不是特别频繁，如果orm有性能问题，再手写sql。\nORM的目的就是屏蔽掉DB层，很多语言的ORM只要把你的类或结构体定义好，再用特定的语法将结构体之间的一对一或者一对多关系表达出来。那么任务就完成了。然后你就可以对这些映射好了数据库表的对象进行各种操作，例如save，create，retrieve，delete。至于ORM在背地里做了什么阴险的勾当，你是不一定清楚的。使用ORM的时候，我们往往比较容易有一种忘记了数据库的直观感受。\n决定在项目中是否使用 ORM（对象关系映射）库通常取决于多种因素，包括项目规模、开发团队的经验、项目的复杂性以及特定需求等。以下是一些关于是否使用 ORM 的考虑因素：\n优点：\n\n简化数据库操作：ORM 将数据库操作转换为对象操作，使代码更易读、易写、易维护。它可以让开发者更专注于业务逻辑而不是 SQL 语句。\n跨数据库平台：ORM 库通常能够提供跨不同数据库平台的支持，因此你可以更轻松地在不同的数据库之间切换。\n对象映射：ORM 可以将数据库表映射到程序中的对象，简化了对象和数据库之间的转换。\n内置功能：ORM 通常提供了许多内置功能，例如数据验证、查询构建器、关联查询等，简化了开发流程。\n\n缺点：\n\n性能损耗：有时候 ORM 会引入一定程度的性能损耗，因为它需要进行对象和数据库之间的映射转换。\n学习曲线：ORM 框架可能有自己的学习曲线，开发团队需要时间来熟悉并掌握框架的使用。\n复杂查询的处理：在一些复杂的查询场景下，ORM 可能会限制灵活性，需要编写复杂的 ORM 特定语法或者原生 SQL。\n\n何时使用 ORM：\n\n小型到中型项目：对于小型到中型的项目，ORM 可能更有益于提高开发效率，并减少基本的 CRUD 操作的重复代码。\n团队熟悉度：如果团队对某个 ORM 框架比较熟悉，并且该框架能够满足项目需求，那么使用 ORM 可能是个不错的选择。\n快速开发：对于需要快速迭代和开发的项目，ORM 可以加快开发速度。\n\n何时不使用 ORM：\n\n性能要求极高：对于对性能要求极高、需要高度优化的场景，可能直接使用原生 SQL 或避免 ORM 更合适。\n复杂查询：某些复杂的查询可能不容易通过 ORM 来表达，可能需要编写复杂的 ORM 语句或者原生 SQL。\n\nGo官方提供了database/sql\n包来给用户进行和数据库打交道的工作，database/sql\n库实际只提供了一套操作数据库的接口和规范，例如抽象好的SQL预处理（prepare），连接池管理，数据绑定，事务，错误处理等等。官方并没有提供具体某种数据库实现的协议支持。\n\ngorm框架是go的一个数据库连接及交互框架，一般用于连接关系型数据库。\n本人最近用go写业务后台，数据库是mysql，用的是gorm连接，一般而言，是数据库先启动，再启动业务后台服务。\n本人在程序启动的时候，会事先进行数据库连接。\n下面是我发现的现象：\n1.数据库先启动，再启动业务后台，业务操作正常，此时断掉数据库，业务操作异常(数据库操作报错)， 此时启动数据库，再进行业务请求，业务操作正常(数据库请求正常)。\n2.业务后台先启动，数据库后启动，此时业务操作异常(数据库操作报错)\n很明显，现象1是我想要的结果，断掉数据库后再重新启动数据库，gorm内部给人感觉会自动重连一样。\n现象1表现出了期望的行为，而现象2则没有展现出自动重连的效果。这可能是因为在现象2中，GORM 连接并未尝试重新连接到数据库，而是仍然使用先前建立的连接，导致了业务操作异常。\n在 GORM 中，默认情况下，并不会对连接断开进行自动重连。但是你可以设置 GORM 的连接参数以实现断开连接后的自动重连功能。\n// 示例使用 DryRun 模式来检查是否有额外或低效的查询// Assuming 'db' is your GORM database connection instance// 开启 DryRun 模式db = db.Session(&amp;gorm.Session{DryRun: true})// 构建查询query := db.Preload(\"Orders\").Find(&amp;[]User{})// 打印生成的 SQL 语句fmt.Println(query.Statement.SQL.String()) // 这里会输出生成的 SQL 语句// 分析生成的 SQL 查询语句，确保它们是你预期的，且是有效的\n避免N+1查询问题？\n1. 使用预加载（Eager Loading）：\nORM工具通常提供预加载功能，例如使用 Preload 或 Join 等方法可以一次性加载所有需要的关联数据，而不是在循环中逐个加载。 Preload().Find(&amp;product)\n2. 批量查询：\n可以通过在单个查询中检索所有相关的数据来避免多次查询。这可以通过ORM工具的批量查询方法来实现，以减少数据库的访问次数。\n3. 使用联合查询（Join）：\n利用数据库的关联查询功能，通过在查询中使用JOIN语句一次性获取所需的数据，而不是进行多次单独的查询。 Select\n4. 缓存数据：\n对于频繁使用的数据，可以考虑将其缓存在内存中，以减少对数据库的实际访问次数。\ncasbin\nCasbin 是一个强大的、高效的开源访问控制框架，其权限管理机制支持多种访问控制模型。储存RBAC关系中user role的ORM\nCasbin 可以：\n\n支持自定义请求的格式，默认的请求格式为{subject, object, action}。\n具有访问控制模型model和策略policy两个核心概念。\n支持RBAC中的多层角色继承，不止主体可以有角色，资源也可以具有角色。\n支持内置的超级用户 例如：root 或 administrator。超级用户可以执行任何操作而无需显式的权限声明。\n支持多种内置的操作符，如 keyMatch，方便对路径式的资源进行管理，如 /foo/bar 可以映射到 /foo*\n\nCasbin 不能：\n\n身份认证 authentication（即验证用户的用户名和密码），Casbin 只负责访问控制。应该有其他专门的组件负责身份认证，然后由 Casbin 进行访问控制，二者是相互配合的关系。\n管理用户列表或角色列表。 Casbin 认为由项目自身来管理用户、角色列表更为合适， 用户通常有他们的密码，但是 Casbin 的设计思想并不是把它作为一个存储密码的容器。 而是存储RBAC方案中用户和角色之间的映射关系。\n\n有两个配置文件，model.conf和policy.csv。 其中，model.conf存储了访问模型，policy.csv存储了特定的用户权限配置。 Casbin的使用非常精炼。 基本上，我们只需要一个主要结构：enforcer。 当构建这个结构时，model.conf和policy.csv将被加载。\n换句话说，要新建一个Casbin执行器，你必须提供一个Model和一个Adapter。\nWire\nWire is a dependency inject platform. dig和Facebook的inject，它们都是使用反射机制来实现运行时依赖注入(runtime dependency injection)，而wire则是采用代码生成的方式来达到编译时依赖注入(compile-time dependency injection)。使用反射带来的性能损失倒是其次，更重要的是反射使得代码难以追踪和调试（反射会令Ctrl+左键失效…）。而wire生成的代码是符合程序员常规使用习惯的代码，十分容易理解和调试。\nprovider和injector是wire的两个核心概念。\n\nprovider: a function that can produce a value. These functions are ordinary Go code.\n\n通过提供provider函数，让wire知道如何产生这些依赖对象。wire根据我们定义的injector函数签名，生成完整的injector函数，injector函数是最终我们需要的函数，它将按依赖顺序调用provider。\ninjector中声明wire.build,wire_gen.go创建后，您可以通过运行重新生成它[go generate](&lt;https://blog.golang.org/generate&gt;)。****\nBind 函数的作用是为了让接口类型参与 wire 的构建过程。wire 的构建依靠的是参数的类型来组织代码，所以接口类型天然是不支持的。Bind 函数通过将接口类型和实现类型绑定，来达到依赖注入的目的。\neg.\n`type Fooer interface{\nHelloWorld()\n}\ntype Foo struct{}\nfunc (f Foo)HelloWorld(){}\nvar bind = wire.Bind(new(Fooer),new(Foo))`\n这样将 bind 传入 NewSet 或 Build 中就可以将 Fooer 接口和 Foo 类型绑定。\n这里需要特别注意，如果是 *Foo 实现了 Fooer 接口，需要将最后的 new(Foo) 改成 new(*Foo)\nvar Set = wire.NewSet(    provideMyFooer,    wire.Bind(new(Fooer), new(*MyFooer)),    provideBar)\n第一个参数wire.Bind是指向所需接口类型的值的指针，第二个参数是指向实现接口的类型的值的指针。任何包含接口绑定的集合也必须在提供具体类型的同一集合中具有提供者。\n属性自动注入\nwire.Struct函数构造一个结构类型并告诉注入器应该注入哪些字段。注入器将使用字段类型的提供程序填充每个字段。对于生成的结构类型S，wire.Struct同时提供S和*S\n 提供一项额外的灵活性： 它能适应指针与非指针类型，根据需要自动调整生成的代码。\n有时我们不需什么特定的初始化工作， 只是简单地创建一个对象实例， 为其指定属性赋值，然后返回。当属性多的时候，这种工作会很无聊。\nwire.Struct 可以简化此类工作， 指定属性名来注入特定属性：\n//. type S struct {//    MyFoo *Foo//    MyBar *Bar//  }//  var Set = wire.NewSet(wire.Struct(new(S), \"MyFoo\")) -&gt; inject only S.MyFoo//  var Set = wire.NewSet(wire.Struct(new(S), \"*\")) -&gt; inject all fields\n依赖注入的过程。\n1. 定义 Injector\n创建wire.go文件，定义下你最终想用的实例初始化函数例如initApp（即 Injector），定好它返回的东西*App，在方法里用panic(wire.Build(NewRedis, SomeProviderSet, NewApp))罗列出它依赖哪些实例的初始化方法（即 Provider）/或者哪些组初始化方法（ProviderSet）\n2. 定义 ProviderSet（如果有的话）\nProviderSet 就是一组初始化函数，是为了少写一些代码，能够更清晰的组织各个模块的依赖才出现的。也可以不用，但 Injector 里面的东西就需要写一堆。 像这样 var SomeProviderSet = wire.NewSet(NewES,NewDB)定义 ProviderSet 里面包含哪些 Provider\n3. 实现各个 Provider\nProvider 就是初始化方法，你需要自己实现，比如 NewApp，NewRedis，NewMySQL，GetConfig 等，注意他们们各自的输入输出\n4. 生成代码\n执行 wire 命令生成代码，工具会扫描你的代码，依照你的 Injector 定义来组织各个 Provider 的执行顺序，并自动按照 Provider 们的类型需求来按照顺序执行和安排参数传递，如果有哪些 Provider 的要求没有满足，会在终端报出来，持续修复执行 wire，直到成功生成wire_gen.go文件。接下来就可以正常使用initApp来写你后续的代码了。\n","categories":["计算机工程"],"tags":["计算机基础","编程","开源"]},{"title":"入门学习GO的一些记录","url":"/2022/12/07/Go/","content":"Go的优势是什么\n计算机一直在演化，但是编程语言并没有以同样的速度演化。现在的手机，内置的 CPU 核 数可能都多于我们使用的第一台电脑。高性能服务器拥有 64 核、128 核，甚至更多核。但是我 们依旧在使用为单核设计的技术在编程。 编程的技术同样在演化。大部分程序不再由单个开发者来完成，而是由处于不同时区、不同 时间段工作的一组人来完成。大项目被分解为小项目，指派给不同的程序员，程序员开发完成后， 再以可以在各个应用程序中交叉使用的库或者包的形式，提交给整个团队。 如今的程序员和公司比以往更加信任开源软件的力量\nGo 语言是一种让代码分享更容易的编程语言。Go 语言自带一些工具，让使用别人写的包更容易，并且 Go 语言也让分享自己写的包 更容易。 在本章中读者会看到 Go 语言区别于其他编程语言的地方。Go 语言对传统的面向对象开发 进行了重新思考，并且提供了更高效的复用代码的手段。Go 语言还让用户能更高效地利用昂贵 服务器上的所有核心，而且它编译大型项目的速度也很快。\n在Go语言出现之前，开发者们总是面临非常艰难的抉择，究竟是使用执行速度快但是编译速度并不理想的语言（如：C++），还是使用编译速度较快但执行效率不佳的语言（如：.NET、Java），或者说开发难度较低但执行速度一般的动态语言呢？显然，Go语言在这 3 个条件之间做到了最佳的平衡：快速编译，高效执行，易于开发。\nGolang快速高效的轻量级并发支持，垃圾回收使得其成为开发处理大量后端请求的后端系统首选。在 Go 中部署微服务相对容易\nGo uses goroutines and channels to handle concurrency, making the function calls “colorless.” This means there is no need to differentiate between synchronous and asynchronous functions at the syntactic level, simplifying the development process.  Go语言的并发模型是CSP（Communicating Sequential Processes），提倡通过通信共享内存而不是通过共享内存而实现通信。\nGo 的编译器在逻辑上可以被分成四个阶段：词法与语法分析、类型检查和 AST 转换、通用 SSA 生成和最后的机器代码生成，在这一节我们会使用比较少的篇幅分别介绍这四个阶段做的工作，后面的章节会具体介绍每一个阶段的具体内容。\n所有的编译过程其实都是从解析代码的源文件开始的，词法分析的作用就是解析源代码文件，它将文件中的字符串序列转换成 Token 序列，方便后面的处理和解析，我们一般会把执行词法分析的程序称为词法解析器（lexer）。\n而语法分析的输入是词法分析器输出的 Token 序列，语法分析器会按照顺序解析 Token 序列，该过程会将词法分析生成的 Token 按照编程语言定义好的文法（Grammar）自下而上或者自上而下的规约，每一个 Go 的源代码文件最终会被归纳成一个 SourceFile 结构5：\nGo语言支持交叉编译，比如说你可以在运行 Linux 系统的计算机上开发可以在 Windows 上运行的应用程序。这是第一门完全支持 UTF-8 的编程语言，这不仅体现在它可以处理使用 UTF-8 编码的字符串，就连它的源码文件格式都是使用的 UTF-8 编码。\nhttps://go.dev/doc/faq#Is_Go_an_object-oriented_language\n强类型静态编译型语言。隐式继承\n可以在事后添加接口，而无需注释原始类型。因为类型和接口之间没有明确的关系，所以没有要管理或讨论的类型层次结构。\n声明变量\nvar a string， var 声明自动匹配，\n使用操作符 := 可以高效地创建一个新的变量，称之为初始化声明。\n如果在相同的代码块中，我们不可以再次对于相同名称的变量使用初始化声明，例如：a := 20 就是不被允许的，编译器会提示错误 no new variables on left side of :=，但是 a = 20 是可以的，因为这是给相同的变量赋予一个新的值。\n如果你在定义变量 a 之前使用它，则会得到编译错误 undefined: a。\n如果你声明了一个局部变量却没有在相同的代码块中使用它，同样会得到编译错误，例如下面这个例子当中的变量 a：\n:=表示声明并赋值，只要:=左边有一个新变量都可以用:=,否则只能用=; 且不能用于声明全局变量\niota 是 Go 语言的一个保留字,用作常量计数器。由于 iota 具有自增特性,所以使用iota能简化定义，在定义枚举时很有用。\n使用iota时只需要记住以下两点\n1.iota在const关键字出现时将被重置为0。\n2.const中每新增一行常量声明将使iota计数一次(iota可理解为const语句块中的行索引)。\n\"_\"表示匿名变量，无法访问\n引用传递是指在调用函数时将实际参数的地址传递到函数中，那么在函数中对参数所进行的修改，将影响到实际参数。\n//Go 语言支持匿名函数，可作为闭包。匿名函数是一个\"内联\"语句或表达式。匿名函数的优越性在于可以直接使用函数内的变量，不必申明。以下实例中，我们创建了函数 getSequence() ，返回另外一个函数。该函数的目的是在闭包中递增 i 变量func getSequence() func() int {   i:=0   return func() int {      i+=1     return i   }}func main(){   /* nextNumber 为一个函数，函数 i 为 0 */   nextNumber := getSequence()   /* 调用 nextNumber 函数，i 变量自增 1 并返回 */   fmt.Println(nextNumber())   fmt.Println(nextNumber())   fmt.Println(nextNumber())   /* 创建新的函数 nextNumber1，并查看结果 */   nextNumber1 := getSequence()   fmt.Println(nextNumber1())   fmt.Println(nextNumber1())}\n命名规范\n1.1 Go是一门区分大小写的语言。\n命名规则涉及变量、常量、全局函数、结构、接口、方法等的命名。 Go语言从语法层面进行了以下限定：任何需要对外暴露的名字必须以大写字母开头，不需要对外暴露的则应该以小写字母开头。\n\n当命名（包括常量、变量、类型、函数名、结构字段等等）以一个大写字母开头，如：Analysize，那么使用这种形式的标识符的对象就可以被外部包的代码所使用（客户端程序需要先导入这个包），这被称为导出（像面向对象语言中的 public）；\n命名如果以小写字母开头，则对包外是不可见的，但是他们在整个包的内部是可见并且可用的（像面向对象语言中的 private ）\n\n1.2 包名称\n保持package的名字和目录保持一致，尽量采取有意义的包名，简短，有意义，尽量和标准库不要冲突。包名应该为小写单词，不要使用下划线或者混合大小写。\npackage domainpackage main\n1.3 文件命名\n尽量采取有意义的文件名，简短，有意义，应该为小写单词，使用下划线分隔各个单词。\napprove_service.go\n1.4 结构体命名\n\n\n采用驼峰命名法，首字母根据访问控制大写或者小写\n\n\nstruct 申明和初始化格式采用多行，例如下面：\n  type MainConfig struct {    Port string `json:\"port\"`    Address string `json:\"address\"`}config := MainConfig{\"1234\", \"123.221.134\"}\n\n\n%v 按默认格式输出 %+v 在%v的基础上额外输出字段名 %#v 在%+v的基础上额外输出类型名\ndefer关键字\ndefer是当前函数的声明周期结束后才会出栈**，所以return和defer在一个函数中时return先defer后return**\n动态数组传参是引用传递，而且不同元素长度的动态数组形参一致\n\n\n\nfunc main() {  slice := []int{0, 1, 2, 3}  m := make(map[int]*int)for key, val := range slice {      m[key] = &amp;val // 这里是将指针指向循环变量 val 的地址  }    for k, v := range m {      fmt.Println(k, \"-&gt;\", *v) // 输出 map 中的值}\n上面代码输出什么？\n问题的根源在于 val 是在循环中声明的，它是一个局部变量。在每次迭代中，val 的值会更新为切片中当前索引位置的值。然而，&amp;val 取得的是变量 val 的地址，而不是其值的地址。\n在 for key, val := range slice 循环结束后，val 的值将是迭代完成后的最后一个元素 3。因为 m 中存储的是 &amp;val，也就是 val 的地址，而这个地址在整个循环过程中都没有发生改变，所以无论遍历 m 中的哪个键值对，最终输出的结果都会是 *v，即指向 val 地址的值，也就是 3。\n\nnew 和 make 都可以用来分配空间，初始化类型，但是它们确有不同。\n\nnew(T) 为一个 T 类型新值分配空间并将此空间初始化为 T 的零值，返回的是新值的地址，也就是 T 类型的指针 *T，该指针指向 T 的新分配的零值。p1 := new(int)\nslice 的零值是 nil，使用 make 之后 slice 是一个初始化的 slice，即 slice 的长度、容量、底层指向的 array 都被 make 完成初始化，此时 slice 内容被类型 int 的零值填充，形式是 [0 0 0]，map 和 channel 也是类似的。\nhttps://go.dev/doc/faq#Is_Go_an_object-oriented_language\nWe decided to take a step back and think about what major issues were going to dominate software engineering in the years ahead as technology developed, and how a new language might help address them. For instance, the rise of multicore CPUs argued that a language should provide first-class support for some sort of concurrency or parallelism. And to make resource management tractable in a large concurrent program, garbage collection, or at least some sort of safe automatic memory management was required.\n强类型静态编译型语言。隐式继承\n可以在事后添加接口，而无需注释原始类型。因为类型和接口之间没有明确的关系，所以没有要管理或讨论的类型层次结构。\nUTF-8 是被广泛使用的编码格式，是文本文件的标准编码，其它包括 XML 和 JSON 在内，也都使用该编码。由于该编码对占用字节长度的不定性，Go 中的字符串也可能根据需要占用 1 至 4 个字节（示例见第 4.6 节），这与其它语言如 C++、Java 或者 Python 不同（Java 始终使用 2 个字节）。Go 这样做的好处是不仅减少了内存和硬盘空间占用，同时也不用像其它语言那样需要对使用 UTF-8 字符集的文本进行编码和解码。\n协程Goroutine\nGoroutine的并发编程模型, GC 内存模型 都基于GMP模型，简要解释一下GMP的含义：\n\nG（Goroutines）： Goroutines 是 Go 语言中轻量级线程的抽象，它们是由 Go 运行时管理的并发执行单元。Goroutines 可以看作是函数执行的轻量级版本，它们可以在程序中并发运行，但相比于传统的线程来说，它们的创建和销毁的成本更低，因此可以创建成千上万个 Goroutines 而不会产生太大的开销。\nM（Machine）： M 代表着执行 Goroutines 的执行线程（Machine）。每个物理线程（OS 线程）都对应着一个 M，而 M 负责调度和执行 Goroutines。Go 运行时会维护一个 M 池，用来管理和调度 Goroutines。\nP（Processor）： P 是 G 和 M 之间的调度器。P 的作用是在 M 上运行 Goroutines，P 维护了一个 Goroutines 的队列。P 将 Goroutines 分配给 M 执行，当一个 M 的 Goroutines 执行完毕时，P 会将其分配给其他的 M。\n\nGo’s Global Memory Pool (GMP) is a shared memory space where goroutines can access data. It provides a mechanism for goroutines to share variables without explicit synchronization.\nHow it Works:\n\nGoroutines acquire a lock to access the GMP.\nThe lock is released when the goroutine has finished.\nMultiple goroutines can access the GMP concurrently without interfering with each other.\n\nIn the context of programming languages, “colorless” refers to the uniformity of function calls, where there is no distinction between synchronous and asynchronous functions. This concept contrasts with “colored” functions in languages that require explicit handling of asynchronous operations, such as using async and await keywords.\nColorless Concurrency Explained\nThe Problem with Colored Functions\nIn languages like JavaScript and Python, asynchronous functions must be explicitly marked with async, and their calls must be awaited using await. This leads to a distinction between synchronous and asynchronous functions, often referred to as “colored” functions. This can introduce complexity in code management because:\n\nEvery function that interacts with an asynchronous function must also be marked as asynchronous.\nThe programmer must consistently use await when calling asynchronous functions, which can lead to errors if forgotten.\n\nGo’s Approach: Colorless Concurrency\nGo was designed with concurrency as a fundamental feature. Instead of marking functions as asynchronous, Go uses goroutines and channels to handle concurrency, making the function calls “colorless.” This means there is no need to differentiate between synchronous and asynchronous functions at the syntactic level, simplifying the development process.\nGolang不同于其他语言对并发的支持：     例如，用户在写一个 Web 服务器，希望同时处理不同的 Web 请求，如果 使用 C 或者 Java，不得不写大量的额外代码来使用线程。在 Go 语言中，net/http 库直接使用了 内置的 goroutine。每个接收到的请求都自动在其自己的 goroutine 里处理。goroutine 使用的内存 比线程更少，Go 语言运行时会自动在配置的一组逻辑处理器上调度执行 goroutine。每个逻辑处 理器绑定到一个操作系统线程上。这让用户的应用程序执行效率更高，而开发工作量 显著减少。\n在目前的绝大多数语言中，都是通过加锁等线程同步方案来解决这一困难问题，Go语言却另辟蹊径，它将共享的值通过Channel传递(实际上多个独立执行的线程很少主动共享资源)。在任意给定的时刻，最好只有一个Goroutine能够拥有该资源。数据竞争从设计层面上就被杜绝了。\n//go 关键字放在方法调用前新建一个 goroutine 并让他执行方法体go GetThingDone(param1, param2);//上例的变种，新建一个匿名方法并执行go func(param1, param2) {}(val1, val2)//直接新建一个 goroutine 并在 goroutine 中执行代码块go {    //do someting...}\n因为 goroutine 在多核 cpu 环境下是并行的。如果代码块在多个 goroutine 中执行，我们就实现了代码并行。那么问题来了，怎么拿到并行的结果呢？这就得用 channel 了。\n\ngoroutine（go协程）是由Go runtime管理的轻量级线程。\n\n这句话表明了协程是用户态，因为是由Go runtime管理，而非OS内核管理\n并发编程中最常见的例子就是生产者消费者模式，该模式主要通过平衡生产线程和消费线程的工作能力来提高程序的整体处理数据的速度。简单地说，就是生产者生产一些数据，然后放到成果队列中，同时消费者从成果队列中来取这些数据。这样就让生产消费变成了异步的两个过程。当成果队列中没有数据时，消费者就进入饥饿的等待中；而当成果队列中数据已满时，生产者则面临因产品挤压导致CPU被剥夺的下岗问题。\nChannel\nYes, it is true that Go channels are concurrent but not inherently asynchronous. Let’s break down the details:\nGo channels provide a powerful mechanism for safe and efficient communication between goroutines. While the communication itself is synchronous, channels are essential for structuring concurrent programs in Go. If you need true asynchrony (e.g., non-blocking I/O), you might explore techniques like using goroutines with channels for signaling or leverage libraries designed for asynchronous operations.\n原子函数和互斥锁都能工作，但是依靠它们都不会让编写并发程序变得更简单，更不容易出错，或者更有趣。在 Go 语言里，你不仅可以使用原子函数和互斥锁来保证对共享资源的安全访 问以及消除竞争状态，还可以使用通道，通过发送和接收需要共享的资源，在 goroutine 之间做 同步。\n它包括三种类型的定义。可选的&lt;-\n代表channel的方向。如果没有指定方向，那么Channel就是双向的，既可以接收数据，也可以发送数据。\nch &lt;- v    // Send v to channel ch.v := &lt;-ch  // Receive from ch, and           // assign value to v.\n&lt;-总是优先和最左边的类型结合。(The &lt;- operator associates with the leftmost chan possible)\n创建管道c := make(chan int)\n无缓冲的通道（unbuffered channel）是指在接收前没有能力保存任何值的通道。这种类型的通 道要求发送 goroutine 和接收 goroutine 同时准备好，才能完成发送和接收操作。如果两个 goroutine 没有同时准备好，通道会导致先执行发送或接收操作的 goroutine 阻塞等待。这种对通道进行发送 和接收的交互行为本身就是同步的。其中任意一个操作都无法离开另一个操作单独存在。\n无缓冲通道 (unbuffered channel) 的确会导致发送方 goroutine 和接收方 goroutine 同步化。\n\n发送操作会阻塞，直到有另一个 goroutine 准备从通道接收数据。\n接收操作会阻塞，直到有另一个 goroutine 准备向通道发送数据。\n\n正因为这种同步的特性，无缓冲通道也被称为 同步通道 (synchronous channel)。\n这个特性使得无缓冲通道非常适合用于：\n\n保证消息顺序: 由于发送和接收操作的同步性，无缓冲通道能够保证消息按照发送顺序被接收。\n信号量机制: 可以使用无缓冲通道来实现信号量，例如通知一个 goroutine 另一个 goroutine 已经完成任务。\n同步多个 goroutine: 可以使用无缓冲通道来协调多个 goroutine 的执行，例如等待所有 goroutine 都完成后再进行下一步操作\n\n缓冲通道 (buffered channel) 在 Go 的并发编程中非常有用，尤其适用于以下场景:\n1. 缓解生产者和消费者速度差异:\n\n当消息的生产速度和消费速度可能出现暂时性不一致时，缓冲通道可以充当队列，存储一部分消息。\n生产者可以持续生成消息，无需等待消费者立即处理，避免阻塞。\n消费者可以按照自己的节奏从通道中获取消息。\n\n2. 允许有限的异步操作:\n\n虽然 Go 的通道本质上是同步的，但缓冲通道可以提供一定程度的异步行为。\n发送者只有在缓冲区满时才会阻塞，允许一定程度的 “发射后不管” (fire-and-forget) 操作。\n\n3. 提高程序吞吐量:\n\n在某些情况下，缓冲通道可以提高程序的整体吞吐量。\n例如，如果消费者需要处理消息的时间较长，缓冲通道可以让生产者继续生成消息，而不必等待消费者完成。\n\n一些具体的例子:\n\n日志系统: 生产者可以快速地将日志消息发送到缓冲通道，而消费者可以异步地将消息写入磁盘。(注意消费者数量，用其他机制保证消息顺序性)\n工作队列: 生产者可以将任务放入缓冲通道，而多个消费者可以并发地从通道中获取任务并执行。\n消息传递系统: 缓冲通道可以用于实现消息队列，允许不同组件之间进行异步通信。\n\n默认情况下，在另一端准备好之前，发送和接收都会阻塞。这使得 goroutine 可以在没有明确的锁或竞态变量的情况下进行同步。\n发动和接收数据应当在并行线上，而不能是串行的，因为发送和接收都会阻塞，如果串行，就会死锁（就是一个一直阻塞在那等对端），但不用为此操心，因为go在执行时候（编译会通过）会报错\nmake的第二个参数指定缓存的大小：ch := make(chan int, 100)。\n通过缓存的使用，可以尽量避免阻塞，提供应用的性能。\nselect是Golang在语言层面提供的多路IO复用的机制，其可以检测多个channel是否ready(即是否可读或可写)\nselect中各个case执行顺序是随机的；\n如果某个case中的channel已经ready，则执行相应的语句并退出select流程；\n如果所有的case的channel都没有ready，则有default会走default然后退出select，没有default，select将阻塞直至channel ready；\ncase后面不一定是读channel，也可以写channel，只要是对channel的操作就可以；\nselect {}                        //空的select 语句会永远阻塞，因为没有 goroutine 可以提供任何数据。因此，主协程会引发恐慌并停止僵局。\nfunc (n*node)heartbeatDetect() {for {    select {    case&lt;-n.heartbeat:    // 收到心跳信号则退出select等待下一次心跳    breakcase&lt;-time.After(time.Second*3):    // 心跳超时，关闭连接          n.conn.Close()    return}  }}\n垃圾回收GC\n相信很多人对垃圾收集器的印象都是暂停程序（Stop the world，STW），随着用户程序申请越来越多的内存，系统中的垃圾也逐渐增多；当程序的内存占用达到一定阈值时，整个应用程序就会全部暂停，垃圾收集器会扫描已经分配的所有对象并回收不再使用的内存空间，当这个过程结束后，用户程序才可以继续执行，Go 语言在早期也使用这种策略实现垃圾收集，但是今天的实现已经复杂了很多。\nCan you explain how Go’s garbage collection works?\n所有的 GC 算法其存在形式可以归结为追踪（Tracing）和引用计数（Reference Counting）\nmemory leak  ignored  finally crash.\nGo 使用垃圾收集器来自动管理内存，它负责检测和回收不再被程序使用的内存，以减少内存泄漏和提高性能。\nGarbage collection is a mechanism used in programming languages to automatically manage memory by identifying and reclaiming memory that is no longer in use (garbage), allowing it to be reused.\nWhen a program runs, memory is allocated for various data structures and objects. As the program executes, some of these allocated memory blocks become unreachable or unreferenced—meaning they are no longer needed by the program. Garbage collection is responsible for identifying and freeing these unused memory blocks to prevent memory leaks and optimize memory usage.\nDuring runtime, the garbage collector scans the memory heap, identifies which objects are reachable (still in use), and identifies those that are unreachable (garbage). It then reclaims the memory occupied by the unreachable objects so that it can be reused for future allocations. apa\nFor instance, non-pointer Go values stored in the goroutine stack, because go compiler know when to free it.  Go 编译器无法确定其生命周期，无法以这种方式分配内存的 Go 值被称为逃逸到堆。 “堆”可以被认为是内存分配的总称.\n\nTricolor Marking:\n\nObjects in memory are categorized into three colors: white, grey, and black.\nInitially, all objects are considered white (unscanned or unvisited).\nThe GC starts with a set of root objects (global variables, stack references, etc.) and marks them as grey.\nIt traverses through the object graph, marking reachable objects as grey and adding them to a work queue.\nAs the GC progresses, grey objects turn black, indicating that they have been scanned and all their references have been examined.\nObjects that remain white after this process are considered unreachable (garbage).\n\n\nConcurrent Marking:\n\nGo’s garbage collector performs garbage collection concurrently with the program’s execution, reducing stop-the-world pauses.\nWhile the garbage collection is in progress, the application can continue running, except for short stop-the-world phases necessary for specific tasks like stack scanning.\n\n\nSweeping and Reclamation:\n\nOnce the marking phase is complete, the sweep phase identifies all white (unmarked) objects and reclaims their memory.\nThe memory is then added back to the heap for future allocations.p\n\n\n\nGolang在GC的演进过程中也经历了很多次变革，大概分为**「3个阶段」**\n\nGo V1.3之前的标记-清除法(mark and sweep) 清除比标记和扫描快得多\nGo V1.5的三色并发标记法\nGo V1.8混合写屏障机制\n\n垃圾收集（GC）的频率和启动时间对于 CPU 时间和内存之间的权衡至关重要。GC 的执行频率由 GOGC 参数决定，它影响着 GC 应该启动的时间以及内存回收的程度。\n例如，将 GOGC 设置为 100（默认值为 100），表示在分配了大约 100 个新对象后触发 GC。加倍 GOGC 将使堆内存开销加倍，并使 GC CPU 成本大致减半.\n就延迟而言，stop-the-world GC 可能需要相当长的时间来执行其标记和清除阶段 降低 GC 频率也可能会导致延迟改善\nTracing garbage collection（追踪式垃圾收集）是一种垃圾回收（GC）算法，通常应用于动态内存管理系统中。从根对象出发，根据对象之间的引用信息，一步步推进直到扫描完毕整个堆，并标记哪些对象是活跃的（可达的），然后清理那些未被标记的对象。\n最传统的标记清除算法，垃圾收集器从垃圾收集的根对象出发，递归遍历这些对象指向的子对象并将所有可达的对象标记成存活；标记阶段结束后，垃圾收集器会依次遍历堆中的对象并清除其中的垃圾，整个过程需要标记对象的存活状态，用户程序在垃圾收集的过程中也不能执行，我们需要用到更复杂的机制来解决 STW 的问题。\n追踪式垃圾收集相对于其他算法（比如引用计数等）有其优势和劣势。优势在于它能够处理循环引用（Circular references）并回收垃圾，但是劣势在于它可能引起暂停（停顿式垃圾收集），即在标记和清理的过程中，会暂停应用程序的执行。\n虽然 Go 语言的垃圾回收会有一些额外的开销，但是编程时，能显著降低开发难度。Go 语言把无趣的内存管理交给专业的编译器去做，而让程序员专注于更有趣的事情。\n程序运行时将所使用的内存划分成两部分：\n\n\n\n栈（Stack） 函数的局部内存空间，用于保存函数的局部变量，也包括函数调用时的参数、返回值。栈内存与函数的生命周期完全一致，在函数调用时创建、退出时销毁，由程序自动管理，并且栈上数据仅对当前函数可见，因此，只要内存使用的生命周期在函数生命周期之内，都可以分配栈上的空间。 栈更加高效与安全，但对于使用场景有严格限制。\n\n\n\n\n堆（Heap） 全局内存空间，由整个程序共享。堆内存的分配与释放需要单独管理，像C语言有专门的分配与释放内存的内置函数，但现在几乎所有的高级编程语言都提供了自动的内存管理策略，Go 语言便是其中之一。由于堆内存的全局可见性，还需要保证并发访问下数据的同步性。 堆更加灵活，但面临着更复杂的管理问题，为程序带来了额外的开销。\n\n\n\n由此可见，对于一个内存区域，它在程序中的可见范围决定了应该使用哪种内存：如果该内存区域的可见范围跨越了多个函数，那么就必须使用堆内存，而如果其可见范围仅仅局限于一个函数之内，则可以优先考虑栈空间。\n程序的变量就是对内存区域的抽象，因此，对变量的可见范围（作用域）进行分析，进而判断应该将其分配到栈上还是堆上的过程，叫着逃逸分析。即为了简化内存管理，我们默认所有变量都使用栈空间，但如果一个变量由于被多个函数所引用，必须将其转移到堆上，我们就说该变量“逃逸”到了堆上。\n语法糖\n‘…’ 其实是go的一种语法糖。用法一：表示多个不确定数量的参数\n用法二：slice打散传递\n\narr2 := []int{1,2,3}\narr1 = append(arr1,0)\narr1 = append(arr1,arr2…)\n\n数据类型\nGo语言中数组、字符串和切片三者是密切相关的数据结构。这三种数据类型，在底层原始数据有着相同的内存结构，在上层，因为语法的限制而有着不同的行为表现。首先，Go语言的数组是一种值类型，虽然数组的元素可以被修改，但是数组本身的赋值和函数传参都是以整体复制的方式处理的。Go语言字符串底层数据也是对应的字节数组，但是字符串的只读属性禁止了在程序中对底层字节数组的元素的修改。字符串赋值只是复制了数据地址和对应的长度，而不会导致底层数据的复制。切片的行为更为灵活，切片的结构和字符串结构类似，但是解除了只读限制。切片的底层数据虽然也是对应数据类型的数组，但是每个切片还有独立的长度和容量信息，切片赋值和函数传参数时也是将切片头信息部分按传值方式处理。因为切片头含有底层数据的指针，所以它的赋值也不会导致底层数据的复制。其实Go语言的赋值和函数传参规则很简单，除了闭包函数以引用的方式对外部变量访问之外，其它赋值和函数传参数都是以传值的方式处理。要理解数组、字符串和切片三种不同的处理方式的原因需要详细了解它们的底层数据结构。\n数组是一个由固定长度的特定类型元素组成的序列，一个数组可以由零个或多个元素组成。数组的长度是数组类型的组成部分。因为数组的长度是数组类型的一个部分，不同长度或不同类型的数据组成的数组都是不同的类型，因此在Go语言中很少直接使用数组（不同长度的数组因为类型不同无法直接赋值）。和数组对应的类型是切片，切片是可以动态增长和收缩的序列，切片的功能也更加灵活，但是要理解切片的工作原理还是要先理解数组。\n转换:   Atoi (string to int) and Itoa (int to string).\n\nstrconv.Itoa：用于将整数转换为字符串表示，例如将整数 5 转换为字符串 “5”。\nstring(int)：将整数解释为 Unicode 码点并转换为相应的字符，这在处理字符串压缩时并不适用。\n\nrune类型是Go语言中的一个基本类型，其实就是一个int32的别名\n，主要用于表示一个字符类型大于一个字节小于等于4个字节的情况下，特别是中文字符。\n. A byte has a limit of 0 – 255 in numerical range. It can represent an ASCII character.\n在 Go 语言中，byte 是 uint8 类型的别名，表示一个 8 位无符号整数。它主要用于表示原始的二进制数据，如文件内容、网络数据包等。\nfmt.Println([]byte(\"falcon\"))     fmt.Println([]byte(\"čerešňa\"))}$ go run str2bytes.go[102 97 108 99 111 110][196 141 101 114 101 197 161 197 136 97]\n根据内存和性能来看，在函数间传递大数组是一个开销很大的操作。在函数之间传递变量时，总是以值的方式传递的。如果这个变量是一个数组，意味着整个数组，不管有多长，都会完整复制，并传递给函数。\n指针的一个应用是你可以传递一个变量的引用（如函数的参数），这样不会传递变量的拷贝。指针传递是很廉价的，只占用 4 个或 8 个字节。当程序在工作中需要占用大量的内存，或很多变量，或者两者都有，使用指针会减少内存占用和提高效率。被指向的变量也保存在内存中，直到没有任何指针指向它们，所以从它们被创建开始就具有相互独立的生命周期。\nSlice切片\ntype slice struct {  array unsafe.Pointer  len   int  cap   int}\n编译期间的切片是 cmd/compile/internal/types.Slice 类型的，但是在运行时切片可以由如下的 reflect.SliceHeader 结构体表示，其中:\n\nData 是指向数组的指针;\nLen 是当前切片的长度；\nCap 是当前切片的容量，即 Data 数组的大小：\n\n在Go中，切片是一种动态数组的抽象，它提供了一种方便且灵活的方式来处理数据集合。切片本身并不包含数据，而是对底层数组的一个引用，并提供了对该数组局部的访问。\n以下是关于切片的一些关键概念：\n\n底层数组： 切片是建立在数组的基础之上的。切片不拥有自己的数据，而是引用一个底层数组，这个数组负责实际的存储。当切片被创建时，底层数组会被分配一段空间，切片的操作实际上是在这个空间内进行的。\n切片的传递： 切片是引用类型，当切片作为参数传递给函数时，函数接收的是切片的引用。因此，在函数内对切片的修改会影响到调用者的切片。\n\n**切片本身是一个只读对象，其工作机制类似数组指针的一种封装。**切片常见的操作有 reslice、append、copy。与此同时，切片还具有可索引，可迭代的优秀特性。\n在对slice进行append等操作时，可能会造成slice的自动扩容。其扩容时的大小增长规则是：\n\n如果期望容量大于当前容量的两倍就会使用期望容量；\n如果当前切片的长度小于 1024 就会将容量翻倍；\n如果当前切片的长度大于 1024 就会每次增加 25% 的容量，直到新容量大于期望容量；\n\n在 Go 语言中，切片是引用类型。当你将一个切片作为参数传递给函数时，实际上传递的是切片的副本，但这个副本指向的是相同底层数组（底层数据结构）。\n传递切片时，副本包括了切片本身的一些元数据信息（如指向底层数组的指针、长度、容量等），但它们共享相同底层数组的数据。\n因此，在函数内部对传递的切片的修改（例如修改切片中的元素值）会影响到原始切片指向的相同底层数组的数据。但如果在函数内部重新分配了新的底层数组给传递的切片，或者修改了切片的长度和容量等元数据信息，这些修改不会影响到原始切片。\npackage mainimport \"fmt\"func modifySlice(s []int) {    // 在函数内修改切片元素    s[0] = 100    s = append(s, 200) // 在此追加元素，但不会影响原始切片的长度或容量}func main() {    original := []int{1, 2, 3, 4, 5}    fmt.Println(\"Original Slice:\", original)    modifySlice(original)    fmt.Println(\"After Modification:\", original)//100,2,3,4,5    slice := make([]int, 2, 10)    slice1 := slice[1:2]    slice2 := append(slice1, 1)    slice2 = append(slice2, 1)    slice2[0] = 10001    fmt.Println(slice)                                //【0,0】    fmt.Println(slice1)         // [0]    fmt.Println(cap(slice2))    //18}\n深拷贝与浅拷贝的区别\n\n修改后的影响：\n\n浅拷贝：修改拷贝对象的嵌套元素会影响原对象，因为它们共享相同的嵌套对象。\n深拷贝：修改拷贝对象不会影响原对象，因为所有嵌套对象都被复制了。\n\n\n性能：\n\n浅拷贝：性能较好，因为只复制顶层结构，嵌套对象仍然共享。\n深拷贝：性能较差，因为需要递归复制所有嵌套对象，尤其是对于深层次嵌套的对象结构。\n\n\n\n实际应用\n深拷贝和浅拷贝在实际编程中有不同的应用场景：\n\n浅拷贝：适用于简单数据结构，或者在需要共享数据的场景下。\n深拷贝：适用于需要独立副本的数据结构，特别是在并发编程或多线程环境中，以避免数据竞争和不一致性。\n\n在 Go 语言中，copy 函数并不执行深拷贝，而是执行浅拷贝。copy 函数用于将一个源切片的元素复制到目标切片中，但只会复制切片中的元素值   如果切片的元素是引用类型（例如，指向结构体或切片等），copy 函数只会复制引用，而不会复制引用指向的实际数据。这意味着对目标切片中的引用类型元素的修改将影响源切片中相应的元素，因为它们引用的是相同的底层数组中的对象。\n分析可以发现，“Hello, world”字符串底层数据和以下数组是完全一致的：\nvar data = [...]byte{    'h', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd',}沟通一个切片的开闭，需要约定一个前提: 语境的开始从0开始，开始从1开始（通常情况从下标0开始）左闭右开**new := old[5:]，使用 [start:] 的形式截取，推荐new := old[5:len(old)-1]，通过计算原数组长度，截取从开始下标至最后一个下标（由于下标从0开始，所以长度减一）**\n切片的行为更为灵活，切片的结构和字符串结构类似，但是解除了只读限制。切片的底层数据虽然也是对应数据类型的数组，但是每个切片还有独立的长度和容量信息，切片赋值和函数传参数时也是将切片头信息部分按传值方式处理。因为切片头含有底层数据的指针，所以它的赋值也不会导致底层数据的复制。\nString\n在 Go 中，遍历字符串（string）时，可以使用 range 关键字来遍历字符串的 Unicode 码点（code point）。因为 Go 中的字符串是不可变的字节序列，而且每个字符可能由多个字节组成，因此使用 range 关键字能够更好地处理 Unicode 字符。\n在 Go 语言中，字符串底层实际上是一个字节（byte）数组，也就是说，字符串中的每个字符在底层都是由一个或多个字节（byte）表示的。但是，当我们遍历或处理字符串时，字符可以被视为 rune 或 byte，具体取决于使用的方式。\n字符串中的字符表示\n\n字节（byte）表示：\n\n当直接通过索引访问字符串的元素时，得到的是 byte 类型。byte 是 uint8 的别名，表示一个无符号的 8 位整数。\n字符串在内存中是以 UTF-8 编码存储的，每个字符可能由一个或多个字节组成。\n\n\nUnicode 代码点（rune）表示：\n\n当使用 for range 遍历字符串时，得到的是 rune 类型。\nrune 是 int32 的别名，用于表示 Unicode 代码点。\nrune 可以表示所有的 Unicode 字符，包括那些需要多个字节表示的字符。\n\n\n\nstring与[]byte在底层结构上是非常的相近（后者的底层表达仅多了一个cap属性，因此它们在内存布局上是可对齐的），这也就是为何builtin中内置函数copy会有一种特殊情况copy(dst []byte, src string) int的原因了。对于[]byte与string而言，两者之间最大的区别就是string的值不能改变\n字符串的值不能被更改，但可以被替换。 string在底层都是结构体stringStruct{str: str_point, len: str_len}，string结构体的str指针指向的是一个字符常量的地址， 这个地址里面的内容是不可以被改变的，因为它是只读的，但是这个指针可以指向不同的地址。\n那么，以下操作的含义是不同的：\ns := \"S1\" // 分配存储\"S1\"的内存空间，s结构体里的str指针指向这块内存s = \"S2\"  // 分配存储\"S2\"的内存空间，s结构体里的str指针转为指向这块内存b := []byte{1} // 分配存储'1'数组的内存空间，b结构体的array指针指向这个数组。b = []byte{2}  // 将array的内容改为'2'\nGolang 有 time.Time数据类型来处理挂钟时间和time.Duration来处理单调时间。 第一个基本方法是 time.Now()，它返回当前日期和时间，精确到纳秒。返回的值具有数据类型 time.Time，它是一个结构。根据 Golang 的官方文档，“A Time 代表具有纳秒精度的瞬间”。\ntime.Duration有一个基本类型 int64。持续时间表示两个瞬间之间经过的时间，以 int64 纳秒计数”。最大可能的纳秒表示可达 290 年。\n指针,传参\n\nGo 中函数传参仅有值传递一种方式；\nslice、map、channel都是引用类型，但是跟c++的不同；\nslice能够通过函数传参后，修改对应的数组值，是因为 slice 内部保存了引用数组的指针，并不是因为引用传递。\n\n\ngo 中，slice、map、channel都是引用类型，所以都会有如上的特性。\n\n在默认情况下，Go 语言使用的是值传递，即在调用过程中不会影响到实际参数。\n注意1：无论是值传递，还是引用类型传递，传递给函数的都是变量的副本，不过，值传递是值的拷贝。引用传递是地址的拷贝，一般来说，地址拷贝更为高效。而值拷贝取决于拷贝的对象大小，对象越大，则性能越低。\n注意2：map、slice、chan、指针、interface默认以引用的方式传递。\n不定参数传值 就是函数的参数不是固定的，后面的类型是固定的。（可变参数）\n匿名函数的定义就是没有名字的普通函数定义。\nGo 中数组赋值和函数传参都是值复制的。那这会导致什么问题呢？\n假想每次传参都用数组，那么每次数组都要被复制一遍。如果数组大小有 100万，在64位机器上就需要花费大约 800W 字节，即 8MB 内存。这样会消耗掉大量的内存。于是乎有人想到，函数传参用数组的指针。\nslice1 := make(int[],3)  //声明切片并分配空间初始值为0var slice2 = []int//Objective-C, Swift, Ruby, Lua中的关键字，与C++里的NULL不同，NULL是一个宏定义，值为0，nil表示无值if slice2==nil{}//追加元素,如果append时超过容量cap，容量将自动变为2倍numbers = append(numbers,2)\nnil 指针也称为空指针。\nvar a int= 20   /* 声明实际变量 */   var ip *int        /* 声明指针变量 */   ip = &amp;a  /* 指针变量的存储地址 */   fmt.Printf(\"a 变量的地址是: %x\\\\n\", &amp;a  )   /* 指针变量的存储地址 */   fmt.Printf(\"ip 变量储存的指针地址: %x\\\\n\", ip )   /* 使用指针访问值 */   fmt.Printf(\"*ip 变量的值: %d\\\\n\", *ip )普通占位符占位符     说明                           举例                   输出%v      相应值的默认格式。            Printf(\"%v\", people)   {zhangsan}，%+v     打印结构体时，会添加字段名     Printf(\"%+v\", people)  {Name:zhangsan}%#v     相应值的Go语法表示            Printf(\"#v\", people)   main.Human{Name:\"zhangsan\"}%T      相应值的类型的Go语法表示       Printf(\"%T\", people)   main.Human%%      字面上的百分号，并非值的占位符  Printf(\"%%\")            %布尔占位符占位符       说明                举例                     输出%t          true 或 false。     Printf(\"%t\", true)       true整数占位符占位符     说明                                  举例                       输出%b      二进制表示                             Printf(\"%b\", 5)             101%c      相应Unicode码点所表示的字符              Printf(\"%c\", 0x4E2D)        中%d      十进制表示                             Printf(\"%d\", 0x12)          18%o      八进制表示                             Printf(\"%d\", 10)            12%q      单引号围绕的字符字面值，由Go语法安全地转义 Printf(\"%q\", 0x4E2D)        '中'%x      十六进制表示，字母形式为小写 a-f         Printf(\"%x\", 13)             d%X      十六进制表示，字母形式为大写 A-F         Printf(\"%x\", 13)             D%U      Unicode格式：U+1234，等同于 \"U+%04X\"   Printf(\"%U\", 0x4E2D)         U+4E2D\nMap\nMap 是一种无序的键值对的集合。Map 最重要的一点是通过 key 来快速检索数据，key 类似于索引，指向数据的值。\nMap 是一种集合，所以我们可以像迭代数组和切片那样迭代它。不过，Map 是无序的，我们无法决定它的返回顺序，这是因为 Map 是使用 hash 表来实现的。\n\n/*查看元素在集合中是否存在 */    capital, ok := countryCapitalMap [ \"American\" ] /*如果确定是真实的,则存在,否则不存在 */\n[mapstructure](&lt;https://github.com/mitchellh/mapstructure&gt;)\n用于将通用的map[string]interface{}解码到对应的 Go 结构体中，或者执行相反的操作。很多时候，解析来自多种源头的数据流时，我们一般事先并不知道他们对应的具体类型。只有读取到一些字段之后才能做出判断。这时，我们可以先使用标准的encoding/json库将数据解码为map[string]interface{}类型，然后根据标识字段利用mapstructure\n库转为相应的 Go 结构体以便使用\nfunction方法\nA method is on an object or is static in class.A function is independent of any object (and outside of any class).\nFor Java and C#, there are only methods.\nFor C, there are only functions.\nFor C++ and Python it would depend on whether or not you’re in a class.\n1) 在定义时调用匿名函数\n匿名函数lambda：是指一类无需定义标识符（函数名）的函数或子程序\n。 所谓匿名函数，通俗地说就是没有名字的函数，lambda函数没有名字，是一种简单的、在同一行中定义函数的方法。 lambda函数一般功能简单：单行expression决定了lambda函数不可能完成复杂的逻辑，只能完成非常简单的功能。\n匿名函数可以在声明后调用，例如：\n1. func(data int) { 2.     fmt.Println(\"hello\", data) 3. }(100)\n表示对匿名函数进行调用，传递参数为 100。\n\n将匿名函数赋值给变量\n\n匿名函数可以被赋值，例如：\n1. // 将匿名函数体保存到f()中 2. f := func(data int) { 3.     fmt.Println(\"hello\", data) 4. } 5.  6. // 使用f()调用 7. f(100)\n匿名函数的用途非常广泛，它本身就是一种值，可以方便地保存在各种容器中实现回调函数和操作封装。\nWhat is () before a function Golang?\nit’s called receiver argument, 函数名前的括号规定了接收到的object,就像面向对象的类,想要调用Person类中的eat方法首先需要创建一个Person对象\nThe parenthesis before the function name is the Go way of defining the object on which these functions will operate.\ntype Vertex struct {  X, Y float64}func (v Vertex) Abs() float64 {  return math.Sqrt(v.X*v.X + v.Y*v.Y)}func main() {  v := Vertex{3, 4}  fmt.Println(v.Abs())}\n闭包和匿名函数经常被用作同义词。但严格来说，匿名函数就是字面意义上没有被赋予名称的函数，而闭包则实际上是一个函数的实例，也就是说它是存在于内存里的某个结构体。如果从实现上来看的话，匿名函数如果没有捕捉自由变量，那么它其实可以被实现为一个函数指针，或者直接内联到调用点，如果它捕捉了自由变量那么它将是一个闭包；而闭包则意味着同时包括函数指针和环境两个关键元素。在编译优化当中，没有捕捉自由变量的闭包可以被优化成普通函数，这样就无需分配闭包结构体，这种编译技巧被称为函数跃升\n闭包\n闭包（closure）是一个函数以及其捆绑的周边环境状态（lexical environment，词法环境）的引用的组合。换而言之，闭包让开发者可以从内部函数访问外部函数的作用域。在 JavaScript 中，闭包会随着函数的创建而被同时创建。\n闭包使得Javascript的垃圾回收机制GC不会收回a()所占用的资源，因为a()的内部函数b()的执行需要依赖a()中的变量i\ndefer 这些调用直到 return 前才被执。因此，可以用来做资源清理。\npackage mainimport \"fmt\"type Test struct {    name string}func (t *Test) Close() {    fmt.Println(t.name, \" closed\")}func Close(t Test) {    t.Close()}func main() {    ts := []Test{{\"a\"}, {\"b\"}, {\"c\"}}    for _, t := range ts {        defer Close(t)    }}\ndefer后面的语句在执行的时候，函数调用的参数会被保存起来，但是不执行。也就是复制了一份。但是并没有说struct这里的this指针如何处理，通过这个例子可以看出go语言并没有把这个明确写出来的this指针当作参数来看待。\n多个 defer 注册，按 FILO 次序执行 ( 先进后出 )。哪怕函数或某个延迟调用发生错误，这些调用依旧会被执行。\n尽可能不要在goroutine中使用闭包!\nException\nGolang 没有结构化异常，使用 panic 抛出错误，recover 捕获错误。\n异常的使用场景简单描述：Go中可以抛出一个panic的异常，然后在defer中通过recover捕获这个异常，然后正常处理。\nPython&amp;Java try-catch 机制正是提案试图避免的那种事情。 Panic 和 recover 不是通常意义的异常机制。通常的方式是将 exception 和一个控制结构相关联，鼓励细粒度的 exception 处理，导致代码往往不易阅读。在 error 和调用一个 panic 之间确实存在差异，而且我们希望这个差异很重要。在 Java 中打开一个文件会抛出异常。在我的经验中，打开文件失败是最平常不过的事。而且还需要我写许多代码来处理这样的 exception。\nDifference:\n\nlog.Panicln() is used to log an error message and then trigger a panic, providing some context or details about the error before the program exits due to the panic.\npanic() is used to immediately halt the program’s execution without providing additional context or logging. It is usually used for unrecoverable errors where the program cannot continue.\n\nReflect\n反射是指程序在运行时runtime 检查其自身结构并根据该信息修改其行为的能力。程序在编译时，变量被转换为内存地址，变量名不会被编译器写入到可执行部分。在运行程序时，程序无法获取自身的信息。\n支持反射的语言可以在程序编译期将变量的反射信息，如字段名称、类型信息、结构体信息等整合到可执行文件中，并给程序提供接口访问反射信息，这样就可以在程序运行期获取类型的反射信息，并且有能力修改它们。C/C++语言没有支持反射功能，只能通过 typeid 提供非常弱化的程序运行时类型信息；Java、C# 等语言都支持完整的反射功能；Lua、JavaScript类动态语言，由于其本身的语法特性就可以让代码在运行期访问程序自身的值和类型信息，因此不需要反射系统。\nGo语言程序中的类型（Type）指的是系统原生数据类型，如 int、string、bool、float32 等类型，以及使用 type 关键字定义的类型，这些类型的名称就是其类型本身的名称。例如使用 type A struct{} 定义结构体时，A 就是 struct{} 的类型。\nMap、Slice、Chan 属于引用类型，使用起来类似于指针\n结构体标签（Struct Tag）\n通过 reflect.Type 获取结构体成员信息 reflect.StructField 结构中的 Tag 被称为结构体标签（StructTag）。结构体标签是对结构体字段的额外信息标签。结构体标签（Struct Tag）类似于 C#\n 中的特性（Attribute）。C# 允许在类、字段、方法等前面添加 Attribute，然后在反射系统中可以获取到这个属性系统。例如：\nTag 在结构体字段后方书写的格式如下：key1:\"value1\" key2:\"value2\"\nkey会指定反射的解析方式，如下： json(JSON标签) orm(Beego标签)、gorm(GORM标签)、bson(MongoDB标签)、form(表单标签)、binding(表单验证标签)\n结构体标签由一个或多个键值对组成。键与值使用冒号分隔，值用双引号括起来。键值对之间使用一个空格分隔。\n指定映射的字段名。为了做到这一点，我们需要为字段设置mapstructure标签。例如下面使用username代替上例中的name：\ntype Person struct {  Namestring `mapstructure:\"username\"`}\nJSON\nProtocol Buffers (Protobuf) over JSON, particularly in evolving systems: backward and forward compatibility through field numbering. Let’s break down why this is so powerful:\nThe Problem with Unstructured Data and Versioning:\n\nJSON’s flexibility is a double-edged sword. While easy to read and work with, it lacks a strict schema. This means changes to field names or data types can easily break compatibility between clients and servers that were written for different versions of the API.\nVersioning headaches. Teams often resort to complex versioning schemes (e.g., in the URL or headers) to manage compatibility, adding overhead and complexity.\n\nBenefit\n\nBackward Compatibility: Older clients, unaware of newer fields (added with higher numbers), will simply ignore them. The data for the fields they do understand is still present and correctly parsed.\nForward Compatibility: Newer clients can receive messages with fields they don’t yet know about (from older servers). They’ll skip over these unknown fields without issues.\nNo Versioning Hell: You often avoid the need for explicit API versioning in your Protobuf services, simplifying development and maintenance.\n\nPB常用于后端多个服务通信，尤其适用通信接口于字段多，数据结构复杂，需要充分压缩数据的场景\nSerialization:\n\nWhat it is: The process of converting a data structure (like a Protobuf message, a Python object, etc.) into a stream of bytes or a string representation.\nProtobuf and Serialization: Protobuf’s primary function is efficient serialization! Its .proto definitions define the structure, and Protobuf compilers generate code (in various languages) with methods to\n\nJSON(marshal, unmarshal)\n控制层返回json字符串数据给前端，前端通过ajax处理将数据展示给用户\nSubstring 从以连续顺序放置在两个指定索引之间的字符串中取出字符。另一方面， 子序列可以通过删除中间的一些元素或不删除元素从另一个序列导出，但始终保持原始序列中元素的相对顺序。\njson.Decoder会一个一个元素进行加载，不会把整个json数组读到内存里面，适用于从数据流中解码多个值。例如http连接与socket连接的读取与写入，或者是文件的读取\njson.Unmarshal适用于读取已经在内存中的json数据进行解码，例如直接是[]byte的输入\nBoth JSON and XML can be used to receive data from a web server.\nparse to JS object:  const obj = JSON.parse(‘{“name”:“John”, “age”:30, “city”:“New York”}’);\n**Json Marshal：将数据编码成json字符串jsonstu,err := json.Marshal(stu)if err!=nil{        fmt.Println(\"生成json字符串错误\")    }{\"name\":\"张三\",\"Age\":18,\"HIgh\":true,\"class\":{\"Name\":\"1班\",\"Grade\":3}}**\nJSON、BSON 等格式进行序列化及对象关系映射（Object Relational Mapping，简称 ORM）系统都会用到结构体标签，这些系统使用标签设定字段在处理时应该具备的特殊属性和可能发生的行为。这些信息都是静态的，无须实例化结构体，可以通过反射获取到。\nType int `json: \"type\" id:\"100\"` //ERror:json后多了个空格,无法解析{//reflect 获取字段tagvar u User  t:=reflect.TypeOf(u)  for i:=0;i&lt;t.NumField();i++{    sf:=t.Field(i)    fmt.Println(sf.Tag.Get(\"json\"),\",\",sf.Tag.Get(\"bson\"))  }\nJson字符串转User对象的例子，这里主要利用的就是User这个结构体对应的字段Tag，json解析的原理就是通过反射获得每个字段的tag，然后把解析的json对应的值赋给他们。\n利用字段Tag不光可以把Json字符串转为结构体对象，还可以把结构体对象转为Json字符串。\nObject orient\nGolang 没有类和继承等经典的 OOP 功能，但它确实通过其类型系统和接口的使用支持一些基本的 OOP 概念。 Golang 中的关键 OOP 功能包括：在 Golang 中，方法是与特定类型关联的函数。它们是用接收器定义的\nthis是指向当前对象的指针(姑且用C里面的指针来看吧)\nself是指向当前类的指针\n多态\nGo 语言提供了另外一种数据类型即接口，它把所有的具有共性的方法定义在一起，任何其他类型只要实现了这些方法就是实现了这个接口。\n继承(组合实现)\ntype Human struct{}\ntype Superman struct{\nHuman//表示继承Human类的方法\n}\ninterface\n在 Go 中，接口是隐式实现的。如果一个type（在本例中为 PostController）包含接口 (PostControllerInterface) 中定义的所有方法，则认为它实现了该接口。\n当您定义一个结构体 (PostController) 时，其方法与接口 (PostControllerInterface) 中的方法签名相匹配，Go 编译器会自动推断该 PostController 结构体实现了 PostControllerInterface。\ntype A interface {    Get(k string) interface{}    Set(k string, v interface{})}func NewA() A {    return &amp;a{}}type a struct {    // ...}func (a0 *a) Get(k string) interface{} {    // ...    return nil}func (a0 *a) Set(k string, v interface{}) {    // ...}\n结构体\nGo 中实现 “构造子工厂” 方法。为了方便通常会为类型定义一个工厂，按惯例，工厂的名字以 new 或 New 开头。假设定义了如下的 File 结构体类型：\ntype File struct {fd      int     // 文件描述符name    string  // 文件名}下面是这个结构体类型对应的工厂方法，它返回一个指向结构体实例的指针：func NewFile(fd int, name string) *File {if fd &lt; 0 {return nil}\nf := NewFile(10, \"./test.txt\")在 Go 语言中常常像上面这样在工厂方法里使用初始化来简便的实现构造函数。\n如果 File 是一个结构体类型，那么表达式 new(File) 和 &amp;File{} 是等价的。\nReceiver type\nWith receiver functions you don’t have to mess around with classes or deal with inheritance. The person type has no knowledge of the receiver function. One advantage of using receiver function is when we couple it with iterfaces. I hope to write about interfaces shortly. In a nutshell, by using interfaces we can use the same receiver function to receive arguments of multiple types.\nreceiver在其他语言以及go语言里也叫做 函数签名（函数签名是最普遍的叫法）\n相当于类方法\ntype MyStruct struct {    x int}func (m MyStruct) Set1() {    m.x = 1}func (m *MyStruct) Set2() {    m.x = 2}\ngo get -u\n标志指示 get 更新提供命令行上命名的包的依赖项的模块，以便在可用时使用更新的次要版本或补丁版本。\nPKG\nGo的Web框架大致可以分为这么两类：\n\nRouter框架\nMVC类框架\n\n!https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1f4134d3-a1ff-4102-81c1-0b26b70b10e5/Untitled.png\n\nController，与上述类似，服务入口，负责处理路由，参数校验，请求转发。\nLogic/Service，逻辑（服务）层，一般是业务逻辑的入口，可以认为从这里开始，所有的请求参数一定是合法的。业务逻辑和业务流程也都在这一层中。常见的设计中会将该层称为 Business Rules。\nDAO/Repository，这一层主要负责和数据、存储打交道。将下层存储以更简单的函数、接口形式暴露给 Logic 层来使用。负责数据的持久化工作。\n\n每一层都会做好自己的工作，然后用请求当前的上下文构造下一层工作所需要的结构体或其它类型参数，然后调用下一层的函数。在工作完成之后，再把处理结果一层层地传出到入口，如图 5-14所示。\n!https://chai2010.gitbooks.io/advanced-go-programming-book/content/images/ch6-08-controller-logic-dao.png\nRoute框架\nhttproute使用压缩字典树radix tree\n字典树常用来进行字符串检索，例如用给定的字符串序列建立字典树。对于目标字符串，只要从根节点开始深度优先搜索，即可判断出该字符串是否曾经出现过，时间复杂度为O(n)\n，n可以认为是目标字符串的长度。为什么要这样做？字符串本身不像数值类型可以进行数值比较，两个字符串对比的时间复杂度取决于字符串长度。如果不用字典树来完成上述功能，要对历史字符串进行排序，再利用二分查找之类的算法去搜索，时间复杂度只高不低。可认为字典树是一种空间换时间的典型做法。\nGo语言的net/http注册的路径和相应的处理函数都存入了m字段中，我们只要知道处理HTTP请求的时候，会调用Handler接口的ServeHTTP方法，而ServeMux正好实现了Handler。\nfunc (mux*ServeMux)ServeHTTP(w ResponseWriter, r*Request) {//省略一些无关代码  h, _:= mux.Handler(r)  h.ServeHTTP(w, r)}\n上面代码中的mux.Handler会获取到我们注册的Index函数，然后执行它，具体mux.Handler的详细实现不再分析了，大家可以自己看下源代码。\n现在我们可以总结下net/http包对HTTP请求的处理。\nHTTP请求-&gt;ServeHTTP函数-&gt;ServeMux的Handler方法-&gt;Index函数\n这就是整个一条请求处理链，现在我们明白了net/http里对HTTP请求的原理。\nnet/http的默认路径处理HTTP请求的时候，会发现很多不足，比如：\n\n不能单独的对请求方法(POST,GET等)注册特定的处理函数\n不支持Path变量参数\n不能自动对Path进行校准\n\n所以我们得自己写一个处理请求的router\n中间件\n非业务的需求都是在http请求处理前做一些事情，并且在响应完成之后做一些事情。我们有没有办法使用一些重构思路把这些公共的非业务功能代码剥离出去呢？回到刚开头的例子，我们需要给我们的helloHandler()\n增加超时时间统计，我们可以使用一种叫function adapter的方法来对helloHandler()\n进行包装：\n中间件是一种业务无关的，在正常的的业务handler处理前后的，独立的逻辑处理片段，嵌入在 HTTP 的请求和响应之间。它可以获得 Echo#Context\n 对象用来进行一些特殊的操作， 比如记录每个请求或者统计请求数。\neg. 一个http请求过程来窥视一番。\n当你在浏览器中输入一个网址时，它会通过 DNS 解析到目标服务注册的公网IP地址\n请求到达目标服务的 web 反向代理服务器 Tengine 之后，经过一定的过滤转发到目标服务A上\n服务A通过 RPC框架 Dubbo 请求服务B的结果做中间计算，并且从 Tair 缓存中读取计算因子，计算结果\n服务A接着使用 Druid 通过 TDDL 写入计算结果到 MySQL Master 节点然后返回结果\n异步过程中 Canal 通过模拟 Binlog 主从复制的原理，迅速将这条 Binlog 消费并下发到消息队列 RocketMQ\n服务C通过 RocketMQ 消费到事件之后，通过配置中心 ConfigServer 拉取到的策略进行对应策略的事件处理。\n这个过程中我们使用了一系列的中间件来协同各个微服务完成整个流程，如web反向代理服务器 Tengine、RPC框架 Dubbo、缓存 Tair、连接池 Driud、数据库代理层 TDDL、Binlog 同步工具 Canal、消息队列 RocketMQ、配置中心 ConfigServer。\n中间件可以理解为洋葱穿透。\nZooKeeper是一个分布式协调服务，它的主要作用是为分布式系统提供一致性服务，提供的功能包括：配置维护、命名服务、分布式同步、组服务等。Kafka的运行依赖ZooKeeper。\nApache Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行状态计算。Flink 被设计为在所有常见的集群环境中运行，以内存中的速度和任何规模执行计算。\n","categories":["计算机工程"],"tags":["计算机基础","编程"]},{"title":"Not Enough RAM! Gemma3n vs Qwen3 on 16Gb device Test, Building My Private AI Assistant","url":"/2025/08/02/LLM-Finetune/","content":"Running large language models like Gemma 3 on a Mac mini M4 with only 16GB of RAM quickly forced me to confront the hard limits of local inference. Initially, I tried running the 12B quantized GGUF models via Ollama, but kept hitting 500 errors—turns out, even Q4_0 quantization wasn’t enough to fit the model into available memory. Digging deeper, I realized that most of the 16GB RAM was already claimed by macOS and background services, leaving barely 6–7GB free for inference.\nInstead of forcing it, I shifted strategy: I profiled my system memory using vm_stat, trimmed background processes, and tested multiple quantization levels (Q4, Q5, even Q8) across models. Still, running 12B remained unstable. The breakthrough came when I discovered the MLX community’s Apple-optimized versions of Gemma.\nThese use Metal-backed GPU acceleration and memory-efficient formats like int4, making it possible to run 7B and even 12B models smoothly on the same hardware. By choosing the right quantization and inference backend (MLX over GGUF), I found a setup that balances speed, memory, and model capability, all locally on Apple Silicon.\nSo this week, I went full mad-scientist mode trying to pick the best model to finetune for a secure, local, codable chatbot experience. The contenders? Gemma 3B/7B and Qwen3-7B/8B. My goals were clear:\n\nMust run on my Mac mini (M4, 16GB RAM)\nMust respond like a helpful dev agent — not lobotomized\nMust be finetunable (via LoRA or QLoRA, ideally)\nMust support private data integration (RAG or post-training)\nMust be able to code, explain, and assist\n\nThis document details my exploration of large language models, including my experiments with quantization and fine-tuning.\nPhase 0: Qwen3 vs. Gemma3 – Which to Choose?\nSimply put, Qwen3 (especially Qwen3-8B) performs better with instruction tuning – it has a more flexible language style and answers feel more “human.” Gemma3 is more akin to a Google-led team,\nwith a cleaner architecture and a stronger focus on research, making it ideal for DIY projects. However, the default base version doesn’t chat at all; you need to train it yourself or select a\nversion with an “it” suffix.\nThe key takeaway here is: a bad model isn’t necessarily a bad model – it’s often just under-trained.\nPhase 1: What is DeepSeek Abliteration and Why Does it Work?\nRecently, DeepSeek pulled off a clever move: they used the chain-of-thought output from their large model, DeepSeek-R1-0528, to distill Qwen3-8B. This resulted in a new model:\nDeepSeek-R1-0528-Qwen3-8B-GGUF\nTechnically, it’s a fine-tuned branch of Qwen3, optimized through DeepSeek’s R1 method.\nModels like:\n\nLLaMA3-Instruct\nMythoMax / Hermes2\nOpenChat 3.5\n\nfall into the same category – they use custom SFT (Supervised Fine-Tuning) data and RLHF (Reinforcement Learning from Human Feedback) / Preference Tuning to train base models into specific\nbehaviors.\nExample:\nPrompt: “Please write a Python function to determine if a string is a palindrome.”\n\nQwen3-8B-Base: Outputs a lot of explanations, but no code.\nDeepSeek Abliterated Qwen3-8B: Directly provides the function body, test cases, and an explanation of why.\n\nThis demonstrates the power of abliteration – it re-teaches a previously cautious model to think and output complete code logic.\nPhase 2: Technical Deep Dive into Model Quantization (Especially for Local Deployment)\nIf you’re using a Mac mini or a laptop with 32GB of memory or less, quantization is a topic you can’t ignore.\nWhat is Base Model Quantization?\nBase models are the original, un-instruction-tuned large models, having undergone massive self-supervised training. Quantization is the process of compressing the float16/32 weights into int4,\nint8, etc. formats.\n\n\n\nModel Size\nPrecision\nMemory Usage\nInference Speed\n\n\n\n\nFP16\nHigh\nHigh\nSlow\n\n\nQ4_0\nMedium-Low\nVery Low\nFast\n\n\nQ4_K_M\nMedium-High\nLow\nFast\n\n\nQ8_0\nHigh\nMedium-High\nSlightly Slow\n\n\n\nCompare\n\n\n\n模型\n大小\nRAM 需求\nTPS\n中文能力\n适合\n\n\n\n\nGemma-3n E4B Q4_K_M\n~4B\n&lt;6GB\n✅ 快\n一般\n本地日常使用、agent\n\n\nQwen3-14B Q4_K_M\n~14B\n13–16GB+\n❌ 慢\n很强\n离线高质量推理，慢也能忍\n\n\nQwen1.5-7B Q4_K_M\n~7B\n7–9GB\n⚠ 中\n强\n折中选择\n\n\n\nGGUF memory calc\nTotal memory ≈ weight memory + KV cache memory + other buffers\nWeight memory ≈ GGUF file size × decompression factor (approximately 1.5 for Q2_K)\nKV cache ≈ number of layers × context_length × embedding_length × bytes_per_value\nOther buffers ≈ hundreds of MB\nTherefore, when deploying on Ollama, llama.cpp, or mlc-llm, choose the appropriate GGUF version – Q4_K_M and Q8_0 offer a good balance.\nFirst, the model weights themselves. After quantization, the GGUF file stores compressed integer weights, but llama.cpp decodes them into compact tensors when loading. For 2-bit × group quantization (Q2_K), the measured memory usage is approximately 1.5 times the file size—for example, a 9.3 GB Q2_K file will occupy approximately 14 GB after loading. This is consistent with the common saying in the community that “loading a quantized model requires ∼(file size) × (bits/8) × decompression factor”, and also confirms the experience that “memory requirements are about 2-4 times the disk size” (WizardLM-7B 14GB file loading requires ∼30GB of memory) ([Artificial Intelligence Stack Exchange][1]).\nSecondly, the KV cache. The longer the statement context, the more key/value pairs need to be stored. Each Transformer layer generates a KV tensor for context_length (e.g. 4096), with a shape of (context_length, head_count_kv, head_size), and numbers are generally stored in FP16. Taking Qwen3-Coder-30B as an example: 48 layers, head_count_kv=4, head_size=128, but embedding_length=2048, this is actually…\nlayers × context × embedding_length × 2 bytes≈ 48 × 4096 × 2048 × 2 bytes≈ 800 MiB\nIf you enable a longer context (like 16KB), the KV cache will linearly double.\nPhase 3: Understanding the Model Repository Structure (Using Qwen as an Example)\nQwen/Qwen3-8B-Base├── Adapters (8)├── Finetunes (80)├── Merges (3)└── Quantizations (30)\nInterpretation:\n\nBase: The original model.\nAdapters: LoRA (Low-Rank Adaptation) fine-tuning modules.\nFinetunes: Full or mixed fine-tuning versions.\nMerges: Weight combinations from multiple finetunes.\nQuantizations: Quantized format versions (like GGUF).\n\n\nBenchmarks &amp; Tradeoffs — Qwen3 vs Gemma3\nLet’s cut to it: Qwen3 models are generally more chatty, multilingual, with better instruction tuning (especially for their Chat versions). But Gemma3 has some architectural advantages, like being more open, more compact, and Google Research-backed.\nBut here’s where it gets spicy — Gemma3 Base is not finetuned for chat out-of-the-box. Same with Qwen3 Base. So if you’re not careful, you’ll think they “suck” when in reality, they’re just raw. The value is in finetuning them your way.\nI tried running:\n\ngemma-3b-it-Q4_K_M (int4 quant via GGUF)\nqwen3-8b-base-Q4_K_M\ndeepseek-moe-16x1.3b just for fun\n\nLoad times were fine. But when asked to “write a function to parse JSON,” only Qwen3 understood my vague prompt. Gemma3 gave up like it was trained on StackOverflow flags.\n\nI stumbled on a distilled model called:\n\nDeepSeek-R1-0528-Qwen3-8B-GGUF\n\nThis is Qwen3-8B Base, post-trained using chain-of-thought data from DeepSeek’s raw model. It’s sharper, more logical, and yes, slightly deranged in a good way. I ran some evals:\nPrompt: “Implement a WebSocket server in Go”\nVanilla Qwen3-8B-Base: Half answer, talks about goroutines, no code.\nAbliterated DeepSeek-Qwen3-8B: Full server, with error handling, log statements, and config options.\nConclusion? Abliteration works — if your model is overaligned, abliterate it with purpose.\n\nCan It Run Locally and Privately?\nYes. With llama.cpp, ggml/gguf formats, and OpenWebUI, you can run a fully uncensored assistant on macOS. For better UX and parameter tweaking (temperature, top_p, stop words), I used OpenWebUI and KoboldCpp.\nSettings that worked for me:\n\nTemperature: 0.7\nTop_p: 0.95\nContext length: 4096 (up to 8192 if you dare)\n\nThis way, the model never calls OpenAI. Everything is local, encrypted, and yours. You can even hook it into a vector DB (like Chroma or Milvus) for RAG-style knowledge injection.\n\nFirst, select the appropriate GGUF (recommend Q4_K_M or Q8_0).\nThen, consider whether you want a specific finetune type (e.g., chat, coder, instruction).\n\nIf the model is base + adapter, ensure your framework supports adapters (e.g., llama.cpp &gt; v0.2).\n\nHere’s what I ended up running:\n\nBase: DeepSeek-R1-0528-Qwen3-8B-Q4_K_M\nFrontend: OpenWebUI or custom Vite/React wrapper\nBackend: llama.cpp compiled for Metal (M4 optimized)\nMemory Budget: ~12GB RAM peak for Qwen3-8B\nCustom Finetune: Alpaca-style SFT on coding prompts (still training…)\n\n\nTips for Future Hackers\n\n\nIf it says “chat” and acts dumb, it’s probably over-aligned. Post-train it.\n\n\nLook into abliteration techniques — distilled thought traces are more valuable than safety data.\n\n\nDon’t waste time with RLHF-ed-only models if you’re building a coder assistant.\n\n\nUse gguf formats for performance, especially on Apple Silicon. Compile with Metal backend.\n\n\nLearn to smell lobotomized models. They always avoid certain topics, answer vaguely, and fail to write full functions.\n\n\nLocal deployment isn’t complicated; quantization and Ollama can handle it.\n\n\nDon’t let excessive alignment limit your model’s capabilities.\n\n\nIf you’re looking for privacy, security, and customization, local LLM is the only solution.\n\n\nComing Next: Local LoRA fine tune and Gemma3n image and audio test\nI want to see how small I can go and still retain quality coding assistance. Spoiler: Gemma3-2B already shows promise with some light training, but hallucination risk is high.\n—\nThat’s it. From confusion to abliteration to building my private dev assistant, this journey taught me one thing: you don’t need OpenAI to build a good coder chatbot. You just need good data, good models, and the will to destroy and rebuild.\nIf you’re curious or want to try abliteration yourself, ping me. I’ll share my LoRA configs, ggml hacks, and more.\nRunning Large GGUF Models on Apple Silicon: A Real-World Guide for M4 Mac (16GB) Users\nYou’ve downloaded a massive 30-billion-parameter AI model — maybe Qwen3-Coder-30B-A3B-Instruct-UD-Q2_K_XL.gguf — excited to run it locally on your sleek M4 Mac with 16GB of unified memory.\nYou fire up llama-server, set --n-gpu-layers 999, and wait.\nThen… crash.\nggml_metal_graph_compute: command buffer 1 failed with status 5error: Insufficient Memory (kIOGPUCommandBufferCallbackErrorOutOfMemory)\nOr worse: silent hangs, SIGSTOP, LLDB showing 0xffffffffffffffff, and no way to kill the process.\nThis is not a failure of will — it’s a collision between ambition and reality.\nIn this article, we’ll walk through:\n\nWhy large GGUF models fail on 16GB Macs\nThe truth about quantization (spoiler: Q2_K ≠ small memory)\nHow to actually run models without crashing\nAnd how to fix the infamous pos_min == -1 bug in llama.cpp\n\nLet’s get real about local LLMs on Apple Silicon.\n\n🔍 Part 1: The Myth of “Small” Quantized Models\nYou see a file:\nQwen3-Coder-30B-A3B-Instruct-UD-Q2_K_XL.gguf — 10.97 GiB\n“Perfect!” you think. “It’s under 16GB. It should fit.”\nBut reality hits hard.\n📊 Why 11 GB ≠ 11 GB\n\n\n\nComponent\nMemory Use\n\n\n\n\nModel weights (Q2_K hybrid)\n~11 GB\n\n\nKV Cache (for context)\n~1–2 GB\n\n\nMetal GPU working buffers\n~2–4 GB\n\n\nIntermediate activations\n~1–2 GB\n\n\nmacOS &amp; other apps\n~1–2 GB\n\n\nTotal Peak Usage\n~16–20 GB\n\n\n\n👉 Even with Unsloth UD v2.0 or Q2_K_XL, the runtime memory exceeds your 16GB unified RAM.\nThe M4 GPU doesn’t have “VRAM” — it shares memory with the CPU.\nSo when Metal runs out of space, everything stops.\n\n💥 Part 2: The GPU Memory Crash\nYou see:\nggml_metal_graph_compute: command buffer 1 failed with status 5error: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)\nThis isn’t a software bug — it’s physics.\nThe GPU tried to allocate buffers for computation and failed.\nEven with --mmap (memory mapping), the active layers must be loaded into memory.\n✅ Workaround: Reduce GPU Offloading\n--n-gpu-layers 0    # CPU-only (slow but stable)--n-gpu-layers 32   # Partial GPU (best balance)\nOr switch to a 7B model — they’re faster, fit in memory, and often outperform bloated 30B models in practice.\n\n🐛 Part 3: The pos_min == -1 Bug in llama.cpp\nAfter surviving the memory battle, you hit a new enemy:\n/Users/stanmac/Downloads/llm/llama.cpp/tools/server/server.cpp:3292: pos_min == -1, but n_past &gt; 0 - should not happen\nThis is a known regression in recent llama.cpp builds (April 2025), triggered by:\n\nChat format mismatches\nSlot reuse bugs\nBroken KV cache state\n\nThe server crashes or freezes, and even kill -9 fails.\nLLDB shows:\n* thread #1, stop reason = signal SIGSTOPframe #0: 0xffffffffffffffff \n👉 The process is zombie-stuck — only a reboot can fully free the Metal GPU memory.\n\n✅ Part 4: How to Actually Succeed\nHere’s what works — tested on M4 Mac 16GB.\n✅ 1. Use a Stable llama.cpp Version\nAvoid the latest master. Use a known good commit:\ngit checkout 8f7c702cmake clean &amp;&amp; make server -j\nOr wait for the fix in:\n👉 https://github.com/ggml-org/llama.cpp/pull/13833\n\n✅ 2. Run This Command (Safe &amp; Stable)\n./build/bin/llama-server \\  --model \"Your model path\" \\  --port 10000 \\  --n-gpu-layers 0 \\  --ctx-size 4096 \\  --batch-size 512 \\  --threads 8 \\  --temp 0.7 \\  --repeat-penalty 1.1 \\  --no-cache \\  --log-disable\nKey Flags:\n\n--n-gpu-layers 0: Avoid GPU OOM\n--ctx-size 4096: Limit context to save memory\n--no-cache: Bypass broken KV cache logic\n--log-disable: Reduce noise\n\n\n✅ 3. Use /completion, Not /chat/completions\nAvoid the buggy chat format parser.\nUse:\ncurl http://localhost:10000/completion -d '{  \"prompt\": \"def binary_search(arr, x):\",  \"n_predict\": 128}'\nNot:\ncurl http://localhost:10000/v1/chat/completions -d '{ \"messages\": [...] }'\n\n✅ 4. Kill Stuck Processes\nWhen the server hangs:\npkill -9 -f llama-server\nIf that fails:\n\nOpen Activity Monitor → force quit\nOr reboot (sadly, often necessary)\n\n\n✅ 5. Better: Switch to 7B Models\n\n\n\nModel\nSize\nRAM Use\nSpeed\nQuality\n\n\n\n\nQwen2.5-Coder-7B-Q5_K_S.gguf\n~6 GB\n~9 GB\n⚡⚡\n✅✅✅\n\n\nDeepSeek-Coder-V2-Lite-Q6_K.gguf\n~5.2 GB\n~8 GB\n⚡⚡⚡\n✅✅✅✅\n\n\nCodeLlama-7B-Instruct-Q5_K_M.gguf\n~5.8 GB\n~9 GB\n⚡⚡\n✅✅\n\n\n\n👉 These run fully on GPU, are faster, and don’t crash.\n\n🏁 Conclusion: Be Realistic, Be Smart\n✅ Do:\n\nUse 7B–14B models for best experience\nStick to stable llama.cpp versions\nUse --no-cache, --n-gpu-layers 0 for debugging\nPrefer /completion over chat endpoints\n\n❌ Don’t:\n\nAssume Q2_K = small memory\nRun 30B models on 16GB Macs\nIgnore the pos_min == -1 bug — it’s real\nExpect GPU to save you — it shares memory\n\n","tags":["LLM"]},{"title":"CS NOTES","url":"/2023/10/07/General/","content":"CS NOTES\n杂乱的一些笔记\nElectronics\nAC和DC是电力学中常见的两种电流类型的缩写。\nAC代表交流（Alternating Current），指的是电流的方向和大小周期性地变化。在交流电中，电流的方向会反复改变，正负极性会交替出现。交流电是由电力系统提供的常见电源类型，例如家庭中的电源插座。\nDC代表直流（Direct Current），指的是电流在同一方向上持续不变。在直流电中，电流以恒定的方向和大小流动。直流电通常由电池或直流电源提供，如电子设备中的电池或适配器。\n笔记本电源通常被称为AC/DC适配器，意味着它可以将来自交流电源（例如墙上插座）的交流电转换为电脑所需的直流电。适配器中的电子元件将交流电转换为电脑所需的直流电源，以便供应电脑的各种组件和充电电池。\nThe main difference between AC and DC lies in their applications and characteristics. AC is more suitable for long-distance power transmission and is used in most household appliances, lighting systems, and industrial machinery. On the other hand, DC is commonly used in electronic circuits, batteries, and low-voltage applications. Some electronic components, such as transistors and diodes, require DC power to operate correctly.\nIt’s worth noting that the choice between AC and DC depends on the specific requirements of the electrical system or device. While AC is more efficient for long-distance transmission, DC is often more efficient for low-voltage applications and electronic devices due to lower conversion losses. Both AC and DC have their advantages and are used in various applications depending on their specific benefits.\n物理信道分为：\n\n\n有线信道（比如：双绞线、同轴电缆、光纤、等等）\n\n\n无线信道（比如：微波通讯、电台广播、卫星通讯、等等）\n\n\n存储信道\n\n\n信道的工作模式。大致可以分为如下三种。为了让大伙儿比较好理解，俺对每一种都举相应的例子。\n单工（simplex）\n比如“电台广播”就是典型的【单工】。“电台”可以发信号给“收音机”，但“收音机”【不能】发信号给“电台”。\n半双工（half-duplex）\n比如“单条铁路轨道”，就是典型的【半双工】。火车在单条铁轨上，可以有两种运行方向；但对于同一个瞬间，只能选其中一个方向（否则就撞车了）。\n全双工（full-duplex）\n比如“光纤”就是典型的【全双工】。在同一根光导纤维中，可以有多个光束【同时相向】传播，互相不会干扰对方。\n如今家庭宽带普及，光纤入户，modem 面对的物理介质是“光纤线路”。\n中继器（repeater）\n信号在物理介质中传输，会出现【衰减】（不论是“有线 or 无线”都有可能衰减）。“中继器”的作用是【信号增益】，使得信号能传得更远。\n另外，比如“微波通讯”是直线传播，而地球表面有弧度，还有地形的起伏。所以每隔一定距离要建“微波塔”。这玩意儿也相当于“中继器”。\nQuestions\nLocalhost 会按dns解析流程进行解析，然后和127.0.0.1 一样。在特殊的程序中比如MySQL 命令行会对localhost提前做特别处理。\n证明，n 枚硬币中一定有一枚假币，要用 k 次普通天平找出这枚假币，则问题有解的充分必要条件是 n ≤ 3k。\n必要性很容易说明，其核心思路本文一开始就已经说过了：每使用一次天平，会产生三个分支剧情；使用 k 次天平，一共会产生 3k 个不同的结局。这只够区分 3k 个不同的可能性。然而，究竟谁是假币，这一共有 n 种不同的可能性。所以，n 不能超过 3k。\n用老鼠测试100瓶药一瓶是毒药，喝下去一滴就死，假设一只最多喝100瓶，最少需要几只老鼠可以找出毒药？其实这个问题答案很简单，我们只需要7只小白鼠就够了，而这个问题的解题关键就是数学编码中的二进制。\n1、首先，”100瓶药水其中有1瓶有毒“这个随机变量X的信息熵为：log100 = 6.64\n\n\n菜鸟： 多线程如何实现 继承Thread方法 implements Runnable\n线程池创建和销毁\n设计模式\n缓存 一致性怎么达成\n索引的优缺点\n\n\n全连接队列、半连接队列溢出这种问题很容易被忽视，但是又很关键，特别是对于一些短连接应用（比如Nginx、PHP，当然他们也是支持长连接的）更容易爆发。 一旦溢出，从cpu、线程状态看起来都比较正常，但是压力上不去，在client看来rt也比较高（rt=网络+排队+真正服务时间），但是从server日志记录的真正服务时间来看rt又很短。\n另外就是jdk、netty等一些框架默认backlog比较小，可能有些情况下导致性能上不去\nLinux内核引入全连接队列、半连接队列。服务端收到客户端发起的 SYN 请求后，内核会把该连接存储到半连接队列，并向客户端响应 SYN+ACK，接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列，等待进程调用 accept 函数时把连接取出来。\n\n第三次握手是可以携带数据的，前两次握手是不可以携带数据的\n“三次握手”：为了对每次发送的数据量进行跟踪与协商，确保数据段的发送和接收同步，根据所接收到的数据量而确认数据发送、接收完毕后何时撤消联系，并建立虚连接\n为什么要采用三次握手，两次不行吗\n相信大家比较常回答的是：“因为三次握手才能保证双方具有接收和发送的能力。” 这回答是没问题，但这回答是片面的，并没有说出主要的原因。\n在前面我们知道了什么是 TCP 连接：\n\n用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括 Socket、序列号和窗口大小称为连接。\n\n所以，重要的是为什么三次握手才可以初始化 Socket、序列号和窗口大小并建立 TCP 连接。\n接下来，以三个方面分析三次握手的原因：\n\n三次握手才可以阻止重复历史连接的初始化（主要原因）\n\nTCP 使用三次握手建立连接的最主要原因就是防止「历史连接」初始化了连接。在两次握手的情况下，服务端没有中间状态给客户端来阻止历史连接 直接进入Established状态，导致服务端可能建立一个历史连接，造成资源浪费。\n\n三次握手才可以同步双方的初始序列号\n三次握手才可以避免资源浪费\n\nRFC规范里 服务端通常需要等待完成数据的发送和处理，所以服务端的 ACK 和 FIN 一般都会分开发送，因此是需要四次挥手。\n当被动关闭方（上图的服务端）在 TCP 挥手过程中，「没有数据要发送」并且「开启了 TCP 延迟确认机制」，那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。\nTCP fast open 可以在「第二次建立连接」时减少 TCP 连接建立的时延。过程如下：\n\n在第一次建立连接的时候，服务端在第二次握手产生一个 Cookie （已加密）并通过 SYN、ACK 包一起发给客户端，于是客户端就会缓存这个 Cookie，所以第一次发起 HTTP Get 请求的时候，还是需要 2 个 RTT 的时延；\n在下次请求的时候，客户端在 SYN 包带上 Cookie 发给服务端，就提前可以跳过三次握手的过程，因为 Cookie 中维护了一些信息，服务端可以从 Cookie 获取 TCP 相关的信息，这时发起的 HTTP GET 请求就只需要 1 个 RTT 的时延；\n\nTCP Fast Open 这个特性是不错，但是它需要服务端和客户端的操作系统同时支持才能体验到，而 TCP Fast Open 是在 2013 年提出的，所以市面上依然有很多老式的操作系统不支持，而升级操作系统是很麻烦的事情，因此 TCP Fast Open 很难被普及开来。还有一点，针对 HTTPS 来说，TLS 是在应用层实现的握手，而 TCP 是在内核实现的握手，这两个握手过程是无法结合在一起的，总是得先完成 TCP 握手，才能进行 TLS 握手。\nTCP 如何保证可靠性？\n\nTCP主要提供了检验和、序列号/确认应答、超时重传、最大消息长度、滑动窗口控制等方法实现了可靠性传输。\n面向字节流： TCP将数据视为字节流，而不是独立的数据包。这意味着数据可以被分割成任意大小的数据块，并且不需要按原始数据包的顺序接收。\n如果我们考虑实际网络传输过程中的各种影响，假设发送端陆续调用 send 函数先后发送 「Hi.」和「I am Xiaolin」 报文，那么实际的发送很有可能是这几种情况。 第一种情况，这两个消息被分到同一个 TCP 报文，\n第二种情况，「I am Xiaolin」的部分随 「Hi」 在一个 TCP 报文中发送出去，像这样： 图片 第三种情况，「Hi.」 的一部分随 TCP 报文被发送出去，另一部分和 「I am Xiaolin」 一起随另一个 TCP 报文发送出去，像这样。\n\n为什么 UDP 是面向报文的协议？\n\n当用户消息通过 UDP 协议传输时，操作系统不会对消息进行拆分，在组装好 UDP 头部后就交给网络层来处理，所以发出去的 UDP 报文中的数据部分就是完整的用户消息，也就是每个 UDP 报文就是一个用户消息的边界，这样接收方在接收到 UDP 报文后，读一个 UDP 报文就能读取到完整的用户消息。\n**屏蔽“有连接 or 无连接”的差异  网络层的定位是“抽象物理层和数据链路层，提供端到端”的通信能力。**当网络部署全世界的时候，最初的设计者认为网络重路由、乱序的概率，很容易会高过了有连接服务能够容忍的概率。毕竟，有连接服务要先付出代价“建立连接”来换取“通信过程中的高效”。一旦遇到网络重路由或者乱序，就要重新付出“建立连接”的代价。**尽管网络层本身是无连接和不可靠的，但上层协议，如传输层的TCP，可以提供可靠性和连接性，以确保数据的正确传输和顺序。**这种分层的设计有助于网络的模块化，不同层次可以独立开发和升级，从而促进了互联网的增长和演进。\nMysql表格设计？\n平衡范式与冗余(效率优先；往往牺牲范式)拒绝3B(拒绝大sql语句：big sql、拒绝大事务：big transaction、拒绝大批量：big batch);\n第一范式（1NF）是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性。如果出现重复的属性，就可能需要定义一个新的实体，新的实体由重复的属性构成，新实体与原实体之间为一对多关系。在第一范式（1NF）中表的每一行只包含一个实例的信息。简而言之，第一范式就是无重复的列。\n说明：在任何一个关系数据库中，第一范式（1NF）是对关系模式的基本要求，不满足第一范式（1NF）的数据库就不是关系数据库。\n第一范式存在问题：冗余度大、会引起修改操作的不一致性、数据插入异常、数据删除异常。\n先建立索引或者分区，然后再查询\n判断循环链表 原链表反向啊，构造双向链表，快慢指针\n序列化过程中的无限递归错误?            遇到序列化过程中的无限递归错误通常是由于对象之间存在循环引用而导致的。比如，对象A引用了对象B，而对象B又引用了对象A，这样的情况可能导致无限递归序列化。\n在应用解耦、流量削峰等场景下常需要用到消息队列，生产者不断生产消息投递到队列里面，不必等待消费者消费；消费者从队列里面取消息进行处理，不需要找生产者要数据；这就是典型的生产者消费者模式，平衡了生产者消费者的处理能力，达到了解耦的目的。\n当面试中被问“你了解那些设计模式”的时候，只回答一个单例模式未免显得有点不够专业，多了解一下效果可能会稍好一点。和单例模式对比，生产者消费者模式实现也比较简单，适合手写；对于其中的细节又可以再挖掘。\n不多叨叨了，本文就自己实现一个阻塞队列，然后创建生产者和消费者，来实现一个简单的生产者消费者模式。\n\n生产者在队列未满时一直生产，满了则停止生产；\n消费者在队列不为空的时候一直消费，空了则停止消费；\n当消费者发现队列里面没有消息了通知生产者生产；\n当生产者生产了消息通知消费者消费。\n\n1.eg大量外部数据排序怎么做， 堆排序、桶排序、归并排序的实现\n大数据小内存排序问题，很经典，很常见，类似的还有比如 “如何对上百万考试的成绩进行排序” 等等。\n归并排序可以采用自底向上的迭代方式进行合并，每次将相邻的两个小数组合并成一个更大的有序数组，直到整个数组被合并成一个大的有序数组。在这个过程中，可以将每个小数组看作是完全二叉树中的一棵子树，利用完全二叉树的特性进行合并\n大数据检索三种方法：\n\n数据库排序（对数据库设备要求较高）\n分治法（常见思路）\n位图法（Bitmap）\n\n2.排序的稳定性是什么\n3.协程和线程池，加不加锁\n4.线程和进程，OS的线程与JVM的线程一一对应，并行与并发\n5.Servlet Springboot关系\n6.Java的泛型，作用是什么？源码\n7.反射和注解\n8.数据库事务，索引是什么\n9.解释java传递实参的引用时为什么是值传递？\n布隆过滤器的原理是，当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。这就是布隆过滤器的基本思想。本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构\n\n布隆过滤器（Bloom Filter）可以用于去重操作，它可以高效地判断一个元素是否已经存在于一个集合中，从而避免重复添加相同的元素。在添加元素之前，首先使用布隆过滤器来检查该元素是否已经存在于集合中。如果布隆过滤器返回 “可能存在”，则可以进一步进行精确的去重检查。如果布隆过滤器返回 “一定不存在”，则可以安全地将该元素添加到集合中，因为它肯定不会导致重复。\n选择布隆过滤器还是位图取决于具体的应用需求，如果需要高效地过滤掉一部分元素，布隆过滤器是一个不错的选择；如果需要精确查询元素是否存在，位图更适合。\nBitmap\nBitMap算法的核心思想是用bit数组来记录0-1两种状态，然后再将具体数据映射到这个比特数组的具体位置，这个比特位设置成0表示数据不存在，设置成1表示数据存在。\nBitMap算在在大量数据查询、去重等应用场景中使用的比较多，这个算法具有比较高的空间利用率。\n使用Bitmap也是快速确定大量正整数中给定数字是否存在的可行选择。位图是一种将元素集合表示为位的数据结构，其中每个位代表特定元素的存在或不存在。\n以下是如何使用位图来解决检查十亿个唯一正整数中是否存在某个数字的问题：\n\n**初始化：**创建一个位图（位数组），其大小可以容纳您正在处理的正整数范围（例如，如果您的数字在 1 到 10^9 的范围内，则创建一个具有 10^9 的位图位）。将所有位初始化为 0。\n**设置位：**对于正整数集中的每个数字，将位图中的相应位设置为 1。\n**检查存在：**要检查给定的数字是否存在，只需检查位图中的相应位即可。如果该位为1，则该数存在于集合中；如果为0，则该数字不在集合中。\n\n比hashmap占用空间少，用于elasticsearch，redis\n如果我们需要记录某一用户在一年中每天是否有登录我们的系统这一需求该如何完成呢？如果使用KV存储，每个用户需要记录365个，当用户量上亿时，这所需要的存储空间是惊人的。\nRedis 为我们提供了位图这一数据结构，每个用户每天的登录记录只占据一位，365天就是365位，仅仅需要46字节就可存储，极大地节约了存储空间。\nJVM内存模型， 储存在栈的对象引用指向堆中的对象；jVM传形参时会在栈中创建一个新的对象，每当引用类型作为参数传递时，都会创建一个对象引用（实参）的副本（形参），该形参保存的地址和实参一样。Java中其实还是值传递的，只不过对于对象参数，值的内容是对象的引用。\nSpringboot 注解， 多线程池 锁LOCK，synchronized， 多线程(同时启动10个线程 结束时 信号量)\nmysql  隔离级别       redis\nArray[] 与 ArrayList的区别\nLinkedList ArrayList  O(n)\nHashMap , HashSet\n+128原码是1000 0000，反码是0111 1111，补码是0111 1111+1=1000 0000表示-128   整型int -128到127\n同时启动10个线程，如何保证子线程全部结束再去执行主线程\n线程的同时启动，常见的有两种方式控制，在jdk1.5的版本下就有这两种控制类：CyclicBarrier 和CountDownLatch。\nThreadPoolExecutor\n1.thread.join，2. 信号量CountDownLatch等等          3.线程池ThreadPoolExecutor的shutdown与awaitTermination方法\n工作原理\n\n初始化阶段：线程池被创建时，会初始化一定数量的线程，并将它们放入池中等待任务。\n提交任务：当有任务需要执行时，将任务提交至线程池。线程池会将任务放入任务队列中等待执行。\n线程执行任务：空闲线程从任务队列中获取任务并执行。如果没有可用的空闲线程，任务可能会等待，或者线程池会创建新的线程来处理任务。\n任务执行完成：任务执行完毕后，线程将结果返回给调用者，然后线程重新变为可用状态，等待下一个任务。\n无效连接请求：无效连接请求是指未经授权或不存在的主机或服务尝试建立连接。这可能是由恶意攻击者伪造的连接请求，也可能是过时的连接请求，由于网络延迟或其他问题而导致被错误地处理。\n拒绝服务攻击：拒绝服务（DoS）攻击是指恶意行为者试图通过发送大量的连接请求来超负荷服务器，从而使其无法正常提供服务。这种攻击会导致合法用户无法访问服务器。\n半开连接：半开连接是指在TCP三次握手的过程中，连接的一方在握手过程中的某一步中断，导致连接处于不稳定状态。这可能会占用服务器资源，同时也可能导致通信问题。\n网络错误：网络错误，如断网、网络延迟、丢包等，都可能导致连接在建立或传输过程中发生问题，从而造成错误连接。\n过时连接状态：如果连接状态没有得到适时清理或更新，服务器可能会在一段时间后认为某个连接仍然有效，尽管实际上已经失效。这可能导致资源的浪费。\n恶意连接：恶意连接是指由攻击者创建的连接，可能旨在获取未经授权的访问、传播恶意软件或进行其他恶意行为。\n\nHTTPS是如何保证安全传输的？\nhttps通过使⽤对称加密、 ⾮对称加密、 数字证书等⽅式来保证数据的安全传输。客户端向服务端发送数据之前，需要先建⽴TCP连接，所以需要先建⽴TCP连接，建⽴完TCP连接后， 服务端会先给客户端发送公钥，客户端拿到公钥后就可以⽤来加密数据了，服务端到时候接收到数据就可以⽤私钥解密数据，这种就是通过⾮对称加密来传输数据。Client收到server key exchange公钥后发送client key exchange（与主密钥）通过⾮对称加密的⽅式来传输对称加密的秘钥， 通信建立后使用对称加密传输数据减少开销\nHTTP1.1 第一个标准版本，核心一次一份\nHTTP 帧现在对 Web 开发人员是透明的。在 HTTP/2 中，这是一个在 HTTP/1.1 和底层传输协议之间附加的步骤。Web 开发人员不需要在其使用的 API 中做任何更改来利用 HTTP 帧；当浏览器和服务器都可用时，HTTP/2 将被打开并使用。\nHTTP/2.0（HTTP/2）：\nHTTP/2 是 HTTP/1.1 的后续版本，于 2015 年 5 月实现标准化。它引入了几项关键改进：\n\n多路复用：在 HTTP/1.1 中，请求和响应是按顺序处理的，导致队头阻塞问题。HTTP/2 使用多路复用，允许在单个连接上同时发送和接收多个请求和响应。这可以提高性能并减少延迟。\n标头压缩：HTTP/2 使用 HPACK 压缩来减少请求和响应中标头字段的开销，从而进一步提高效率。\n服务器推送：服务器可以在客户端请求资源之前主动将资源推送到客户端的缓存。这可以显着加快页面加载时间。\n流优先级：HTTP/2 允许对请求进行优先级排序，从而可以更快地加载更重要的资源。\n二进制协议：HTTP/2 是一种二进制协议，与 HTTP/1.1 基于文本的协议相比，它的解析和生成效率更高。\n\nHTTP/3：\nHTTP/3 是 HTTP 协议的最新版本，基于 Google 的 QUIC（快速 UDP 互联网连接）协议。它旨在通过解决基于 TCP 的协议（如 HTTP/1.1 和 HTTP/2）的一些限制来进一步提高性能。HTTP/3 的主要特性包括：\n\n多路复用和并发：与 HTTP/2 一样，HTTP/3 支持多路复用以允许并发多个请求和响应。然而，它使用 UDP 而不是 TCP，这可以进一步减少延迟。\n减少队头阻塞：HTTP/3 底层的 QUIC 协议旨在最大限度地减少基于 TCP 的协议可能出现的队头阻塞问题。\n改进的安全性：QUIC 提供内置加密，默认情况下更加安全。\n连接迁移：HTTP/3 允许连接在网络之间迁移而不丢失，这对于在 Wi-Fi 和蜂窝网络之间切换的移动设备特别有利。\n高延迟环境中的更好性能：HTTP/3 针对高延迟网络条件（例如移动网络或卫星连接）下的性能进行了优化。\n\nQUIC (Quick UDP Internet Connections) is a transport layer network protocol developed by Google. It was designed to improve upon some of the limitations of TCP (Transmission Control Protocol) and TLS (Transport Layer Security) to provide faster and more secure communication over the Internet. Here are some key features and characteristics of QUIC:\n\nLow Latency: QUIC is designed to reduce latency in communication. It achieves this by combining the features of both transport and security layers into a single protocol, reducing the round trips needed to establish a connection.\nMultiplexing: QUIC supports multiplexing, which allows multiple streams of data to be sent over a single connection. This feature reduces head-of-line blocking issues and enables more efficient use of network resources.\nConnection Migration: QUIC connections are designed to be mobile-friendly. They can migrate between different network interfaces or IP addresses without dropping the connection. This is especially useful for mobile devices that switch between Wi-Fi and cellular networks.\nEncryption: QUIC mandates encryption by default, using the Datagram Transport Layer Security (DTLS) protocol. This ensures that data exchanged over QUIC connections is secure.\nAdaptive Congestion Control: QUIC includes built-in congestion control mechanisms to adapt to network conditions dynamically.\nFast Handshake: QUIC has a faster handshake process compared to TCP/TLS, which reduces the time needed to establish a secure connection.\nForward Error Correction (FEC): QUIC can use FEC to recover lost or corrupted packets, reducing the need for retransmissions and improving reliability.\nHTTP/3: HTTP/3 is built on top of QUIC, replacing the older HTTP/2. This combination further improves web performance by reducing latency and providing better multiplexing capabilities.\n\nGoogle值得注意的是，QUIC 可以通过 UDP 运行，这使得它能够绕过一些与 TCP 相关的拥塞控制和中间盒问题。然而，UDP的使用也带来了防火墙和网络配置方面的挑战，这些在部署基于QUIC的服务时需要考虑。\nCrypto\ndouble DES has the security level of a 57-bit key.   AES only supports a 128-bit block size\nWhen the sender sends their public key, the attacker substitutes their own public key. • The receiver believes that public key belongs to the sender and encrypts using it.\nFor example, if you want to establish an encrypted connection with someone over the internet, you need a way to verify that the public key you are using to establish the connection actually belongs to the person you think it does. Without a certificate, you might be vulnerable to a man-in-the-middle attack, where an attacker intercepts your connection and provides you with their own public key, pretending to be the person you are trying to connect with. With a certificate, you can verify the identity of the person you are communicating with by checking the certificate against a trusted list of CA’s.\nPrevention:\nA certificate (X.509) contains a public key. It is signed with\nthe private key of the “trusted” authority\nPlayfair:\n首先该密码需要秘钥，与，然后由秘钥制作相应的密码表。秘钥去重后,将秘钥依次填入5x5表格(先填纵列)，剩下的格子友a-z依次填入，如果前面遇到秘钥中的字母就跳过，将i和j放在同一个格子。如：秘钥是linux, 填成的密码表为：\nl\ta\tf\to\tt\ni/j\tb\tg\tp\tv\nn\tc\th\tq\tw\nu\td\tl\tr\ty\nx\te\tm\ts\tz\n在构建好密码表后，将待加密的明文分为两个一组，同时要保证每组的两个字符不相同，如果相同，则在其中间插入一个x获取q，在进行分组。如果最后还有一个字符单着，则添加一个x。\n分好组后，依次拿出每个组，根据密码表对其加密，加密规则如下:\n如果该组的两个字符在密码表的同一行，则密文分别是其紧靠着的右边这个字符。其中第一列被看做是最后一列的右方。\n如果该组的两个字符在密码表的同一列，则密文分别是其紧靠着的下边这个字符。之中第一行被看做是最后一行的下方。\n如果该组的两个字符即不再同一行，又不在同一类，则密文是其组成的长方形的另外两个顶点（至于横向替换还是纵向替换，这需要双方沟通好）。\n加密算法思想：正向简单 逆向困难\nDiffie和Hellman虽然提出了公钥密码体制的概念，但是很遗憾，他们没有提出一种公钥加密算法\n，而是提出一种密钥协商协议，史称Diffie-Hellman密钥协商协议.\nECDHE 表示 “Elliptic Curve Diffie-Hellman Ephemeral”，是一种密钥交换协议，用于在加密通信中安全地协商密钥。\n这种协议结合了椭圆曲线加密（ECC）和 Diffie-Hellman 密钥交换算法。ECDHE 允许通信双方在没有事先共享密钥的情况下协商出一个对称密钥，用于保护它们之间的加密通信。\nEG. Here is an example of the Diffie Hellman protocol, with non-secret values in blue, and secret values in red. Alice bob做菜 混颜色 私钥 公钥\n\nRSA算法的安全性在于，我们有一个大合数n = pq，其中p和q都是特别大的质数（现在要求差不多150位十进制），如果只给定这个大合数n的话，人们还没找到一个比较高效的算法，在只知道n的条件下，算出n到底是哪两个质数相乘得来的。\n\n非对称加密： RSA 是一种非对称加密算法，公钥用于加密，私钥用于解密。这种特性使得安全通信成为可能。\n数字签名： RSA 可用于数字签名，确保消息的完整性和验证消息发送方的身份。\n\n如果两个质数选的满足了一定的条·件，那么很可能你自己实现的RSA就容易被破解。\n谁知道私钥d，谁就能分解整数n。所以我们不能对不同的用户使用相同的n，否则这两个用户可以分别互相算出对方的私钥。\n\n\n\n\n\nECC\nRSA\n\n\n\n\nKey length\n\n\n\n\n256 bit\n2048 bit\n\n\n\nCPU&amp;Memory usage\nlow\nhigh\n\n\nNetwork consumption\n\n\n\n\nlow\nhigh\n\n\n\nCracking difficulty\n\n\n\n\ndifficult to crack\ndifficult to crack, but easier than ECC in theory\n\n\n\nAnti aggression\n\n\n\n\nstrong\naverage\n\n\n\nEncryption efficiency\n\n\n\n\nhigh\naverage\n\n\n\n\nt\n在数论中，对正整数n，欧拉函数φ(n)是小于或等于n的正整数中与n互质的数的数目。此函数以其首名研究者欧拉命名，它又称为φ函数（由高斯所命名）或是欧拉总计函数（totient function，由西尔维斯特所命名）。\n例如φ(8) = 4\n\n\n\n快速幂            计算a的n次方，如果n是偶数（不为0），那么就先计算a的n/2次方，然后平方；如果n是奇数，那么就先计算a的n-1次方，再乘上a；递归出口是a的0次方为1。\n\nMITM\n单独说DH协商算法不能防止中间人没有意义。因为密钥协商算法要解决的是在不可靠链路上安全地协商密钥。而通信双方身份认证又是另外一个体系的东西。\nMITM:\n\n服务器向客户端发送公钥。\n攻击者截获公钥，保留在自己手上。\n然后攻击者自己生成一个【伪造的】公钥，发给客户端。\n客户端收到伪造的公钥后，生成加密hash值发给服务器。\n攻击者获得加密hash值，用自己的私钥解密获得真秘钥。\n同时生成假的加密hash值，发给服务器。\n服务器用私钥解密获得假秘钥。\n服务器用加秘钥加密传输信息\n\n可以看出我们这里数据是明文传输的，存在窃听风险。但是我们为了阐述数字签名机制是如何运转的，故意将保证信息机密性的机制省略了。\n如果想要保证数据的机密性，我们常见的做法是，通信双方通过非对称加密安全交换对称加密的密钥，后续通信过程的数据都使用对称加密保证数据机密性。\n并且「签名」的作用本身也不是用来保证数据的机密性，而是用于验证数据来源的防止数据被篡改的，也就是确认发送者的身份。接受者 Alice 收到后，取下数字签名，同时用 Bob 的公钥解密，得到「摘要1」，证明确实是 Bob 发的。\n再对邮件内容使用相同的散列函数计算「摘要2」，与上面得到的「摘要1」进行对比，两者一致就说明信息未被篡改。\n求模运算的规律\n(a + b) % p = (a % p + b % p) % p(a - b) % p = (a % p - b % p + p) % p(a * b) % p = (a % p * b % p) % p  a ^ b % p = ((a % p)^b) % p\nHave an Authentication Server (AS) – Users initially negotiate with AS to identify self – AS provides a non-corruptible authentication credential (ticket granting ticket TGT)\n• Have a Ticket Granting server (TGS) – Users subsequently request access to other services from TGS on basis of users TGT\n\nMD5 的作用是让大容量信息在用数字签名软件签署私人密钥前被\"压缩\"成一种保密的格式（就是把一个任意长度的字节串变换成一定长的十六进制数字串）。\nMD5 其实在我们生活中是很常用的，似乎你并没有注意到，当你下载了一个镜像之后，你会发现下载页面还提供了一组 MD5 值，那么这组 MD5 值是用来做什么的呢？了解了 MD5 的作用之后，你就不难想到，MD5 是用来验证文件的一致性的，当你下载好镜像之后，你需要对该镜像做一次 MD5 的校验，得到的 MD5 值与下载页面提供的 MD5 值进行对比，以此来验证该镜像是否被篡改。\nMD5算法，可以用来保存用户的密码信息。为了更好的保存，可以在保存的过程中，加入盐。/在保存用户密码的时候，盐可以利用生成的随机数。可以将密码结合MD5加盐，生成的数据摘要和盐保存起来 。以便于下次用户验证使用。在用户表里面，也保存salt。\nNetwork\nNetwork\nDNS是互联网基础设施的重要组成部分，它使用户能够轻松地访问网站，同时允许网站拥有灵活的服务器架构，因为它们可以更改其IP地址而不会影响用户的访问。这个过程通常在背后快速完成，使用户可以快速访问他们想要的网站，而无需手动输入IP地址。\nDNS污染，又称为域名服务器缓存污染(DNS cache pollution)或者域名服务器快照侵害(DNS cache poisoning)\nDNS污染是指一些刻意制造或无意中制造出来的域名服务器分组，把域名指往不正确的IP地址。一般来说，网站在互联网上一般都有可信赖的域名服务器，但为减免网络上的交通，一般的域名都会把外间的域名服务器数据暂存起来，待下次有其他机器要求解析域名时，可以立即提供服务。一旦有相关网域的局域域名服务器的缓存受到污染，就会把网域内的电脑导引往错误的服务器或服务器的网址。\n本地电脑的dns是最高优先级，也就是说如果你电脑的dns设置跟其他2个不一样，那么当你访问网站的时候，是自动遵循你电脑的dns路线的。第二优先级的是路由器的dhcp的dns，如果你电脑端的dns没有设置=自动获取的话，这里获取的首先就是dhcp的dns，如果你电脑端和dhcp都没有设置dns，都是默认的话，使用的就是wan端口的dns，这样说大家都明白了吧，所以一般wan口的dns都是会自动获取到电信分配的dns的，即便是你路由器的dhcp和电脑端的dns都不设置都是可以正常上网的，当然你如果要防止dns劫持，那么你只需要在你电脑端设置一个安全的dns就行了。\n专业点的说法就是WAN 端口的 DNS 服务器从ISP电信运营商获取，路由器向之发送域名解析请求；\nClash支持四种策略类型：\n\nDIRECT：通过 interface-name 直接连接目标（不查找系统路由表）\nREJECT：直接丢弃数据包\nProxy：将数据包转发到指定的代理服务器\nProxy Group：将数据包转发到指定的策略组\n\n规则类型及用法：\n\nDOMAIN：DOMAIN,www.google.com,policy 将 www.google.com 的流量转发到 policy\nDOMAIN-SUFFIX：DOMAIN-SUFFIX,youtube.com,policy 将所有以 youtube.com 结尾的域名转发到 policy。例如，www.youtube.com 和 foo.bar.youtube.com\nDOMAIN-KEYWORD：DOMAIN-KEYWORD,google,policy 将所有包含 google 关键字的域名转发到 policy。例如，www.google.com 或 googleapis.com\nGEOIP：根据IP地理位置（国家代码）转发。使用 MaxMind GeoLite2 数据库。需要先解析IP再查找国家代码，可用 no-resolve 跳过解析。例：GEOIP,CN,policy\nIP-CIDR：根据IPv4地址段转发。需要先解析IPv4地址，可用 no-resolve 跳过。例：IP-CIDR,127.0.0.0/8,DIRECT\nIP-CIDR6：根据IPv6地址段转发。需要先解析IPv6地址，可用 no-resolve 跳过。例：IP-CIDR6,2620:0:2d0:200::7/32,policy\n\nClash DNS\n1. Clash 的 DNS 和代理两个模块是独立的, 实际使用会出现问题。什么时候用 namerserver 和 fallback 取决于 fallback-filter ，和 rules 无关。对于 TCP 流量，DNS 解析的结果不是实际连接的 IP ，Clash 会传递域名在代理服务器上解析。UDP 流量则用本地解析的结果。\n当 DNS 请求被发送到 Clash DNS 时, Clash 内核会通过管理内部的域名和其 fake-ip 地址的映射, 从池中分配一个 空闲 的 fake-ip 地址.\n以使用浏览器访问 http://google.com 为例.\n浏览器向 Clash DNS 请求 google.com 的 IP 地址\nClash 检查内部映射并返回 198.18.1.5\n浏览器向 198.18.1.5 的 80/tcp 端口发送 HTTP 请求\n当收到 198.18.1.5 的入站数据包时, Clash 查询内部映射, 发现客户端实际上是在向 google.com 发送数据包\n根据规则的不同:\n\nClash 可能仅将域名发送到 SOCKS5 或 shadowsocks 等出站代理, 并与代理服务器建立连接\n或者 Clash 可能会基于 SCRIPT、GEOIP、IP-CIDR 规则或者使用 DIRECT 直连出口查询 google.com 的真实 IP 地址\n\n配置误区之 fallback dns\nfallback dns 是 redir-host 时代的产物，是为了解决 DNS 污染问题。clash dns路径：\n\nnameserver → 成功则用\nfallback → 失败了再试 fallback\nfallback 超时或失败 → 请求返回失败\n\nredir-host 模式也是 tun 模式的一种（还有一种叫 fake IP），tun 模式工作在第三层网络层，拿不到请求的域名，只能拿到连接的 IP 地址。那他如何根据域名规则分流呢？答案是先拦截 53 端口的 DNS 请求，自己建立一个映射表，在终端发起请求的时候来匹配访问的域名，从而进行后续分流等操作。\n但是如果多个网站部署在同一个 IP 下，或者 DNS 被污染到同一个 IP，redir-host 就不准了（除非加上嗅探功能）。配置 fallback dns 之后，如果获取到的 IP 被判定为被污染（需要回滚），就使用 fallback dns 解析出来的结果。为了确保拿到的结果完全正确，fallback dns 要设置为境外的加密 DNS 服务器，因为无加密或者境内的 DNS 都会被污染。\n\n国内域名直接走 UDP：223.5.5.5、119.29.29.29 在大陆网络几乎零丢包、低延迟。\n污染域名强制走加密：通过 nameserver-policy 单域名指定，保证只有必要流量进境外通道。\n无 fallback：一旦启用 fallback，就会在主解析失败后才触发，加重了整体解析链路延迟。\nDHCP 的 DNS 服务器可以自主设置或者继承WAN的DNS服务器，所有自动获取DNS服务器的连接设备向之发送域名解析请求； 所以上网使用域名解析时使用上面3个dns服务器的优先级循序：本机电脑的dns服务器—&gt;路由器中的dhcp服务器中的dns服务器—&gt;路由器中的wan口中的dns服务器。这3个服务器中只要有一个设置了都能保证上网的域名解析，\n另外，你设备上，例如Windows也可以在网络连接上自主设置DNS服务器，或者接受DHCP服务器分配的DNS服务器，本地受用。\n\nDHCP\n第一步：Client通过广播（broadcast）发送DHCP Discover 报文，Look for服务器端\n第二步：Server通过单播（unicast）发送DHCP Offer 报文向客户端提供IP地址等网络信息\n第三步：Client通过广播DHCP Request 报文告知服务器端本地选择使用哪个IP地址\n第四步：Server通过DHCP Ack报文告知客户端IP地址是合法可用的\nDHCP返回：ip地址、网关（router、子网掩码、DNS\nNAT\nnetwork address translation\n动机（使用NAT的原因）: 本地网络只有一个有效IP地址:\n不需要从ISP分配一块地址，可用一个IP地址用于所有的（局域网）设备 –省钱\n可以在局域网改变设备的地址情况下而无须通知外界\n可以改变ISP（地址变化）而不需要改变内部的设备地址\n局域网内部的设备没有明确的地址，对外是不可见的 –安全\n\n如果流量从Inside端口进来，那么先执行路由，后执行NAT(本地 到 全局)。\n如果流量从Outside端口进来，那么先执行NAT(全局 到 本地)，后执行路由。\n\n安全外壳协议( SSH **)**是一种加密 网络协议，用于在不安全的网络上安全地运行网络服务。[1]其最著名的应用是 remote login and command-line execution.\nSSH 应用程序基于客户端-服务器架构，将SSH 客户端实例与SSH 服务器连接。[2] SSH 作为分层协议套件运行，包含三个主要分层组件：传输层提供服务器身份验证、机密性和完整性；用户认证协议向服务器验证用户；连接协议将加密隧道复用为多个逻辑通信通道。[1]\nSSH 是在类 Unix操作系统上设计的，作为Telnet和不安全的远程Unix shell协议的替代品\nAcross the Wall\nVPN协议和NAT协议都是通过重新构建一个IP首部来实现的，但他们的实现又有区别，VPN是将内部IP数据报加密后打包成外部IP数据报的数据部分，它的主要目的是为了数据的保密性，而NAT是纯进行地址转换，它的目的是为了解决本地编址的内部网络与外网通信的问题。\nVPN的实现主要使用了两种基本技术：隧道传输 和 加密技术\nVPN 通常处于网络层（第三层）或数据链路层（第二层）：\n\nIP层的VPN，如IPsec或L2TP，通常工作在网络层，它们通过创建虚拟的私有网络来路由IP数据包。\n数据链路层的VPN，如PPTP，工作在数据链路层，通常用于点对点通信。\n\nWARP是建立在Cloudflare 1.1.1.1的免费DNS服务器上。从技术上讲，Warp本质上就是一款VPN服务，Warp使用来自1.1.1.1的DNS服务器，并对它们之间的所有流量进行加密。\n通常，当你访问http://Baidu.com时，URL会被翻译成网站所在服务器的IP地址，由ISP的DNS服务器托管。当你使用Warp时，Warp将手机上的DNS服务器固定为1.1.1.1。因此，所有请求都转到Cloudflare的安全服务器。\nWarp在此基础上增加了一个额外的流程，使得设备和Cloudflare服务器之间的所有流量都是加密流量。但Warp是基于WireGuard隧道的UDP协议，中国大陆绝大部分运营商都会对这类流量进行惩罚式、限速式的限制策略，导致Warp在大陆使用上突发很高，但均速很低。(只能用于failover 😅 再加上Cloudflare的许多IP被国内Block，使得Warp在大陆接近一个不可用的状态。\nWireGuard 是由 Jason Donenfeld 等人用 C 语言编写的一个开源 VPN 协议，被视为下一代 VPN 协议，旨在解决许多困扰 IPSec/IKEv2、OpenVPN 或 L2TP 等其他WireGuard 就是采用 UDP 转发流量的 VPN 工具。他最大的优点也就是最大的缺点，采用 UDP 转发流量确实是能够有效的干扰墙的封锁，但是其稳定性实在是不敢恭维。\nClash是一个开源的多协议代理工具，可以用于实现网络流量的代理和转发。它支持多种代理协议（如Shadowsocks、VMess、Trojan、Socks5等）和路由规则，能够实现灵活的网络流量控制和代理功能。\ntrojan是近些年兴起的网络工具，项目官网 https://github.com/trojan-gfw。与**强调加密**和混淆的SS/SSR等工具不同，trojan将通信流量伪装成互联网上最常见的https流量，从而有效防止流量被检测和干扰。在敏感时期，基本上只有**trojan和 v2ray伪装 能提供稳如狗的体验**。\n想要长期稳定高效的科学上网，socks5 类型的代理基本是必须要掌握的。\nSOCKS工作在比HTTP代理更低的层次Session：SOCKS使用握手协议来通知代理软件其客户端试图进行的SOCKS连接，然后尽可能透明地进行操作，而常规代理可能会解释和重写报头（例如，使用另一种底层协议，例如FTP；然而，HTTP代理只是将HTTP请求转发到所需的HTTP服务器）。虽然HTTP代理有不同的使用模式，HTTP CONNECT方法允许转发TCP连接；然而，SOCKS代理还可以转发UDP流量（仅SOCKS5），而HTTP代理不能。HTTP代理通常更了解HTTP协议，执行更高层次的过滤（虽然通常只用于GET和POST方法，而不用于CONNECT方法）。\nsocks5 类型的代理服务器在网络层级上是工作于应用层的会话层，很多流量都无法代理，因从即便是开了所谓的全局，也不能给游戏加速，毕竟游戏的网络传输一般都是跑在传输层的。像 Ping 和 Trace 这些 ICMP 命令自然也是无法通过代理的。（当然也有方法可以用软件强制接管虚拟网卡达到真全局的目的，比如 SSTAP，tun2socks 等等）\n如果客户端和服务器都可以独立发包，但是偶尔发生延迟可以容忍（比如：在线的纸牌游戏，许多MMO类的游戏），那么可考虑使用TCP长连接如果客户端和服务器都可以独立发包，而且无法忍受延迟（比如：大多数的多人动作类游戏，一些MMO类游戏），那么考虑使用UDP加速器原理 加速器的原理很简单，就是UDP代理\n主要难在两点，其一是怎么处理游戏客户端到加速器服务器之间的UDP连接，其二是怎么让游戏客户端去连接这个加速器（一般游戏客户端是没有设置代理服务器的功能的）\n处理UDP有两种思路，一种是协议套娃，将游戏的UDP包外面套一层TCP（UDP over TCP ），到了目的地再把TCP解包成UDP，最后在发送到游戏服务器，返回的数据包也做同样处理；另外一种是伪造TCP（FakeTCP），对UDP数据包加上伪造的TCP包头，让其看起来像是TCP协议，欺骗运营商。\n主动检测\nHTTP所有没有正确结构和密码的连接都将被重定向到预设端点,因此,如果可疑探针连接(或者只是您的粉丝连接到您的博客XD),木马服务器的行为与该端点完全相同(默认情况下)。\n被动检测\n因为流量受到保护TLS(用户有责任使用有效的证书),所以如果你正在访问一个HTTP站点,流量看起来\n是一样的(握手后HTTPS只有一个);如果您没有访问某个站点,那么流量看起来与“保持活动状态”或“保持活动状态“相同。因此,木马还可以绕过ISP限制。RTT TLS HTTP HTTPS WebSocket QoS\nHTTPS加密\n直接把 TLS 1.2 升级成 TLS 1.3，TLS 1.3 大幅度简化了握手的步骤，完成 TLS 握手只要 1 RTT，而且安全性更高。\n在 TLS 1.2 的握手中，一般是需要 4 次握手，先要通过 Client Hello （第 1 次握手）和 Server Hello（第 2 次握手） 消息协商出后续使用的加密算法，再互相交换公钥（第 3 和 第 4 次握手）\nSSL握手的步骤如下：\n\nSSL或TLS客户端先向服务端发送一个加密通信请求，叫做ClientHello请求。该请求包含以下信息：\n\n客户端支持的SSL或者TLS版本\n客户端生成的随机数，用于生成后续通信的随机字符串（“对话密钥”）\n客户端支持的加密算法\n\n\nSSL或TLS服务端收到客户端请求后，向客户端发出响应，叫做ServerHello。该响应包含以下信息：\n\n服务端从客户端提供的SSL或TLS列表中选择的版本\nSesstion ID 和 另外生成的随机数\n服务端的数字证书（如果服务端需要用于客户端身份验证的数字证书，则服务端发送一个客户端证书请求，其中包含受支持的证书类型列表和可接受的认证机构(CAs)的专有名称。）\n确认使用的加密算法\n\n\n客户端收到服务端响应后，首先校验服务端发来的数字证书决定是否继续通信。\nTLS 第三次握手    客户端验证完证书后，认为可信则继续往下走。 接着，客户端就会生成一个新的随机数 (pre-master)，用服务器的 RSA 公钥加密该随机数，通过「Client Key Exchange」消息传给服务端。 服务端收到后，用 RSA 私钥解密，得到客户端发来的随机数 (pre-master)。 至此，客户端和服务端双方都共享了三个随机数，分别是 Client Random、Server Random、pre-master。 于是，双方根据已经得到的三个随机数，生成会话密钥（Master Secret），它是对称密钥，用于对后续的 HTTP 请求/响应的数据加解密。 生成完「会话密钥」后，然后客户端发一个「Change Cipher Spec」，告诉服务端开始使用加密方式发送消息。\n如果服务端发送了一个客户端证书请求，客户端将会发送一个用客户端私钥加密的随机字符串和客户端的数字证书，或者没有数字证书的警告。在某些强制客户端证书的实现中，如果客户端没有数字证书，则握手会失败\n服务端接受并验证客户端证书\n客户端向服务端发送一条完成的消息，该消息使用密钥加密，表示握手的客户端部分已经完成。\n服务端向客户端发送一条完成的消息，该消息使用密钥加密，表示握手的服务端部分已经完成\n在SSL或TLS会话期间，服务端和客户端现在可以交换使用共享密钥对称加密的消息\n\n\nA rainbow table is a precomputed table of passwords and their hashes,\n彩虹表对包含大量盐的单向哈希无效。例如，考虑使用以下函数生成的密码哈希（其中“ + ”是串联运算符）：\nsaltedhash(password) = hash(password + salt)\n要么\nsaltedhash(password) = hash(hash(password) + salt)\nsalt 值不是秘密的，可以随机生成并与密码哈希一起存储。大盐值通过确保每个用户的密码被唯一地散列来防止预计算攻击，包括彩虹表。这意味着具有相同密码的两个用户将具有不同的密码哈希值（假设使用不同的盐）。为了成功，攻击者需要为每个可能的盐值预先计算表。salt 必须足够大，否则攻击者可以为每个 salt 值制作一个表。对于使用 12 位盐的旧Unix 密码，这将需要 4096 个表，这会显着增加攻击者的成本，但对于 TB 硬盘驱动器来说并非不切实际。SHA2-crypt和bcrypt方法——用于Linux、BSD Unixes 和Solaris — 有 128 位的盐。[4]这些较大的盐值使得针对这些系统的预计算攻击对于几乎任何长度的密码都不可行。即使攻击者可以每秒生成一百万张表，他们仍然需要数十亿年才能为所有可能的盐生成表。 Or by key  strengthening\nInjection—     Could try to “sanitise” (clean/make safe) data input, but there is a better solution. • Do not create SQL (or similar statements) by adding together strings. • Can use special routines designed to produce these statements. • Languages designed for the web contain functions to help with this. • In Java, PreparedStatement is a class to do this.\nSocket\n一个是 fread/fwrite 读写，一个是 recv 和 send 读写（在 Linux 下你用 read 和 write 的话，文件和 socket 两者都能读写，只是无法直接设置一些特殊的 flag）。\n一般的文件以及 socket 客户端读写的都是数据，而 socket 服务端 accept 读出来的是可以读写的客户端文件。\n\n监听套接字 (Listening Socket)：这是服务器端的套接字，通过调用**bind、listen和accept函数来建立。监听套接字用于等待客户端的连接请求，当客户端请求连接时，accept**函数会返回一个新的已完成连接套接字。\n已完成连接套接字 (Connected Socket)：这是服务器端的套接字，也是客户端的套接字。它们通过**connect函数（客户端）或accept函数（服务器端）建立连接后，用于实际的数据传输。这些套接字可以通过read和write**函数来进行数据的读取和写入。\n\n成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。\n严格来讲，“网关”是一个逻辑概念，【不要】把它当成具体的网络设备。充当“网关”的东东，可能是：路由器 or XX层交换机 or XX层防火墙 or 代理服务器 …\n“网关”也分不同的层次。如果不加定语，通常指的是“3层网关”（网络层网关）。列几种比较常见的，供参考：\n路由器充当网关——3层（网络层）\n3层交换机充当网关——3层（网络层）\n4层交换机充当网关——4层（传输层）\n应用层防火墙充当网关——7层（应用层）\n代理服务器充当网关——（取决于代理的层次，参见前一个小节）\n“隧道协议”可以做到更灵活的包裹——既可以对层次相隔很远的协议进行包裹，也可以对同一层的协议进行包裹，甚至可以“倒挂”——所谓的“倒挂”就是让【上】层反过来包裹【下】层。\n举例：\n俺曾经写过一篇《\n如何让【不支持】代理的网络软件，通过代理进行联网（不同平台的 N 种方法）\n》，其中介绍了“HTTP 代理”的两种模式：“转发模式 ＆ 隧道模式”。对于“HTTP 代理”的隧道模式，可以实现【TCP over HTTP】（把 TCP 协议打包到 HTTP 协议内部）\nTransport\n**屏蔽“有连接 or 无连接”的差异  网络层的定位是“抽象物理层和数据链路层，提供端到端”的通信能力。**当网络部署全世界的时候，最初的设计者认为网络重路由、乱序的概率，很容易会高过了有连接服务能够容忍的概率。毕竟，有连接服务要先付出代价“建立连接”来换取“通信过程中的高效”。一旦遇到网络重路由或者乱序，就要重新付出“建立连接”的代价。**尽管网络层本身是无连接和不可靠的，但上层协议，如传输层的TCP，可以提供可靠性和连接性，以确保数据的正确传输和顺序。**这种分层的设计有助于网络的模块化，不同层次可以独立开发和升级，从而促进了互联网的增长和演进。\nUDP（User Datagram Protocol）和TCP（Transmission Control Protocol）是两种常用的传输层协议，用于在计算机网络中传输数据。它们在性质、特点和用途上有许多区别，下面是它们的主要区别：\n\n连接性：\n\nTCP：是一种面向连接的协议，建立连接后会在通信双方之间建立可靠的数据传输通道。数据按顺序传输，确保可靠性，且会进行确认和重传以保证数据的完整性。\nUDP：是一种无连接的协议，发送方将数据报发送到目标地址，接收方直接接收，没有建立持续的连接，不进行确认和重传。\n\n\n可靠性：\n\nTCP：保证数据的可靠传输，通过序号、确认和重传机制来确保数据的有序性和完整性。适用于需要确保数据不丢失和按顺序到达的应用，如网页浏览、文件传输等。\nUDP：不保证数据的可靠传输，数据发送后不会收到确认。适用于实时性要求较高，可以容忍一些数据丢失的应用，如音频、视频流等。\n\n\n流控制：\n\nTCP：具有流控制机制，通过滑动窗口协商发送方和接收方之间的数据传输速率，避免数据过载。\nUDP：没有流控制机制，发送方将数据发送出去，不会根据接收方的接收能力进行调整。\n\n\n带宽和延迟：\n\nTCP：由于其确认和重传机制，以及流控制，可能会引入一些额外的带宽消耗和延迟。\nUDP：通常具有较低的延迟和带宽消耗，适合实时通信和多播等场景\n\n\n\nNetwork层\n网络层的两种交换技术：电路交换（有连接） VS 分组交换（无连接）。\nIP/ARP 是根据IP地址获取MAC地址的一种协议。/ICMP\n\nIP地址： IP协议使用IP地址来唯一标识网络上的每个设备。IPv4和IPv6是两个常见的IP地址版本。IPv4使用32位地址，而IPv6使用128位地址，提供了更广泛的地址空间以应对互联网的增长需求。\n面向无连接： IP是一种面向无连接的协议，这意味着每个数据包（或数据报）都是独立的，不需要在通信之前建立连接。每个数据包独立传输，因此不会维护通信状态。\n不可靠传输： IP提供了不可靠的传输，这意味着它不保证数据包的传输顺序、可靠性或交付。数据包可能会在传输过程中丢失、重复、延迟或乱序，因此上层协议（如TCP）负责处理可靠性和顺序问题。\n\n\n根据RFC 791，IP地址是一个32位的二进制数字，通常表示为四个八位字节的点分十进制表示法，如xxx.xxx.xxx.xxx。\n\nA类地址：\n\nA类地址的第一个字节的最高位始终为0，这表示A类地址的范围是1.0.0.0到126.0.0.0。\n这类地址通常用于大型网络，因为其范围允许约1670万个主机地址。\n\n\nB类地址：\n\nB类地址的前两个字节的最高两位始终为10，这表示B类地址的范围是128.0.0.0到191.255.0.0。\nB类地址通常用于中等规模的网络，可容纳约6.5万个主机地址。\n\n\nC类地址：\n\nC类地址的前三个字节的最高三位始终为110，这表示C类地址的范围是192.0.0.0到223.255.255.0。\nC类地址通常用于小型网络，每个C类网络可以容纳约254个主机地址。\n\n\n\n在实际网络中，已经有许多其他IP地址分配方案和规则，包括子网掩码、无类域间路由（CIDR）等，这些使得IP地址的分配更加灵活和高效。所以，不再严格使用A、B、C类地址来划分网络规模。\nVLSM(可变长子网掩码) 是为了有效的使用无类别域间路由（CIDR）和路由汇聚(route summary)来控制路由表的大小，它是网络管理员常用的IP寻址技术\nCIDR无类网络是一种相对于有类网络的网络，无类网络IP地址的掩码是变长的。\n在有类网络的基础上，拿出一部分主机ID作为子网ID。\n例如：\nIP地址为192.168.250.44 子网掩码不能是小于24位。\n因为这是一个C类地址（前3Bytes是网络号），子网掩码只能大于24位。\n而掩码255.255.248.0（21位）是不符合规定的。\n·\n如果一个网络中的主机有100台，\n那么，可以用子网掩码/25来划分这个C类网络（“192.168.250.0/24”）：\n划分成192.168.250.0/25 和192.168.250.128/25两个子网。\n—主机192.168.250.44/25 属于子网192.168.250.0/25。\n**路由器用于不同网络之间的通信，进行跨网络的路由决策，而交换机用于内部网络的局域网络通信，将数据帧从一个接口转发到另一个接口。**在许多网络中，路由器和交换机通常是一起使用的，以实现内部通信和与外部网络的连接。\n路由器（Router）：\n\n网络层设备： 路由器位于OSI模型的网络层，负责在不同网络之间进行数据包的转发和路由选择。\n跨网络通信： 路由器用于将数据包从一个网络传送到另一个网络，通常在不同IP子网之间执行路由操作。\n决策基于IP地址： 路由器的路由决策是基于目标IP地址进行的，它查找路由表以确定数据包应该被转发到哪个接口或下一个路由器。\n网络分割和隔离： 路由器可以分隔不同的网络，提供网络隔离和安全性。\n网络地址转换（NAT）： 一些路由器支持NAT，允许多个设备共享一个公共IP地址。\n\n交换机（Switch）：\n\n数据链路层设备： 交换机位于OSI模型的数据链路层，主要用于在局域网络（LAN）内的设备之间进行数据帧的交换。\n内部局域网络通信： 交换机用于在同一网络内的设备之间传输数据，通常在相同IP子网内工作。\n决策基于MAC地址： 交换机的决策是基于目标设备的MAC地址进行的，它使用MAC地址表来确定数据帧应该被发送到哪个接口。\n高性能： 交换机通常提供高性能的数据交换，因为它们在硬件级别进行操作，不需要进行复杂的路由选择。\n无状态： 交换机通常是无状态设备，不存储关于通信的历史信息，而路由器可能会维护路由表和状态信息。\n\nLinux\nLinus Torvalds 创建了前者。后者是全球数百万开发人员之间的协作，涉及GNU 项目, Linux 内核开发团队由 Torvalds 领导， X Window 系统的各种开发人员在过去的 29 年中，以及其他。这就是自由软件基金会要求将使用来自 GNU 项目的软件的完整 Linux 操作系统称为“GNU/Linux”的原因。全称GNU/Linux，是一种免费使用和自由传播的类UNIX操作系统，其内核由林纳斯·本纳第克特·托瓦兹于1991年10月5日首次发布，它主要受到Minix和Unix思想的启发，是一个基于POSIX的多用户、多任务、支持多线程和多CPU的操作系统。它能运行主要的Unix工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。Linux有上百种不同的发行版，如基于社区开发的debian、archlinux，和基于商业开发的Red Hat Enterprise Linux、SUSE、Oracle Linux等。\nLinus built kernel to solve unix hard-use and lack I/O RPC function.  区分 Linux（内核）和 Linux（操作系统）。\nKernel 是计算机操作系统核心的计算机程序，通常可以完全控制系统中的所有内容**。**[1]它是操作系统代码的一部分，始终驻留在内存中[2]并促进硬件和软件组件之间的交互。完整的内核通过设备驱动程序控制所有硬件资源（例如 I/O、内存、密码） ，仲裁涉及这些资源的进程之间的冲突，并优化公共资源的利用，例如 CPU 和缓存使用、文件系统和网络套接字。在大多数系统上，内核是启动时最先加载的程序之一（在引导加载程序）。它处理其余的启动以及内存、外围设备和来自软件的输入/输出(I/O) 请求，将它们转换为中央处理器的数据处理指令。\n内核的关键代码通常被加载到一个单独的内存区域，该区域受到保护，不会被应用程序软件或操作系统的其他不太重要的部分访问。内核在这个受保护的内核空间中执行其任务，例如运行进程、管理硬盘等硬件设备以及处理中断。相比之下，浏览器、文字处理器或音频或视频播放器等**应用程序使用单独的内存区域，即用户空间。**这种分离可以防止用户数据和内核数据相互干扰并导致不稳定和缓慢，[1]以及防止出现故障的应用程序影响其他应用程序或使整个操作系统崩溃。即使在内核包含在应用程序地址空间中的系统中，内存保护也用于防止未经授权的应用程序修改内核。\n内核的接口是一个低级 抽象层。当进程向内核请求服务时，它必须调用系统调用，通常是通过包装函数。\n有不同的内核架构设计。整体内核完全在单个地址空间中运行，CPU 在管理模式下执行，主要是为了提高速度。微内核在用户空间中运行大部分但不是所有服务，[3]就像用户进程一样，主要是为了弹性和模块化。[4] MINIX 3是微内核设计的一个著名例子。相反，Linux 内核是整体式的，尽管它也是模块化的，因为它可以在运行时插入和删除可加载的内核模块。\n计算机系统的这个中央组件负责执行程序。内核负责随时决定将许多正在运行的程序中的哪些分配给处理器\n$表明是非root用户登录，#表示是root用户登录，它们是终端shell的命令提示符 几种常用终端的命令提示符\nBASH: root账户: # ,非root账户: You can't use 'macro parameter character #' in math mode KSH: root账户: # ,非root账户:  CSH[TCSH]: root账户: % ,非root账户: %\n而/ 是根节点， ~ 是 home如果以root账号登陆   ~ 是 /root/\nProgram: a file containing instructions to be executed, static Process: an instance of a program in execution, live entity\nexecve（执行文件）在父进程中fork一个子进程，在子进程中调用exec函数启动新的程序。exec函数一共有六个，其中execve为内核级系统调用，其他（execl，execle，execlp，execv，execvp）exec(): often used after fork() to load another process\n提出一个问题：当在O_APPEND打开后，然后用 lseek移动到其他的位置，然后再用write写，这个时候，请问你数据写到哪里去了？\n是在末端，还是lseek移动到得位置。答案是在末端。\n因为 O_APPEND打开后，是一个原子操作：移动到末端，写数据。这是O_APPEND打开的作用。中间的插入时无效的\n通信双方交流的信息单元（比特、字节、字、双字等等）应该以什么样的顺序进行传送。如果不达成一致的规则，通信双方将无法进行正确的编/译码从而导致通信失败。Big Endian 和 Little Endian\nFTP\n主动模式的FTP是指服务器主动连接客户端的数据端口，被动模式的FTP是指服务器被动等待客户端连接自己的数据端口。\n被动模式的FTP通常用在处于防火墙之后的FTP客户访问外界FTP服务器的情况，因为在这种情况下，防火墙通常配置为不允许外界访问防火墙之内的主机，而只允许由防火墙之内的主机发起对外的连接请求。因此，在这种情况下不能使用主动模式的FTP传输，而被动模式的FTP可以良好的工作。\n主动模式需要服务器主动向客户端发起连接，而现在普通的客户端大多位于NAT之后，所以主动模式常常无法进行。即使可以，也需要客户端打开防火墙，允许服务端的20端口访问。所以服务端基本需要支持被动模式，但被动模式需要开放一段端口。为了安全，可以选择一小段端口，然后在防火墙开放这一段端口。\nfork()    它不接受任何参数并返回一个整数值。下面是 fork() 返回的不同值。\n负值：创建子进程不成功。零：返回新创建的子进程。正值：返回给父级或调用者。该值包含新创建的子进程的进程 ID。\n系统调用，父进程调用fork会创建一个进程副本，代码中还可以通过fork返回值是否为0来区分是子进程还是父进程。fork 系统调用用于创建一个新进程，称为子进程，它与进行 fork() 调用的进程（父进程）同时运行。创建新的子进程后，两个进程都将执行 fork() 系统调用之后的下一条指令。子进程使用与父进程相同的 pc（程序计数器）、相同的 CPU 寄存器、相同的打开文件。不同线程\nfork是用来创建子进程的，这个函数的特别之处在于一次调用，两次返回，一次返回到父进程中，一次返回到子进程中，我们可以通过返回值来判断其返回点：\npid_t child = fork(); if( child &lt; 0  ) {     //fork error.     perror(\"fork process fail.\\n\"); } else if( child ==0  ) {   // in child process     printf(\" fork succ, this run in child process\\n \"); } else {                        // in parent process     wait(); printf(\" this run in parent process\\n \"); }\n\nfork只能创建调用该函数的线程的副本，进程中其他运行的线程，fork不予处理。这就意味着，对于多线程程序而言，寄希望于通过fork来创建一个完整进程副本是不可行的。\ncd /去root目录   cd ~去当前用户所在目录\nLinux 里利用 grep 和 find 命令查找文件内容\n从文件内容查找匹配指定字符串的行：\n$ grep \"被查找的字符串\" 文件名\n例子：在当前目录里第一级文件夹中寻找包含指定字符串的 .in 文件\ngrep \"thermcontact\" /.in\n从文件内容查找与正则表达式匹配的行：\n$ grep –e \"正则表达式\" 文件名\n查找时不区分大小写：\n$ grep –i \"被查找的字符串\" 文件名\ncmd\n~代表你的/home/用户明目录                 ls -a 查看隐藏\n单点故障( SPOF ) 是系统的一部分，如果发生故障，整个系统将停止工作\nSystems can be made robust by adding redundancy in all potential SPOFs. Redundancy can be achieved at various levels.\nThe assessment of a potential SPOF involves identifying the critical components of a complex system that would provoke a total systems failure in case of malfunction. Highly reliable systems should not rely on any such individual component.\nMIS\n3 main types of metadata: descriptive, administrative, and use\n\nDescriptive metadata enables discovery, identification, and selection of resources. It can include elements such as title, author, and subjects.\nAdministrative metadata facilities the management of resources. It can include elements such as technical, preservation, rights, and use.\nUse metadata, generally used in machine processing, describes relationships among various parts of a resource, such as chapters in a book.Data exhaust is the data generated as a byproduct of people’s online actions and choices. Data exhaust consists of the various files generated by web browsers and their plug-ins such as cookies, log files, temporary internet files and and . sol files (flash cookies).\n\nSecurity Concerns with JSON\nJSON.parse()只会将标准的Json字符串(key和value都由双引号引起来，最外面用单引号括住)转为JSON对象。\neval()在转换字符串的时候是比较松的，即使不是标准的Json字符串也会被转换成Json对象。更重要的是eval()方法会执行要解析的字符串中的代码，这一点是十分危险的。\nXML\nRule= element names must NOT have a space\n&lt;1&gt;banned&lt;/1&gt; – Rule=tags cannot start with a number ◆ banned – Rule= there are a few reserved names, e.g. xml\nXML Schema Definition，通常简称为XSD，是一种精确描述XML语言的方法。 XSD根据相应XML语言的语法规则检查XML文档的结构和词汇的有效性。\n//更多请阅读：https://www.yiibai.com/xsd/xsd_overview.html\nRefsnes361970-03-27\n这是相应的简易元素定义：\n&lt;xs:element name=“lastname” type=“xs:string”/&gt;\n&lt;xs:element name=“age” type=“xs:integer” default=””/&gt;\n&lt;xs:element name=“dateborn” type=“xs:date”/&gt;\n兰伯特在论文《The Byzantine Generals Problem》中提到的口信消息型拜占庭问题之解：如果叛将人数为 m，将军人数不能少于 3m + 1 ，那么拜占庭将军问题就能解决了。\n拜占庭将军问题描述的是最困难的，也是最复杂的一种分布式故障场景，除了存在故障行为，还存在恶意行为的一个场景。你要注意，在存在恶意节点行为的场景中（比如在数字货币的区块链技术中），必须使用拜占庭容错算法（Byzantine Fault Tolerance，BFT）。除了故事中提到两种算法，常用的拜占庭容错算法还有：PBFT 算法，PoW 算法。\n计算机分布式系统中，最常用的是非拜占庭容错算法，即故障容错算法（Crash Fault Tolerance，CFT）。CFT 解决的是分布式的系统中存在故障，但不存在恶意节点的场景下的共识问题。 也就是说，这个场景可能会丢失消息，或者有消息重复，但不存在错误消息，或者伪造消息的情况。常见的算法有 Paxos 算法、Raft 算法、ZAB 协议。\n区别与相似点：\n\n可理解性：Raft 相对于 Paxos 更易于理解和实现。Paxos 的理解和实现较为复杂，而 Raft 设计的目标是提供更清晰的算法描述。\n领导选举：在共识算法中，领导者（Leader）的选举是一个关键问题。Paxos 和 Raft 都涉及领导选举，但在实现细节上存在差异。\n日志复制：Paxos 和 Raft 都通过在节点之间复制日志来实现数据的一致性。Raft 将日志复制问题划分为更容易管理的几个阶段。\n\n无论是 Paxos 还是 Raft，它们都是为解决分布式系统中的数据一致性，共识问题而设计的。\n在Paxos算法中，有三种角色：\nProposer：提案Proposal提出者\nAcceptor：决策者，可以批准议案          Learner：最终决策的学习者\nPaxos算法安全性前提如下：\n只有被提出的value才能被选定。\n只有一个value被选定，并且如果某个进程认为某个value被选定了，那么这个value必须是真的被选定的那个。\nPaxos算法类似于两阶段提提交\n————————————————\nRaft是一种用于替代Paxos的共识算法。相比于Paxos，Raft的目标是提供更清晰的逻辑分工使得算法本身能被更好地理解，同时它安全性更高，并能提供一些额外的特性。\nRaft makes several guarantees to the user – only one will be discussed here.\n• Election safety: at most one leader can be elected in a given term.\n• Uses a “heartbeat” mechanism: All nodes continually receive messages from the leader\n\n负载均衡\n负载均衡（Load Balancing）是计算机网络和服务器管理中的一项关键技术，用于分配和管理网络流量、请求或工作负载，以确保系统的稳定性、可靠性和性能。它主要用于分散来自用户或客户端的请求，以均匀分布到多个服务器或资源上，从而避免单一服务器过载，提高系统的可用性和响应速度。\n以下是一些关于负载均衡的重要概念和工作原理：\n\n负载均衡器（Load Balancer）：负载均衡器是位于网络架构中的设备或软件，它接收传入的请求，并根据一定的策略将这些请求分发到一组后端服务器。负载均衡器可以是硬件设备，也可以是软件应用程序，例如NGINX、Apache HTTP Server的负载均衡模块，或云服务提供商的负载均衡服务。\n负载均衡算法：负载均衡器使用不同的算法来决定如何分配请求。一些常见的算法包括轮询（Round Robin）、最少连接（Least Connections）、IP哈希（IP Hash）、权重轮询（Weighted Round Robin）等。\n\n是防止有的人干死了，有的人闲死了，来根据能力（CPU、IO）分配一下工作量\n现有的负载均衡算法主要分为静态和动态两类。静态负载均衡算法以固定的概率分配任务，不考虑服务器的状态信息，如轮转算法、加权轮转算法等；动态负载均衡算法以服务器的实时负载状态信息来决定任务的分配，如最小连接法\n常用的负载均衡策略\n1.轮询(Round Robin)\n按顺序将请求依次分配给服务器，每个服务器按照顺序依次接收请求 适用于服务器性能相近的情况。\n2.最小连接数(Least Connection)\n将请求分配给当前连接数最少的服务器，确保负载相对均衡，适用于长连接的场景。\n3.最少响应时间(Least Response Time)\n将请求分配给响应时间最短的服务器，确保客户端能够获得最快的响应，适用于对响应时间要求较高的场景。\n4.IP哈希(IP Hash)\n根据请求的源IP地址计算哈希值，将同一IP的请求分配给同一台服务器，保证特定客户端的请求都发送到同一服务器，适用于需要会话保持的应用。\n5.加权轮询(Weighted Round Robin)\n根据服务器的权重值，按比例分配请求，权重高的服务器接收到的请求数更多。\n\nNginx\nnginx [engine x] 是一个 HTTP 和反向代理服务器、邮件代理服务器和通用 TCP/UDP 代理服务器。\n由俄罗斯的程序设计师Igor Sysoev所开发以解决C10K问题，官方测试nginx能够支支撑5万并发链接，软件负载均衡： 在应用程序级别实现负载均衡，实现请求的分发。例如，使用特定框架或库，将请求分发给不同的服务实例或节点。这种方法相对来说灵活性较高，但也需要在应用程序代码中实现和维护负载均衡逻辑。\n2.\n并且cpu、内存等资源消耗却非常低，运行非常稳定。\n反向代理是负载均衡的实现方式之一 这个问题描述是性能，只能代理到多个节点，代理单一节点对性能提升明显是没意义的。或者如某个回答说的，应该是提升吞吐量，性能的描述不是很准确。\n反向代理隐藏了真实的服务端，当我们请求 www.baidu.com 的时候，就像拨打10086一样，背后可能有成千上万台服务器为我们服务，但具体是哪一台，你不知道，也不需要知道，你只需要知道反向代理服务器是谁就好了，www.baidu.com 就是我们的反向代理服务器，反向代理服务器会帮我们把请求转发到真实的服务器那里去。\nLVS四层负载均衡集群\n总结：从上面的对比看来四层负载与七层负载最大的区别就是效率与功能的区别。四层负载架构设计比较简单，无需解析具体的消息内容，在网络吞吐量及处理能力上会相对比较高，而七层负载均衡的优势则体现在功能多，控制灵活强大。在具体业务架构设计时，使用七层负载或者四层负载还得根据具体的情况综合考虑。\n一致性Hash算法\nIP_hash将同一IP的请求分配给同一台服务器\n取模哈希函数中基数的变化，这样会导致大部分映射关系改变一致性哈希算法就很好地解决了分布式系统在扩容或者缩容时，发生过多的数据迁移的问题。\n当网站分配请求时，考虑到每个节点的硬件配置有所区别，我们可以引入权重值，将硬件配置更好的节点的权重值设高，然后根据各个节点的权重值，按照一定比重分配在不同的节点上，让硬件配置更好的节点承担更多的请求，这种算法叫做加权轮询。\n加权轮询算法使用场景是建立在每个节点存储的数据都是相同的前提。所以，每次读数据的请求，访问任意一个节点都能得到结果。\n但是，加权轮询算法是无法应对「分布式系统（数据分片的系统）」的，因为分布式系统中，每个节点存储的数据是不同的。\n当我们想提高系统的容量，就会将数据水平切分到不同的节点来存储，也就是将数据分布到了不同的节点。比如一个分布式 KV（key-value） 缓存系统，某个 key 应该到哪个或者哪些节点上获得，应该是确定的，不是说任意访问一个节点都可以得到缓存结果的。\n因此，我们要想一个能应对分布式系统的负载均衡算法\n一致性哈希要进行两步哈希：\n\n第一步：对存储节点进行哈希计算，也就是对存储节点做哈希映射，比如根据节点的 IP 地址进行哈希（mod 2^32）；\n第二步：当对数据进行存储或访问时，对数据进行哈希映射；\n\n所以，一致性哈希是指将「存储节点」和「数据」都映射到一个首尾相连的哈希环上。增加或删除节点时只对后继节点产生影响，但数据分配不均匀可能会导致大量请求访问同一个节点\n使用虚拟节点除了会提高节点的均衡度，还会提高系统的稳定性。当节点变化时，会有不同的节点共同分担系统的变化，因此稳定性更高。\n比如，当某个节点被移除时，对应该节点的多个虚拟节点均会移除，而这些虚拟节点按顺时针方向的下一个虚拟节点，可能会对应不同的真实节点，即这些不同的真实节点共同分担了节点变化导致的压力。\n而且，有了虚拟节点后，还可以为硬件配置更好的节点增加权重，比如对权重更高的节点增加更多的虚拟机节点即可。\n因此，带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景。\n哲学家就餐问题（英语：Dining philosophers problem）是在计算机科学中的一个经典问题，用来演示在并发计算中多线程同步（Synchronization）时产生的问题。\n服务生解法[编辑]\n一个简单的解法是引入一个餐厅服务生，哲学家必须经过他的允许才能拿起餐叉。因为服务生知道哪只餐叉正在使用，所以他能够作出判断避免死锁。\n为了演示这种解法，假设哲学家依次标号为A至E。如果A和C在吃东西，则有四只餐叉在使用中。B坐在A和C之间，所以两只餐叉都无法使用，而D和E之间有一只空余的餐叉。假设这时D想要吃东西。如果他拿起了第五只餐叉，就有可能发生死锁。相反，如果他征求服务生同意，服务生会让他等待。这样，我们就能保证下次当两把餐叉空余出来时，一定有一位哲学家可以成功的得到一对餐叉，从而避免了死锁。\n资源分级解法\n另一个简单的解法是为资源（这里是餐叉）分配一个偏序或者分级的关系，并约定所有资源都按照这种顺序获取，按相反顺序释放，而且保证不会有两个无关资源同时被同一项工作所需要。在哲学家就餐问题中，资源（餐叉）按照某种规则编号为1至5，每一个工作单元（哲学家）总是先拿起左右两边编号较低的餐叉，再拿编号较高的。用完餐叉后，他总是先放下编号较高的餐叉，再放下编号较低的。在这种情况下，当四位哲学家同时拿起他们手边编号较低的餐叉时，只有编号最高的餐叉留在桌上，从而第五位哲学家就不能使用任何一只餐叉了。而且，只有一位哲学家能使用最高编号的餐叉，所以他能使用两只餐叉用餐。当他吃完后，他会先放下编号最高的餐叉，再放下编号较低的餐叉，从而让另一位哲学家拿起后边的这只开始吃东西。\n尽管资源分级能避免死锁，但这种策略并不总是实用的，特别是当所需资源的列表并不是事先知道的时候。例如，假设一个工作单元拿着资源3和5，并决定需要资源2，则必须先要释放5，之后释放3，才能得到2，之后必须重新按顺序获取3和5。对需要访问大量数据库记录的计算机程序来说，如果需要先释放高编号的记录才能访问新的记录，那么运行效率就不会高，因此这种方法在这里并不实用。\nChandy/Misra解法\n允许任意的用户争用任意数量的资源。与资源分级解法不同的是，这里编号可以是任意的。\n高并发解决\n在操作系统中，是指一个时间段中有几个程序都处于已启动运行到运行完毕之间，且这几个程序都是在同一个处理机上运行。其中两种并发关系分别是同步和互斥\n假如有1秒钟有1万个请求，我们的apache服务器的连接数500，处理业务的时间为100ms，4台服务器的连接数1秒内能处理500*4/0.1=2万(QPS)，完全满足这个要求了，但实际情况是随着请求数的增加，机器处于高负荷的状态，CPU切换次数增大会严重影响处理时长，很有可能由100ms变成500ms甚至更长，此时1秒钟处理的连接数为500**4/0.5=4000，这就会导致大量的请求阻塞.时间越长阻塞越大。根据用户的行为特征，越是阻塞就越会去点击请求这样就会雪上加霜，最终拖垮服务进程\n二、解决方案\n1、前后端分离\n前端采用静态html页面，这样就不需要进行视图渲染了，减少性能开销。一些静态页面、css样式、js可以采用CDN加速(Http cache-control。前后端分离可以独立发布，前端有问题，可以直接改好发布而不需要重启服务端，一些大型项目启动服务端会耗费很长时间，如果此时访问量很大的话就会出现阻塞现象。\n2、服务端接口处理速度要快\n1）引入缓存\n就比如上面的场景，优化代码缩短处理时间也是可以解决的。像这种高并发的情况，能不查数据库就不查数据库，因为数据库的连接也是有限的，比如mysql连接z数好像在500左右，而且连接数据库也是耗时间的。这种情况可以想办法将数据放入缓存，比如redis，redis连接数在5万左右且查询速度远比数据库要快，会节省大量查询时间。还有些数据是基础数据一直都不会变的，可以加载到本地缓存比如谷歌的guava\n2）读多写少 尽量使用乐观锁\n悲观锁是互斥的，遇到加锁状态必须等待，这会增加阻塞的概率。而且高并发的情况下，有些线程可能永远都拿不到锁，对用户体验是非常差的。\n乐观锁可以解决这个问题，不过乐观锁有时候会更新不到数据，此时就会去重试，增加系统开销，但相对悲观锁来说会好很多。可以使用redis的watch功能(redis乐观锁)\n3）拆分功能-微服务概念\n比如一个系统里有会员信息、商品信息、交易信息，那么我们可以拆成3个微服务，图片、文件用专门的服务器存储，每个应用单独部署，且每个微服务部署集群，每个微服务有自己的数据库。这样就会用户请求跟数据交互都分流了。\n这个会增加成本，不过既要性能高又要成本低不太现实，看实际情况了。如果高并发长时间才会搞一次，可以采取灵活部署的办法，平常可以一台服务器部署多个微服务，等活动开始的时候增加机器分开部署\n4）削峰\n削峰主要通过消息来处理，比如rabbitMq、kafka,这种行为属于偷懒行为，对于实时性要求不高的场景是可以的。一般QPS为1万以上的推荐kafka，实时性要求非常高的用rabbitmq。\n5）重启与过载保护\n假如服务进程挂了，这样的高并发情况下，重启几乎是无效的，重启就会立马崩溃掉，此时需要加一个服务网关做限流控制，还有用到redis的需要预热操作\n3、从用户行为上做控制\n1）页面上不让用户重复点击，点击后置灰，别小看这个功能，可以减少50%的无效请求。\n2）防作弊，有些用户可以通过技术手段绕过上面的限制，这样就需要在服务端做限制了。比如限流，配置一个用户一定时间内最多访问多少请求，可以用redis存储用户的请求次数，最好用watch功能，避免用户同时操作。有些黄牛可能有多个用户，那么就需要针对IP做限制了，比如增加验证码，当然也可以直接对IP做限流，不过可能会误伤部分用户。\n3）有些黄牛会模拟真实场景，比如12306抢票，这样的话只能通过大数据分析了，根据用户行为筛选出哪些账户是僵尸账户，平常不动，高峰期就活跃了，这样可以有针对性的对这些用户做限制操作\n4、考虑事务问题\n高并发的情况下如果不做控制很容易出现数据错乱的问题，解决这个问题需要数据同步。\n大致思路有：\n1、悲观锁\n比如synchronized,lock,redis分布式锁，悲观锁容易造成阻塞\n2、同步队列\n请求全部进入一个同步队列里，然后一个个操作，跟悲观锁类似，也容易造成阻塞\n3、乐观锁\n数据库加版本号，每次更新操作需要带版本号进行幂等操作。也可以用redis的watch功能，乐观锁操作会有很多无效操作，增加重试开销，但相对引起阻塞的问题这点性能开销还是可以接受的，所以高并发情况下最好还是用乐观锁\nupdate goods set status=2,version=version+1where id=#{id} and version=#{version};\n互联网正在高速发展，使用互联网服务的用户越多，高并发的场景也变得越来越多。电商秒杀和抢购，是两个比较典型的互联网高并发场景。虽然我们解决问题的具体技术方案可能千差万别，但是遇到的挑战却是相似的，因此解决问题的思路也异曲同工。\n个人整理并发解决方案。\na.应用层面：读写分离、缓存、队列、集群、令牌、系统拆分、隔离、系统升级（可水平扩容方向）。\nb.时间换空间：降低单次请求时间，这样在单位时间内系统并发就会提升。\nc.空间换时间：拉长整体处理业务时间，换取后台系统容量空间。\nSystem Design\nhttps://zhuanlan.zhihu.com/p/191057045\n需要设计哪些功能（也可以自己想），需要承受多大的访问量？\n首先可以把Twitter的功能一个个罗列出来，很显然你无法在45分钟的面试中完成所有功能的设计，所以需要筛选出核心功能（Post a Tweet，Timeline，News Feed，Follow/Unfollow a user，Register/Login）。\n然后有的面试官可能会问你系统承受的QPS大概是多少？需要考虑并发用户，读频率（Read QPS）以及写频率（Write QPS）。记住重要的是你的思考和计算过程而不是计算结果。\n分析QPS有什么用？\n\n如果QPS = 100，那么用你的笔记本作Web服务器就好了；\nQPS = 1K，一台好点的Web 服务器也能应付，需要考虑Single Point Failure；\nQPS = 1m，则需要建设一个1000台Web服务器的集群，并且要考虑如何Maintainance（某一台挂了怎么办）。\n\nQPS 和 服务器/数据库之间的关系\n\n一台Web Server承受量约为 1K的QPS（考虑到逻辑处理时间以及数据库查询的瓶颈）；\n一台SQL Database承受量约为 1K的QPS（如果JOIN和INDEX query比较多的话，这个值会更小）；\n一台 NoSQL Database (Cassandra) 约承受量是 10k 的 QPS；\n一台 NoSQL Database (Memcached) 约承受量是 1M 的 QPS。\n\n第二步，Service服务\n所谓服务可以认为是逻辑处理的整合，对于同一类问题的逻辑处理可以归并到一个服务中。这一步实际上就是将整个系统细分为若干个小的服务。\n根据第一步选出的核心功能，我们可以将推特拆分成如下的几个服务：\n!https://pic3.zhimg.com/80/v2-6e94502ebcdfa365216faa12aed65b0a_1440w.webp\n第三步，Storage 存储\n接下来就是4S分析法中最重要的一部分，存储。根据每个服务的数据特性选择合适的存储结构，然后细化数据表结构。\n\n操作系统OS\n!https://pic.leetcode-cn.com/1642125846-FXGHTw-20210216234120.png\n地址空间（address space）表示任何一个计算机实体所占用的内存大小\n程序局部性原理：是指程序在执行时呈现出局部性规律，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域，具体来说，局部性通常有两种形式：时间局部性和空间局部性。\n**时间局部性：**被引用过一次的存储器位置在未来会被多次引用（通常在循环中）。\n**空间局部性：**如果一个存储器的位置被引用，那么将来他附近的位置也会被引用。\n存储器抽象\n在计算机中，每个设备以及进程都被分配了一个地址空间。处理器的地址空间由其地址总线以及寄存器决定。地址空间可以分为Flat——表示起始空间位置为0；或者Segmented——表示空间位置由偏移量决定。在一些系统中，可以进行地址空间的类型转换。至于IP地址空间，IPV4协议并没有预见到IP地址的需求量如此之大，32位的地址空间已经无法满足需求了。因此，开发了IPV6协议，支持128位的地址空间 [1] 。\n\n暴露问题\n把物理地址暴露给进程会带来下面几个严重问题。第一，如果用户程序可以寻址内存的每个字节，它们就可以很容易地（故意地或偶然地）破坏操作系统，从而使系统慢慢地停止运行。即使在只有一个用户进程运行的情况下，这个问题也是存在的。第二，使用这种模型，想要同时（如果只有一个CPU就轮流执行）运行多个程序是很困难的。在个人计算机上，同时打开几个程序是很常见的（一个文字处理器，一个邮件程序，一个网络浏览器，其中一个当前正在工作，其余的在按下鼠标的时候才会被激活）。在系统中没有对物理内存的抽象的情况下，很难做到上述情景，因此，我们需要其他办法。\n为了解决这些问题，现代操作系统使用虚拟内存技术，将物理地址空间抽象成虚拟地址空间，并通过内存管理单元（MMU）来实现地址转换，使得每个进程只能访问自己的虚拟地址空间，从而提高系统的安全性、隔离性和稳定性。\nMemory\n传统存储管理，进程必须把作业一次性装入内存中才能开始运行，且驻留在内存中。那其他app要运行怎么办或暂时用不到的数据占用了很多内存\n虚拟内存为每个进程提供了一个私有的地址空间 每个进程拥有一片连续完整的内存空间 数据不断换入换出外存实现宏观上的大于实际内存的虚拟内存。\n\n虚拟地址空间和物理地址空间： 每个应用程序看到的是一组虚拟地址，而不是真正的物理内存地址。操作系统负责将这些虚拟地址映射到物理内存中的实际位置。\n分页技术： 操作系统将虚拟内存划分成固定大小的页面（通常是4KB），同时也将物理内存划分成相同大小的页框。虚拟页被映射到物理页框，但不一定要将所有虚拟页都加载到物理内存中。\n页面置换： 当应用程序需要访问一个虚拟页，但该页不在物理内存中时，会触发页面置换。操作系统会根据一定的算法，将一个当前不太可能访问的物理页替换出去，然后将需要的虚拟页加载到这个物理页中。\n页表： 操作系统维护一个页表，记录虚拟页与物理页的映射关系。当应用程序访问虚拟地址时，操作系统会查询页表，找到对应的物理页。\n页面调度算法： 操作系统使用不同的算法来决定哪些页应该被替换出物理内存，以便为新的虚拟页腾出空间。一些常见的页面调度算法包括最近最少使用（LRU）、先进先出（FIFO）、最不常用（LFU）等。\n\n虚拟内存的主要优点包括：\n\n允许运行比物理内存更大的应用程序。\n提供了更好的内存管理和分配灵活性。\n能够使多个应用程序同时运行，而不会发生内存冲突。\n提供了更好的内存保护，防止一个应用程序影响到其他应用程序的内存空间。\n\n然而，虚拟内存也有一些缺点，比如访问虚拟内存可能会引入一定的性能开销，因为涉及到物理内存和硬盘之间的数据交换。如果系统中同时运行的应用程序过多，可能会导致频繁的页面置换，从而影响性能。\n总之，虚拟内存是现代操作系统中重要的内存管理技术，它通过将物理内存和硬盘空间结合起来，使得计算机系统能够更有效地管理内存资源，并支持运行多个应用程序。\n例如有一台计算机可以产生 16 位地址，那么一个程序的地址空间范围是 0~64K。该计算机只有 32KB 的物理内存，虚拟内存技术允许该计算机运行一个 64K 大小的程序。\n32位系统使用32位地址线的最大寻址空间为2的32次方bytes，计算后即4294967296 Bytes，也就是我们常说的4096MB，32位地址线的寻址空间封顶即为4GB\n分页系统映射\n内存管理单元（MMU）管理着地址空间和物理内存的转换，其中的页表（Page table）存储着页（程序地址空间）和页框（物理内存空间）的映射表。一个虚拟地址分成两个部分，一部分存储页面号，一部分存储偏移量。CPU 上的内存管理单元（Memory Management Unit，MMU）就是专门用来进行虚拟地址到物理地址的转换的，不过 MMU 需要借助存放在内存中的页表，而这张表的内容正是由操作系统进行管理的。\n页表是一个十分重要的数据结构！\n操作系统为每个进程建立了一张页表。一个进程对应一张页表，进程的每个页面对应一个页表项，每个页表项由页号和块号（页框号）组成，记录着进程页面和实际存放的内存块之间的映射关系。\n寄存器是CPU内部高速存取数据的地方，比缓存更接近CPU的运算器。 8086是16位cpu（字长16位）：\n其CPU一次最多可处理16位数据 寄存器最大宽度位16位 寄存器与PU之间的通路位16位。 地址20位，最大可寻址220 = 1MB地址空间\n同时函数运行时需要额外的寄存器来保存一些信息，像部分局部变量之类，这些寄存器也是线程私有的，一个线程不可能访问到另一个线程的这类寄存器信息。\n所属线程的栈区、程序计数器、栈指针以及函数运行使用的寄存器是线程私有的。\n指针本质上可以在整个OS允许的内存块上任意移动，有时候还会跨界到其他内存块上去。本质上它离机器语言太近，能够造成非常巨大的外延性破坏。一个最经典的例子就是内存践踏造成的缓冲区溢出。\nJava语言中没有明确的指针概念，它使用引用（reference）来操作对象。这是为了提高安全性和简化开发，因为指针容易导致内存泄漏、越界访问和悬空指针等问题。Java的对象引用是由Java虚拟机（JVM）管理的，这种方式有助于减少对内存的直接操作，从而提高了安全性和可靠性。\nC++ 利用 智能指针达成的效果是：一旦某对象不再被引用，系统刻不容缓，立刻回收内存\n一个解决空悬指针的办法是，引入一层间接性，让 p1 和 p2 所指的对象永久有 效。\n在Java中，常见的内存泄漏情况包括：\n\n长期持有对象的引用： 如果一个对象被分配了内存，并被存储在某个全局变量、静态变量、集合或缓存中，但在后续的程序执行过程中未释放对该对象的引用，即使该对象不再需要，也无法被垃圾回收。\n监听器未及时移除： 当一个对象作为监听器注册到某个事件上，但在对象不再需要时，未取消注册或者移除对监听器的引用，这可能导致对象无法被垃圾回收。\n未关闭资源： 例如打开了文件、数据库连接、网络连接等资源，但在使用完后未显式地关闭或释放这些资源，会导致资源泄漏，进而可能导致内存泄漏。\n循环引用： 当两个或多个对象相互引用，并且这些对象之间形成了一个环形的引用结构，即使这些对象在外部不再被使用，但由于它们互相引用导致它们之间的引用计数不为零，无法被垃圾回收。\n\n为避免内存泄漏，需要进行良好的内存管理和编程实践：\n\n及时释放不再需要的对象引用，可以手动将对象引用置为null。\n在使用完资源后及时关闭文件、数据库连接、网络连接等。\n避免循环引用，使用弱引用或软引用来避免形成永久性的对象引用。\n对于监听器等注册的对象，及时取消注册或移除引用。\n\n进程间通信IPC\n可选用的6种方法\n\n管道 Linux管道是一种特殊的文件描述符的「|」竖线就是一个管道，它的功能是将前一个命令（ps auxf）的输出，作为后一个命令（grep mysql）的输入，从这功能描述，可以看出管道传输数据是单向的，如果想相互通信，我们需要创建两个管道才行。管道是通过调用 pipe 函数创建的，fd[0] 用于读，fd[1] 用于写。Channel是Go语言中的一种特殊类型，用于在Goroutine之间进行通信和同步。\n消息队列（Message Queue）：以上三种方式只适合传递传递少量信息，POSIX 标准中 定义了消息队列用于进程间数据量较多小数据的通信。进程可以向队列添加消息，被赋予读权 限的进程则可以从队列消费消息。消息队列克服了信号承载信息量少，管道只能用于无 格式字节流以及缓冲区大小受限等缺点，但实时性相对受限。\n信号（Signal）：信号用于通知目标进程有某种事件发生，除了用于进程间通信外，进 程还可以发送信号给进程自身。信号的典型应用是 kill 命令  kill -l运行在 shell 终端的进程，我们可以通过键盘输入某些组合键的时候，给进程发送信号。例如\n\n\nCtrl+C 产生 SIGINT 信号，表示终止该进程；\nCtrl+Z 产生 SIGTSTP 信号，表示停止该进程，但还未结束；\n\n如果进程在后台运行，可以通过 kill 命令的方式给进程发送信号，但前提需要知道运行中的进程 PID 号，例如：\n\nkill -9 1050 ，表示给 PID 为 1050 的进程发送 SIGKILL 信号，用来立即结束该进程；\n\n所以，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令）。\n信号是进程间通信机制中唯一的异步通信机制\n\n信号量（Semaphore）：为了防止多进程竞争共享资源，互斥和同步问题。信号量就实现了这一保护机制。PV操作，信号量用于两个进程之间同步协作手段，它相当于操作系统提 供的一个特殊变量，程序可以在上面进行 wait() 和 notify() 操作。进程 A 在访问共享内存前，先执行了 P 操作，由于信号量的初始值为 1，故在进程 A 执行 P 操作后信号量变为 0，表示共享资源可用，于是进程 A 就可以访问共享内存。 若此时，进程 B 也想访问共享内存，执行了 P 操作，结果信号量变为了 -1，这就意味着临界资源已被占用，因此进程 B 被阻塞。 直到进程 A 访问完共享内存，才会执行 V 操作，使得信号量恢复为 0，接着就会唤醒阻塞中的线程 B，使得进程 B 可以访问共享内存，最后完成共享内存的访问后，执行 V 操作，使信号量恢复到初始值 1。\n共享内存（Shared Memory）：允许多个进程访问同一块公共的内存空间，这是效率最高的进程间通信形式。原本每个进程的内存地址空间都是相互隔离的，但操作系统提供了让进程主动创建、映射、分离、控制某一块内存的程序接口。当一块内存被多进程共享时，各个进程往往会与其它通信机制，譬如信号量结合使用，来达到进程间同步及互斥的协调操作。\n套接字接口（Socket）RPc：消息队列和共享内存只适合单机多进程间的通信，套接字接口 是更为普适的进程间通信机制，可用于不同机器之间的进程通信。套接字（Socket）起 初是由 UNIX 系统的 BSD 分支开发出来的，现在已经移植到所有主流的操作系统上。\n\n当仅限于本机进程间通信时，套接字接口是被优化过的，不会经过网络协 议栈，不需要打包拆包、计算校验和、维护序号和应答等操作，只是简单地将应用层数 据从一个进程拷贝到另一个进程，这种进程间通信方式有个专名的名称：UNIX Domain Socket，又叫做 IPC Socket。\n\n只支持半双工通信： 意味着数据传输是单向交替的，通信的一方既是发送者也是接收者，但同一时间只能进行一种操作。\n仅限于父子进程或兄弟进程之间使用： 由于UNIX Domain Socket是本地通信的一种方式，因此通常用于同一台机器上的进程间通信，比如父子进程或兄弟进程之间。\n\n\n空分复用技术（Swapping）： 空分复用是操作系统中一种管理内存的机制。它将当前不被使用或暂时不需要的内存页面从物理内存移到磁盘上（称为交换区），以便释放物理内存以供其他程序或进程使用。当需要时，这些页面可以再次被换回物理内存中。这种技术能够有效地扩展可用内存的大小。空分复用需要快速的内存地址映射，使得操作系统能够迅速将虚拟内存地址转换为实际物理内存地址。这种映射是由CPU中的存储器管理单元（Memory Management Unit，MMU）负责完成的。\n时分复用技术（Time Division Multiplexing，TDM）： 时分复用是一种通信技术，用于多路复用，让多个用户或信号共享同一个通信信道或资源。在操作系统中，时分复用也可以被理解为在单个CPU上执行多个进程的技术。这种技术会分割时间成为多个时间片（Time Slice），每个时间片分配给不同的进程，使得它们轮流占用CPU并执行。这种技术可以使系统中的多个进程表现出并发性，尽管实际上在同一时间点只有一个进程在执行。这种并发性是通过快速的进程切换和调度实现的。\n\n多线程/多进程解决了阻塞问题\n线程是抢占式，而协程是非抢占式的，所以需要用户自己释放使用权来切换到其他协程，因此同一时间其实只有一个协程拥有运行权，相当于单线程的能力。\n协程并不是取代线程, 而且抽象于线程之上, 线程是被分割的CPU资源, 协程是组织好的代码流程, 协程需要线程来承载运行, 线程是协程的资源, 但协程不会直接使用线程, 协程直接利用的是执行器(Interceptor), 执行器可以关联任意线程或线程池, 可以使当前线程, UI线程, 或新建新程.。\n线程是协程的资源。协程通过Interceptor来间接使用线程这个资源。\n内核态和用户态\n内核kernel是程序，它需要运行，就必须被分配 CPU。因此，CPU 上会运行两种程序，一种是操作系统的内核程序（也称为系统程序），一种是应用程序。前者完成系统任务，后者实现应用任务。两者之间有控制和被控制的关系，前者有权管理和分配资源，而后者只能向系统申请使用资源。\n文件传输零拷贝\n什么是 DMA 技术？简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。\n\n上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。 其次，还发生了 4 次数据拷贝，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程：\n第一次拷贝，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。 第二次拷贝，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。 第三次拷贝，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。 第四次拷贝，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。\n我们回过头看这个文件传输的过程，我们只是搬运一份数据，结果却搬运了 4 次，过多的数据拷贝无疑会消耗 CPU 资源，大大降低了系统性能。 这种简单又传统的文件传输方式，存在冗余的上文切换和数据拷贝，在高并发系统里是非常糟糕的，多了很多不必要的开销，会严重影响系统性能。\n所以，要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数。\n零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。。\n零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。\n所以，总体来看，零拷贝技术可以把文件传输的性能提高至少一倍以上。\n事实上，Kafka 这个开源项目，就利用了「零拷贝」技术，从而大幅提升了 I/O 的吞吐率，这也是 Kafka 在处理海量数据为什么这么快的原因之一。\n如果你追溯 Kafka 文件传输的代码，你会发现，最终它调用了 Java NIO 库里的 transferTo 方法\n并发\n并发Concurrent：无论上一个开始执行的任务是否完成，当前任务都可以开始执行 （也就是说，A B 顺序执行的话，A 一定会比 B 先完成，而并发执行则不一定。） 与可以一起执行的并行（parallel）相对的是不可以一起执行的串行（serial）\n从宏观方面来说，并发就是同时进行多种时间，实际上，这几种时间，并不是同时进行的，而是交替进行的，而由于CPU的运算速度非常的快，同一时间只有一个线程运行\n并行：则是真正意义上的同时进行多种事情。这种只可以在多核CPU的基础上完成。\n综上，并发与并行并不是互斥的概念，只是前者关注的是任务的抽象调度、后者关注的是任务的实际执行。而它们又是相关的，比如并行一定会允许并发。 所以题目中的例子：\nCpu\nCPU 里有两部分： 一部分叫做 Control Unit，负责控制。比如计数器，指令寄存，等等。\n一部分叫做 Logic Unit，负责运算。比如加法器，累加器，等等。有些也会把 ‘ 数据总线’ 归于逻辑运算单元里\n单核 CPU 多任务：并发（不必等上一个任务完成才开始下一个任务）、串行（只有一个实际执行任务的 CPU 核）\n!https://img-blog.csdnimg.cn/20200706205605930.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW95YW5namlhbjcyNA==,size_16,color_FFFFFF,t_70\n单核cpu同一时间只能运行一个线程\n区别：进程是操作系统分配资源的基本单位，线程是任务调度和执行的\n1.CPU角度来看:\n我们以Intel的Core i5-8250U为例来举例,它是四核八线程的CPU ,\n我认为是一个CPU集成了4个核心,一般来说一个核心对应一个线程,Intel通过超线程技术来实现一个核心对应2个线程,所以它是四核8线程.\n线程数：是同一时刻设备能并行执行的程序个数,这里说的线程是CPU级别的,不是java里的线程.\n2.操作系统角度:\n进程是正在运行的程序的实例。它包含了程序执行所需的代码、数据和系统资源，例如内存空间、文件和设备等。每个进程都是独立的，有自己独立的内存空间，它们之间不能直接访问对方的内存。所以说进程是最小的资源分配单位,\n我们可以看到这个任务管理器的图,我电脑是4核8线程的CPU,我的电脑四核八线程，是采用超线程技术将一个物理处理核心模拟成两个逻辑处理核心.\n我认为,一个逻辑处理器核心同一时间点只能执行一个进程,那理论上最多应该同时执行8个进程啊.\n我的电脑同时开启了213个进程,2900个线程,那它是怎么处理的呢?我找了下资料\n原来操作系统是采用的是时间片轮转的抢占式调度方式，每个进程有各自独立的一块内存，使得各个进程之间内存地址相互隔离,\n由于CPU的执行效率非常高，时间片非常短，在各个任务之间快速地切换，给人的感觉就是多个任务在“同时进行”.\nREAL TIME System\n实时操作系统与一般的操作系统相比，最大的特色就是“实时性”[1]，如果有一个任务需要执行，实时操作系统会马上（在较短时间内）执行该任务，不会有较长的延时。这种特性保证了各个任务的及时执行。实时操作系统分为两大类:\n\n事件驱动型。当一个高优先级的任务需要执行时，系统会自动切换到这个任务。这种根据优先级调度任务的方式称为抢占式任务处理。\n时间触发型。每个任务在各自设定好的的时间间隔内重复、轮流调度。\n\n关于实时系统，有很多人有错误的认知：\n\n\n实时系统一定很快，错误。实时系统不一定快，相反的，实时系统的效率往往不高，如果系统中存在着周期性的高优先级任务，往往会导致实时系统的低优先级任务被周期性的打断，造成整个系统吞吐量下降（比如机械硬盘访问，高低任务交叉执行会导致磁头反复移动，效率降低）。\n\n\n实时系统一定很小，代码精简，错误。现在大型的实时系统已经很大了，一个操作系统的代码中，规模最大的往往是各种驱动，现代实时操作系统因为要支持多种硬件平台，驱动规模不会太小。\n\n\n实时系统里不能有delay/sleep操作，错误。实时系统里可以有delay的，因为delay可能是用户行为，delay本身也是一个确定性的操作，实时系统里应该避免不确定的delay动作。\n\n\n\n实时系统的设计原则是要避免不确定性的设计，实时系统可以很慢，效率可以低，确定性才是必须的\n进程与线程\n线程最大的区别在于上下文切换过程中，线程不用切换虚拟内存，因为同一个进程内的线程都是共享虚拟内存空间的，线程就单这一点不用切换，就相比进程上下文切换的性能开销减少了很多。\n进程（Process）：\n\n定义： 进程是程序在执行时的一个实例。它包含了程序代码、数据以及执行时的环境。每个进程都有独立的内存空间，使得多个进程能够同时运行，互不干扰。\n资源隔离： 不同的进程之间具有独立的内存空间和资源，一个进程的崩溃不会直接影响其他进程。\n通信开销： 进程之间的通信开销较大，需要使用操作系统提供的IPC（进程间通信）机制，如管道、消息队列、共享内存等。\n创建开销： 创建新进程的开销相对较大，涉及复制父进程的内存空间等操作。\n并发性： 进程之间可以实现真正的并发，因为它们在不同的内存空间中执行，不会互相干扰。\n\n线程（Thread）：\n\n定义： 线程是进程中的一个执行单元，一个进程可以包含多个线程。所有线程共享进程的内存空间和资源，可以更高效地进行并发执行。\n资源共享： 线程之间共享同一进程的内存空间，可以直接访问相同的数据和资源。\n通信开销： 由于线程共享内存，上下文切换快，线程之间的通信开销较小，但也需要进行同步和互斥操作以避免竞争条件。\n创建开销： 创建新线程的开销相对较小，因为新线程可以共享父进程的内存空间。\n并发性： 线程之间的并发性比进程更高，因为它们共享相同的内存空间，但同时也会增加竞争条件和同步的复杂性。\n\n协程\n\n协程的调度完全由用户控制。\n一个线程可以拥有多个协程，协程不是被操作系统内核所管理，而完全是由程序所控制。\n与其让操作系统调度，不如我自己来，这就是协程\n\n进程就是包换上下文切换的程序执行时间总和 = CPU加载上下文+CPU执行+CPU保存上下文 由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，涉及当前执行进程 CPU 环境的保存及新调度进程 CPU 环境的设置，而线程切换时只需保存和设置少量寄存器内容，开销很小。\n同步与异步\n**线程同步（Thread Synchronization）**是指在多线程编程中，为了避免多个线程之间对共享资源的并发访问引发的问题（如竞争条件、死锁等），需要采取措施来协调线程的执行，以确保线程之间的操作按照期望的顺序进行。线程同步的目的是保证多个线程能够有序地访问共享资源，避免数据的不一致和错误。\n常见的线程同步机制包括：\n\n\n互斥锁（Mutex）： 互斥锁是一种最基本的线程同步机制，用于保护共享资源免受并发访问。一次只有一个线程能够持有互斥锁，其他线程必须等待直到锁被释放。这样可以防止多个线程同时修改同一资源。\n\n\n信号量（Semaphore）： 信号量是一种计数器，用于控制多个线程对共享资源的访问。它可以用来限制同时访问资源的线程数量，也可以用于线程间的通信。\n\n\n条件变量（Condition Variable）： 条件变量用于在线程之间传递信息，以及在某些条件满足时唤醒等待的线程。它通常和互斥锁一起使用，以实现更复杂的同步需求。\n\n\n读写锁（Read-Write Lock）： 读写锁允许多个线程同时读取共享资源，但只允许一个线程进行写操作。这可以提高读操作的并发性能。多个读线程可以同时获得锁，写线程会阻塞其他写线程和读线程。乐观锁\n\n\n原子操作（Atomic Operations）： 原子操作是不可中断的操作，可以在单个指令中完成。它们可以用于保护简单的共享数据结构，避免竞争条件。\n\n\n自旋锁： 自旋锁与互斥量类似，但它不使线程进入阻塞态；而是在获取锁之前一直占用CPU，处于忙等（自旋）状态。\n\n\n屏障（Barrier）： 屏障用于等待多个线程都达到某个点，然后再继续执行后续操作，常用于需要多个线程协同完成的任务。\n\n\n乐观锁：乐观锁是一种假设在大多数情况下不会发生冲突的锁。它不会在访问共享资源之前获取锁，而是在更新共享资源时进行检查。如果检测到其他线程已经更新了共享资源，那么当前线程可能需要重试或者采取其他措施来处理冲突。乐观锁通常用于多读的场景，可以提高吞吐量。\n\n\n悲观锁：悲观锁是一种假设在大多数情况下会发生冲突的锁。它在访问共享资源之前获取锁，并假定其他线程会干扰。悲观锁通常用于写入操作，以确保在写入共享资源时不会发生冲突。\n\n\n乐观锁的实现方式主要有两种：CAS机制和版本号机制。\n对于ABA问题，比较有效的方案是引入版本号，内存中的值每发生一次变化，版本号都+1；在进行CAS操作时，不仅比较内存中的值，也会比较版本号，只有当二者都没有变化时，CAS才能执行成功。Java中的AtomicStampedReference类便是使用版本号来解决ABA问题的。\n除了CAS，版本号机制也可以用来实现乐观锁。版本号机制的基本思路是在数据中增加一个字段version，表示该数据的版本号，每当数据被修改，版本号加1。当某个线程查询数据时，将该数据的版本号一起查出来；当该线程更新数据时，判断当前版本号与之前读取的版本号是否一致，如果一致才进行操作。\n临界区 对临界资源进行访问的那段代码称为临界区。\n为了互斥访问临界资源，每个进程在进入临界区之前，需要先进行检查。\n// entry section // critical section; // exit section\n\n同步与互斥 同步：多个进程因为合作产生的直接制约关系，使得进程有一定的先后执行关系。 互斥：多个进程在同一时刻只有一个进程能进入临界区。\n信号量 信号量（Semaphore）是一个整型变量，可以对其执行 down 和 up 操作，也就是常见的 P 和 V 操作。\n\ndown : 如果信号量大于 0 ，执行 -1 操作；如果信号量等于 0，进程睡眠，等待信号量大于 0； up ：对信号量执行 +1 操作，唤醒睡眠的进程让其完成 down 操作。 down 和 up 操作需要被设计成原语，不可分割，通常的做法是在执行这些操作的时候屏蔽中断。\n如果信号量的取值只能为 0 或者 1，那么就成为了 互斥量（Mutex） ，0 表示临界区已经加锁，1 表示临界区解锁。\n互斥锁\n1严格轮换法 加入锁变量 2 Peterson算法 进程0出临界区后进程1才会离开忙等\ntypedef int semaphore; semaphore mutex = 1; void P1() { down(&amp;mutex); // 临界区 up(&amp;mutex); }void P2() { down(&amp;mutex); // 临界区 up(&amp;mutex); } 使用信号量实现生产者-消费者\n问题描述：使用一个缓冲区来保存物品，只有缓冲区没有满，生产者才可以放入物品；只有缓冲区不为空，消费者才可以拿走物品。\n因为缓冲区属于临界资源，因此需要使用一个互斥量 mutex 来控制对缓冲区的互斥访问。\n为了同步生产者和消费者的行为，需要记录缓冲区中物品的数量。数量可以使用信号量来进行统计，这里需要使用两个信号量：empty 记录空缓冲区的数量，full 记录满缓冲区的数量。其中，empty 信号量是在生产者进程中使用，当 empty 不为 0 时，生产者才可以放入物品；full 信号量是在消费者进程中使用，当 full 信号量不为 0 时，消费者才可以取走物品。\n注意，不能先对缓冲区进行加锁，再测试信号量。也就是说，不能先执行 down(mutex) 再执行 down(empty)。如果这么做了，那么可能会出现这种情况：生产者对缓冲区加锁后，执行 down(empty) 操作，发现 empty = 0，此时生产者睡眠。消费者不能进入临界区，因为生产者对缓冲区加锁了，消费者就无法执行 up(empty) 操作，empty 永远都为 0，导致生产者永远等待下，不会释放锁，消费者因此也会永远等待下去。\n异步编程\n异步（Asynchronous）：异步编程是一种处理并发操作的方式，其中不需要等待一个操作完成才能执行下一个操作。在异步编程中，一个操作的启动不会阻塞程序的其他部分，而是允许程序继续执行其他操作，之后当异步操作完成时，程序会得到通知或回调。这种方式可以提高程序的响应性和并发性。但由于资源有限，进程的执行不是一贯到底的，这就是进程的异步性。 异步和同步是相对的，同步就是顺序执行，执行完一个再执行下一个，需要等待、协调运行\n“异步阻塞” 意味着可能存在某些情况下，虽然使用了异步编程的方式，但在某些时刻程序仍然被阻塞，等待某些操作的完成。这种情况可能发生在以下情形下：\n\n某些异步操作内部使用了同步操作，导致当前线程在等待这些同步操作完成时被阻塞。\n异步操作之间存在依赖关系，某个异步操作必须等待另一个异步操作完成后才能执行。\n程序中的某些部分仍然采用同步编程方式，而不是全面采用异步编程。\n\n要避免异步操作中的阻塞，通常需要使用异步编程框架和模式，以确保在异步操作中也不会阻塞程序的其他部分。这包括使用回调、Promise、异步/await等机制来管理异步操作的执行顺序和依赖关系，从而实现真正的非阻塞异步编程。\neg.       我们以经典的读取文件的模型举例。（对操作系统而言，所有的输入输出设备都被抽象成文件。）\n在发起读取文件的请求时，应用层会调用系统内核的 I/O 接口。\n如果应用层调用的是阻塞型 I/O，那么在调用之后，应用层即刻被挂起，一直出于等待数据返回的状态，直到系统内核从磁盘读取完数据并返回给应用层，应用层才用获得的数据进行接下来的其他操作。\n如果应用层调用的是非阻塞 I/O，那么调用后，系统内核会立即返回（虽然还没有文件内容的数据），应用层并不会被挂起，它可以做其他任意它想做的操作。（至于文件内容数据如何返回给应用层，这已经超出了阻塞和非阻塞的辨别范畴。）\n阻塞和非阻塞解决了应用层等待数据返回时的状态问题，那系统内核获取到的数据到底如何返回给应用层呢？是否是阻塞还是非阻塞，关注的是接口调用（发出请求）后等待数据返回时的状态。\n同步还是异步，关注的是任务完成时消息通知的方式。由调用方盲目主动问询的方式是同步调用，由被调用方主动通知调用方任务已完成的方式是异步调用。\n同步的优点是：同步是按照顺序一个一个来，不会乱掉，更不会出现上面代码没有执行完就执行下面的代码， 缺点：是解析的速度没有异步的快；\n异步的优点是：异步是接取一个任务，直接给后台，在接下一个任务，一直一直这样，谁的先读取完先执行谁的， 缺点：没有顺序 ，谁先读取完先执行谁的 ，会出现上面的代码还没出来下面的就已经出来了，会报错；\n异步是当一个调用请求发送给被调用者,而调用者不用等待其结果的返回而可以做其它的事情。实现异步可以采用多线程技术或则交给另外的进程来处理。\n软工\n\n需求在初始阶段就能够被精心设计。\n具有容易理解的线性结构。\n易于管理。\n\n瀑布的缺点\n\n既不灵活，又不支持变更。\n任何阶段一旦出现延迟，都会导致项目无法推进。\n由于较为死板，因此项目总体时间较长。\n并不鼓励在初始阶段之后，利益相关者进行积极地沟通。\n\n\nArchitecture comes in Designing phase and Design Patterns comes in Building phase.\nArchitectural pattern is like a blue print and design pattern is actual implementation.\nArchitecture is base which everything else adhere to and design pattern is a way to structure classes to solve common problems.\nAll Architecture is design pattern but all design pattern can not be architecture. Like MVC can come under both. But singleton design pattern can not be an architecture pattern. MVC, MVVM all come under both.\n\n敏捷软件开发\nA build is an executable version of the system.\nIntegration build plan describes the sequences of build in an iteration\nTesting\n白盒测试也称为结构测试，主要用于检测软件编码过程中的错误。程序员的编程经验、对编程软件的掌握程度、工作状态等因素都会影响到编程质量，导致代码错误。\n黑盒测试又称为功能测试，主要检测软件的每一个功能是否能够正常使用。在测试过程中，将程序看成不能打开的黑盒子，不考虑程序内部结构和特性的基础上通过程序接口进行测试，检查程序功能是否按照设计需求以及说明书的规定能够正常打开使用。\n单元测试，需要遵循一下规则：\n1、每一个测试方法上使用@Test进行修饰\n2、每一个测试方法必须使用public void 进行修饰\n3、每一个测试方法不能携带参数\n4、测试代码和源代码在两个不同的项目路径下\n5、测试类的包应该和被测试类保持一致\n6、测试单元中的每个方法必须可以独立测试\njunit如何解决这个问题的呢？答案在于内部提供了一个断言机制，他能够将我们预期的结果和实际的结果进行比对，判断出是否满足我们的期望。相信到这，你已经迫不及待的想认识一下junit，下面我们直接通过案例，来分析一下这个机制。\n灰度发布，又名金丝雀发布，或者灰度测试，是指在黑与白之间能够平滑过渡的一种发布方式。在其上可以进行A/B testing，即让一部分用户继续用产品特性A，一部分用户开始用产品特性B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来。\n灰度发布是对某一产品的发布逐步扩大使用群体范围，也叫灰度放量。灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。\n灰度期：灰度发布开始到结束期间的这一段时间，称为灰度期。\n灰度发布的意义\n灰度发布能及早获得用户的意见反馈，完善产品功能，提升产品质量，让用户参与产品测试，加强与用户互动，降低产品升级所影响的用户范围。\n灰度发布步骤\n定义目标\n选定策略：包括用户规模、发布频率、功能覆盖度、回滚策略、运营策略、新旧系统部署策略等\n筛选用户：包括用户特征、用户数量、用户常用功能、用户范围等\n部署系统：部署新系统、部署用户行为分析系统（web analytics）、设定分流规则、运营数据分析、分流规则微调\n发布总结：用户行为分析报告、用户问卷调查、社会化媒体意见收集、形成产品功能改进列表\n产品完善\n新一轮灰度发布或完整发布\nUML统一建模语言\n是一种为面向对象系统的产品进行说明、可视化和编制文档的一种标准语言，是非专利的第三代建模和规约语言。UML是面向对象设计的建模工具，独立于任何具体程序设计语言。\nwindows中只有“/r/n”才能正确触发“我们理解的换行”操作\nDesign pattern\n生产者消费者模式并不是GOF提出的23种设计模式之一，23种设计模式都是建立在面向对象的基础之上的，但其实面向过程的编程中也有很多高效的编程模式\n创建型模式（Creational Patterns）\n工厂模式（Factory Pattern）：通过定义一个创建对象的接口，但是让子类决定具体实例化哪个类。包括简单工厂、工厂方法和抽象工厂模式。\n单例模式（Singleton Pattern）：确保一个类只有一个实例，并提供全局访问点。\n建造者模式（Builder Pattern）：将一个复杂对象的构建与其表示分离，使得同样的构建过程可以创建不同的表示。\n原型模式（Prototype Pattern）：用于创建对象的一种模式，通过复制现有对象来生成新对象。\n结构型模式（Structural Patterns）实现接口（implements）表示类遵循了接口定义，并实现了接口中声明的方法。\n适配器模式（Adapter Pattern）：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的类可以一起工作。\n装饰者模式（Decorator Pattern）：动态地给对象添加额外的职责，就增加功能而言，装饰者模式比生成子类更加灵活。\n代理模式（Proxy Pattern）：控制对对象的访问，可以用于增加额外的操作或控制对原始对象的访问。\n组合模式（Composite Pattern）：将对象组合成树形结构以表示“部分-整体”的层次结构。\n行为型模式（Behavioral Patterns）\n观察者模式（Observer Pattern）：定义了对象之间的一对多依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都会得到通知并自动更新。    示例：一个购物网站中，用户下单成功后，通知库存系统减少相应商品的库存；同时通知订单处理系统生成订单信息。\n策略模式（Strategy Pattern）：定义了一系列的算法，并将每一个算法封装起来，使它们可以相互替换。\n模板方法模式（Template Method Pattern）：定义一个操作中的算法骨架，而将一些步骤延迟到子类中。使得子类可以不改变一个算法的结构即可重新定义该算法的某些特定步骤。\n命令模式（Command Pattern）：将请求封装成对象，从而允许使用不同的请求、队列或日志请求来参数化其他对象。\n**简单工厂，工厂方法，[抽象工厂]**都属于设计模式中的创建型模式。其主要功能都是帮助我们把对象的实例化部分抽取了出来，优化了系统的架构，并且增强了系统的扩展性。\n★工厂模式中，重要的是工厂类，而不是产品类。产品类可以是多种形式，多层继承或者是单个类都是可以的。但要明确的，工厂模式的接口只会返回一种类型的实例，这是在设计产品类的时候需要注意的，最好是有父类或者共同实现的接口。\n★使用工厂模式，返回的实例一定是工厂创建的，而不是从其他对象中获取的。\n★工厂模式返回的实例可以不是新创建的，返回由工厂创建好的实例也是可以的。\n发现简单工厂模式存在一系列问题：\n\n工厂类集中了所有实例（产品）的创建逻辑，一旦这个工厂不能正常工作，整个系统都会受到影响；\n违背“开放 - 关闭原则”，一旦添加新产品就不得不修改工厂类的逻辑，这样就会造成工厂逻辑过于复杂。\n简单工厂模式由于使用了静态工厂方法，静态方法不能被继承和重写，会造成工厂角色无法形成基于继承的等级结构。\n\n为了解决上述的问题，我们又使用了一种新的设计模式：工厂方法模式。工厂方法模式可以说是简单工厂模式的进一步抽象和拓展，在保留了简单工厂的封装优点的同时，让扩展变得简单，让继承变得可行，增加了多态性的体现。\n区别\n简单工厂 ： 用来生产同一等级结构中的任意产品。一台咖啡机就可以理解为一个工厂模式，你只需要按下想喝的咖啡品类的按钮（摩卡或拿铁），它就会给你生产一杯相应的咖啡，你不需要管它内部的具体实现，只要告诉它你的需求即可。（对于增加新的产品，无能为力）\n工厂方法 ：用来生产同一等级结构中的固定产品。（支持增加任意产品）抽象工厂 ：用来生产不同产品族的全部产品。（对于增加新的产品，无能为力；支持增加产品族）\n不想喝咖啡了想喝啤酒，这个时候如果直接修改简单工厂里面的代码，这种做法不但不够优雅，也不符合软件设计的“开闭原则”,这时直接继承抽象class override方法\n单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一，提供了一种创建对象的最佳方式。这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。\n\n1、单例类只能有一个实例。\n2、单例类必须自己创建自己的唯一实例。\n3、单例类必须给所有其他对象提供这一实例。\n\n优点：\n\n1、在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例（比如管理学院首页页面缓存）。\n2、避免对资源的多重占用（比如写文件操作）。\n\n**缺点：**没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。\n单例模式在单线程环境下的两种经典实现：饿汉式 和懒汉式，但是饿汉式是线程安全的，而懒汉式是非线程安全的。在多线程环境下，我们特别介绍了五种方式来在多线程环境下创建线程安全的单例，即分别使用synchronized方法、synchronized块、静态内部类、双重检查模式 和ThreadLocal 来实现懒汉式单例，并总结出实现效率高且线程安全的懒汉式单例所需要注意的事项。\n\n\n立即加载 ： 在类加载初始化的时候就主动创建实例；\n\n\n延迟加载 ： 等到真正使用的时候才去创建实例，不用时不去主动创建。\n在单线程环境下，单例模式根据实例化对象时机的不同，有两种经典的实现：一种是 饿汉式单例(立即加载)，一种是 懒汉式单例(延迟加载)。**饿汉式单例在单例类被加载时候，就实例化一个对象并交给自己的引用；而懒汉式单例只有在真正使用的时候才会实例化一个对象并交给自己的引用。**代码示例分别如下：\n\n\n饿汉式单例天生就是线程安全. 在线程访问单例对象之前就已经创建好了。再加上，由于一个类在整个生命周期中只会被加载一次，因此该单例类只会创建一个实例，也就是说，线程每次都只能也必定只可以拿到这个唯一的对象.\n使用双重检测同步延迟加载去创建单例的做法是一个非常优秀的做法，**其不但保证了单例，而且切实提高了程序运行效率。**对应的代码清单如下：\n// 线程安全的懒汉式单例public class Singleton3 {//使用volatile关键字防止重排序，因为 new Instance()是一个非原子操作，可能创建一个不完整的实例    private static volatile Singleton3 singleton3;    private Singleton3() {    }    public static Singleton3 getSingleton3() {// Double-Check idiom        if (singleton3 == null) {            synchronized (Singleton3.class) {// 1// 只需在第一次创建实例时才同步                if (singleton3 == null) {// 2                    singleton3 = new Singleton3();// 3                }            }        }        return singleton3;    }}/* Output(完全一致):        1104499981        1104499981        1104499981        1104499981        1104499981        1104499981        1104499981        1104499981        1104499981        1104499981 *///:~\n如上述代码所示，为了在保证单例的前提下提高运行效率，我们需要对 singleton3 进行第二次检查，目的是避开过多的同步（因为这里的同步只需在第一次创建实例时才同步，一旦创建成功，以后获取实例时就不需要同步获取锁了）。这种做法无疑是优秀的，但是我们必须注意一点：　　　　必须使用volatile关键字修饰单例引用。\n使用场景：\n\n1、要求生产唯一序列号。\n2、WEB 中的计数器，不用每次刷新都在数据库里加一次，用单例先缓存起来。\n3、创建的一个对象需要消耗的资源过多，比如 I/O 与数据库的连接等。\n\n**注意事项：**getInstance() 方法中需要使用同步锁 synchronized (Singleton.class) 防止多线程同时进入造成 instance 被多次实例化。\n建议使用第 3 种饿汉方式。只有在要明确实现 lazy loading 效果时，才会使用第 5 种登记方式。如果涉及到反序列化创建对象时，可以尝试使用第 6 种枚举方式。\n创建者模式\nBuilder pattern 主要解决在软件系统中，有时候面临着“一个复杂对象”的创建工作，其通常由各个部分的子对象用一定的算法构成；由于需求的变化，这个复杂对象的各个部分经常面临着剧烈的变化，但是将它们组合在一起的算法却相对稳定。\n1、去肯德基，汉堡、可乐、薯条、炸鸡翅等是不变的，而其组合是经常变化的，生成出所谓的“套餐”。 2、JAVA 中的 StringBuilder。\n**原型模式（Prototype Pattern）**是用于创建重复的对象，同时又能保证性能。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。\n这种模式是实现了一个原型接口，该接口用于创建当前对象的克隆。当直接创建对象的代价比较大时，则采用这种模式。例如，一个对象需要在一个高代价的数据库操作之后被创建。我们可以缓存该对象，在下一个请求时返回它的克隆，在需要的时候更新数据库，以此来减少数据库调用。\njava中的深浅拷贝，需要拷贝的原型类 需要实现Cloneable接口，然后重写其中的clone方法，才可以实现类的拷贝。\n适配器模式（Adapter Pattern）是作为两个不兼容的接口之间的桥梁。这种类型的设计模式属于结构型模式，它结合了两个独立接口的功能。这种模式涉及到一个单一的类，该类负责加入独立的或不兼容的接口功能\n享元模式（Flyweight Pattern）主要用于减少创建对象的数量，以减少内存占用和提高性能。这种类型的设计模式属于结构型模式，它提供了减少对象数量从而改善应用所需的对象结构的方式。\n享元模式尝试重用现有的同类对象，如果未找到匹配的对象，则创建新对象。\nSOLID原则\nSRP 单一责任原则 OCP 开放封闭原则 LSP 里氏替换原则 ISP 接口隔离原则 DIP 依赖倒置原则\n单一责任原则 指的是一个类或者一个方法只做一件事。如果一个类承担的职责过多，就等于把这些职责耦合在一起，一个职责的变化就 可能抑制或者削弱这个类完成其他职责的能力。例如餐厅服务员负责把订单给厨师去做，而不是服务员又要订单又要炒菜。\n开放封闭原则\n对扩展开放，对修改关闭。意为一个类独立之后就不应该去修改它，而是以扩展的方式适应新需求。例如一开始做了普通 计算器程序，突然添加新需求，要再做一个程序员计算器，这时不应该修改普通计算器内部，应该使用面向接口编程， 组合实现扩展。\nhttps://juejin.cn/post/684490360656337306\n★函数模块遵循OCP，基本手段（或主要体现）为设计出通用性的函数。\n为什么要将行为作为参数Parameterization\n用户需求的频繁更改发生在方方面面，程序员常常面对“某种”这个字眼所标志的变化，即“某种”是对各种各样变化的抽象。实验4的需求变化，发生在函数级别，当要输出0-x之间符合“某种”条件的数时，程序员需要在不修改他的源代码的前提下，设计一个函数filter，能够一次性地应对各种可能的条件。换句话说，程序员需要设计一个在各种情况下都能够使用的、具有一般性/通用性的函数。\nLiskov Substitution Principle\n\n子类必须能够替换其父类而不改变程序的正确性。如果一个子类不能完全替换其父类，那么这个继承关系可能存在问题。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。\n\n不符合LSP的最常见的情况是\n1.父类定义的方法或属性有过于严格的约束，而子类重写该方法或属性时，放宽了这些约束。这可能导致父类方法的前置条件（Preconditions）无法满足；\n2.子类不能实现父类的抽象方法\n接口隔离原则 类不应该依赖不需要的接口，知道越少越好。例如电话接口只约束接电话和挂电话，不需要让依赖者知道还有通讯录。\n依赖倒置原则 指的是高级模块不应该依赖低级模块，而是依赖抽象。抽象不能依赖细节，细节要依赖抽象。比如类A内有类B对象，称为类A依赖类B，但是不应该这样做，而是选择类A去依赖抽象。例如垃圾收集器不管垃圾是什么类型，要是垃圾就行。\n迪米特法则（law of demeter,LoD）也成为最少知识原则（least knowledge principle），虽然名字不同，但描述的是同一个规则：一个对象应该对其他对象有最少的了解。通俗地讲，一个类应该对自己需要耦合或调用的类知道得最少 低耦合\nleetcode\n贪心选择的一般特征：贪心选择性质和最优子结构性质。\n贪心算法和动态规划算法都要求问题具有最优子结构性质，这是两类算法的一个共同点。大多数时候，能用贪心算法求解的问题，都可以用动态规划算法求解。但是能用动态规划求解的，不一定能用贪心算法进行求解。\n找到最优子结构 =&gt; 动态规划解最值问题，状态转移方程\n栈、队列\n栈是递归的底层实现， 我们用栈实现队列，用队列实现栈来掌握的栈与队列的基本操作。\n接着，通过括号匹配问题、字符串去重问题、删除字串问题，逆波兰表达式问题来系统讲解了栈在系统中的应用，以及使用技巧。\n通过求滑动窗口最大值，以及前K个高频元素介绍了两种队列：单调队列和优先级队列，这是特殊场景解决问题的利器，是一定要掌握的。\n\n正常循环的情况下，数组的滚动（游标移动）是向后的，引入栈的时候，则可以有了向前滚动的机会（有了一定的反悔的机会），然后这样子就能够解决一些局部的问题（比如说，寻找相邻的大的数字）。由于栈还可以对于没有价值（已经发现了大的数字）的东西删除，这样子的遗忘功能，简化了搜索空间，问题空间。\n\n当一个算法完全不进行多余的运算，那么它是一个时间复杂度最低的算法。但我们往往会对一些结果进行重复的计算，那么栈的引入就是为了解决这样的问题，栈存储了一些重要的运算结果，用于和接下来的元素进行比较。\n单调队列\n可以查询区间最值（不能维护区间k大，因为队列中很有可能没有k个元素）\n优化DP \t优化动态规划方面问题的一种特殊数据结构，且多数情况是与定长连续子区间问题相关联。\n单调栈       对于某个元素i：\n左边区间第一个比它小的数，第一个比它大的数\n确定这个元素是否是区间最值\n右边区间第一个大于它的值\n到 右边区间第一个大于它的值 的距离\n确定以该元素为最值的最长区间\n单调递增栈：只有比栈顶元素小的元素才能直接进栈，否则需要先将栈中比当前元素小的元素出栈，再将当前元素入栈。这样就保证了：栈中保留的都是比当前入栈元素大的值，并且从栈顶到栈底的元素值是单调递增的。\ndef monotoneIncreasingStack(nums):    stack = []    for num in nums:        while stack and num &gt;= stack[-1]:            stack.pop()        stack.append(num)\n2.1 寻找左侧第一个比当前元素大的元素 #\n从左到右遍历元素，构造单调递增栈（从栈顶到栈底递增）：\n\n一个元素左侧第一个比它大的元素就是将其「插入单调递增栈」时的栈顶元素。\n如果插入时的栈为空，则说明左侧不存在比当前元素大的元素。\n\n单调队列实际上是单调栈的的升级版。单调栈只支持访问尾部，而单调队列两端都可以。\n单调队列是指：队列中元素之间的关系具有单调性，而且，队首和队尾都可以进行出队操作，只有队尾可以进行入队操作。\n滑动窗口\n滑动窗口指的是这样一类问题的求解方法，在数组上通过双指针同向移动而解决的一类问题。将嵌套的循环问题，转换为单循环问题，降低时间复杂度。\n：寻找满足xx最长子串/子数组/子序列\n\n1.当不满足条件时，拓展右边界，当满足条件时，缩短左边界，最后得到一个解并暂存\n2.循环第一步，又得到一个解，将其与第一个解相对比，得到最优解并暂存，以此类推。\n347. 前 K 个高频元素\n最大（小）堆是指在树中，存在一个结点而且该结点有儿子结点，该结点的data域值都不小于（大于）其儿子结点的data域值，并且它是一个完全二叉树。\n对于 topk 问题：最大堆求topk小，最小堆求 topk 大。\ntopk小：构建一个 k 个数的最大堆，当读取的数小于根节点时，替换根节点，重新塑造最大堆 topk大：构建一个 k 个数的最小堆，当读取的数大于根节点时，替换根节点，重新塑造最小堆 eg. leetcode 215\n借助 哈希表 来建立数字和其出现次数的映射，遍历一遍数组统计元素的频率 维护一个元素数目为 k的最小堆why？\n每次都将新的元素与堆顶元素（堆中频率最小的元素）进行比较 如果新的元素的频率比堆顶端的元素大，则弹出堆顶端的元素，将新的元素添加进堆中 最终，堆中的 kk 个元素即为前 kk 个高频元素\n是使用小顶堆呢，还是大顶堆？\n有的同学一想，题目要求前 K 个高频元素，那么果断用大顶堆啊。\n那么问题来了，定义一个大小为k的大顶堆，在每次移动更新大顶堆的时候，每次弹出都把最大的元素弹出去了，那么怎么保留下来前K个高频元素呢。\n所以我们要用小顶堆，因为要统计最大前k个元素，只有小顶堆每次将最小的元素弹出，最后小顶堆里积累的才是前k个最大元素。\n原地修改输入数组\nhttps://blog.csdn.net/A233666/article/details/113956814\n如果不是原地修改的话，我们直接 new 一个 int[] 数组，把去重之后的元素放进这个新数组中，然后返回这个新数组即可。\n但是原地删除，不允许我们 new 新数组，只能在原数组上操作，然后返回一个长度，这样就可以通过返回的长度和原始数组得到我们去重后的元素有哪些了。\n这种需求在数组相关的算法题中时非常常见的，通用解法就是我们前文 双指针技巧 中的快慢指针技巧。\n我们让慢指针 slow 走在后面，快指针 fast 走在前面探路，找到一个不重复的元素就告诉 slow 并让 slow 前进一步。这样当 fast 指针遍历完整个数组 nums 后，nums[0..slow] 就是不重复元素。\n位运算\n最大化最小值/ 最小化最大值问题\n基本题型: 给定n个整数序列，将其划分为m个连续子序列，求这m个子序列的和的最大化最小值 或者最小化最大值问题。 Leetcode 410\n解题思路: 二分法\n二分查找细节https://leetcode.cn/problems/binary-search/solution/er-fen-cha-zhao-xiang-jie-by-labuladong/\nwhile 中是 &lt; 还是 &lt;=?\n答：left==right时是否需要终止循环，是否找到\nint right = nums.size(); // 定义target在左闭右开的区间里，即：[left, right)        while (left &lt; right) { // 因为left == right的时候，在[left, right)是无效的空间，所以使用 &lt;\n//二分查找int binary_search(int[] nums, int target) {    int left = 0, right = nums.length - 1;    while(left &lt;= right) {        int mid = left + (right - left) / 2;        if (nums[mid] &lt; target) {            left = mid + 1;        } else if (nums[mid] &gt; target) {            right = mid - 1;        } else if(nums[mid] == target) {            // 直接返回            return mid;        }    }    // 直接返回    return -1;}\n回溯法+剪枝\n这一类问题都需要先画出树形图，然后编码实现。\n编码通过 深度优先遍历 实现，使用一个列表，在 深度优先遍历 变化的过程中，遍历所有可能的列表并判断当前列表是否符合题目的要求\n如果题目要求，结果集不计算顺序，此时需要按顺序搜索，才能做到不重不漏。「力扣」第 47 题（ 全排列 II ）、「力扣」第 15 题（ 三数之和 ）也使用了类似的思想，使得结果集没有重复。\nRecursive algorithms can be both in-place and not-in-place, depending on how they are implemented. In computer science, an “in-place” algorithm is one that uses a constant amount of extra memory or auxiliary data structures to perform its operations, regardless of the size of the input data. On the other hand, a “not-in-place” algorithm uses additional memory that grows with the input size. 因此，虽然精确的空间复杂度分析（O(1)、O(n) 等）对于理论讨论和比较很有用，但现实世界的考虑通常会导致对算法的内存使用情况进行更细致的评估。目标是在空间效率和算法简单性之间取得平衡，使代码更容易理解和维护，同时仍然实现可接受的性能。\n组合数学\nCatalan 数列\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n可以应用于以下问题：\n\n\n有 2n个人排成一行进入剧场。入场费 5 元。其中只有 n个人有一张 5 元钞票，另外 n 人只有 10 元钞票，剧院无其它钞票，问有多少种方法使得只要有 10 元的人买票，售票处就有 5 元的钞票找零？\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n\n\n一位大城市的律师在她住所以北 n 个街区和以东 n 个街区处工作。每天她走 2n 个街区去上班。如果她从不穿越（但可以碰到）从家到办公室的对角线，那么有多少条可能的道路？\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n\n\n在圆上选择 2n 个点，将这些点成对连接起来使得所得到的  条线段不相交的方法数？\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n\n\n一个栈（无穷大）的进栈序列为1,2,3…n  有多少个不同的出栈序列？\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n\n\nn个结点可构造多少个不同的二叉树？\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n\n\n\n97. 交错字符串\n给定三个字符串 s1、s2、s3，请你帮忙验证 s3 是否是由 s1 和 s2 **交错 组成的。\n两个字符串 s 和 t 交错 的定义与过程如下，其中每个字符串都会被分割成若干 非空 子字符串：\n\ns = s1 + s2 + ... + sn\nt = t1 + t2 + ... + tm\n|n - m| &lt;= 1\n交错 是 s1 + t1 + s2 + t2 + s3 + t3 + ... 或者 t1 + s1 + t2 + s2 + t3 + s3 + ...\n\n怎么想到DP解法而不是双指针 有感：\n斐波那契数列：\n\nDFS解法是递归地计算第 n 个斐波那契数。\n该问题可以转换为DP，通过存储每个子问题的解来减少重复计算，例如使用数组或哈希表来存储已计算的结果。\n\nNP-完全问题：\n\n一些问题，如旅行商问题（TSP）等属于NP-完全问题，DFS可能可以用来找到解，但难以以多项式时间转换为DP。\n\n大部分能暴力递归式（dfs）解决的问题就在形式上是dp的了，你只要把暴力递归式的输入参数当成状态来看待，真正的难点在于把暴力递归的状态进行压缩合并变到多项式大小的状态集合，所以不是你意识不到他是不是dp问题，而是你没有足够的经验和思路去把一个算法最简单的状态集合设计出来. When doing leetcode, using dp的难点是你就算知道了用dp，也可能想不出状态转移方程\n有时即使你意识到问题可以用DP解决，也可能遇到难以找到状态转移方程的困难。这可能需要更多的经验、练习和对问题的探索，以便设计出最优的状态集合和转移方程。\n• 动态规划可以被看作是优化后的暴力递归版本。它通常通过存储子问题的解并避免重复计算来提高效率，从而将指数级的时间复杂度降低为多项式级别。\n字符串的题 dfs 可作为一种解法。 遇到字符串(字串，子数组，子序列)题，先想DP.…\n树和二叉树\n快速排序就是个二叉树的前序遍历，归并 排序就是个二叉树的后序遍历\n动归/DFS/回溯算法都可以看做二叉树问题的扩展，只是它们的关注点不同：\n\n动态规划：动态规划是一种将问题分解为子问题，并以自底向上的方式解决的方法。每个子问题的解决方案被记录下来，以避免重复计算。你可以将动态规划问题视为填充一张二维表格，其中每个格子代表一个子问题的解，从而形成一种树状的结构。这与二叉树的概念有些类似。\n回溯：回溯是一种深度优先的搜索方法，通常用于解决排列、组合、子集等问题。你可以将回溯过程看作在一个决策树上的遍历，每个节点代表一个选择，通过遍历树上的路径来寻找解。你的理解关于回溯关注于节点间的「树枝」是正确的。\nDFS：深度优先搜索是一种遍历图或树的方法。它从一个起始节点出发，沿着一个路径一直向下遍历，直到无法继续为止，然后回溯并探索其他分支。你的理解关于DFS关注于单个「节点」也是正确的。\n\n遍历\n\n中序遍历：递归，栈，移动右子树使用pre指针遍历\nMorris遍历\n\n这个算法的核心思想是在遍历过程中修改树的结构，将节点的右子树指向后继节点，然后再恢复树的结构，以便能够顺序遍历节点。这种方法在空间效率上具有显著优势，但需要小心处理节点的指针，以避免陷入无限循环。\nBST AVL\nBST二叉查找树（排序树），若它的左子树不空，则左子树上所有的结点的值均不大于它根结点的值；　　若它的左子树不空，则左子树上所有的结点的值均不小于它根结点的值；\n最重要的性质是：二叉搜索树的中序遍历是有序的\n当节点数目一定，保持树的左右两端保持平衡，树的查找效率最高。\n这种左右子树的高度相差不超过 1 的树为平衡二叉树。\n性质：\n\n可以是空树。\n假如不是空树，任何一个结点的左子树与右子树都是平衡二叉树，高度之差的绝对值不超过 1 。在一棵平衡二叉树中，节点的平衡因子只能取 0 、1 或者 -1 ，分别对应着左右子树等高，左子树比较高，右子树比较高。\n\n以递归解决二叉树这种对称数据结构的策略，称为对称性递归。可以用**对称性递归解决的二叉树问题大多是判断性问题(bool类型函数),**这一类问题又可以分为以下两类：https://leetcode.cn/problems/shu-de-zi-jie-gou-lcof/solution/yi-pian-wen-zhang-dai-ni-chi-tou-dui-che-uhgs/\n1、不需要构造辅助函数。这一类题目有两种情况：第一种是单树问题，且不需要用到子树的某一部分(比如根节点左子树的右子树)，只要利用根节点左右子树的对称性即可进行递归。第二种是双树问题，即本身题目要求比较两棵树，那么不需要构造新函数。该类型题目如下：\n\n相同的树 翻转二叉树\n二叉树的最大深度\n平衡二叉树\n二叉树的直径\n合并二叉树 另一个树的子树 单值二叉树\n\n2、需要构造辅助函数。这类题目通常只用根节点子树对称性无法完全解决问题，必须要用到子树的某一部分进行递归，即要调用辅助函数比较两个部分子树。形式上主函数参数列表只有一个根节点，辅助函数参数列表有两个节点。该类型题目如下：\n\n对称二叉树 剑指 Offer 26. 树的子结构\n\n100. 相同的树，并注意与这道题的区别：剑指 Offer 26. 树的子结构。与字符串对比的话，子树就相当于字符串的子串（要求连续），树的子结构就相当于字符串的子序列（不要求连续）\n为什么还需要非线性结构呢？ 答案是为了高效地兼顾静态操作和动态操作，我们一般使用树去管理需要大量动态操作的数据\n堆排序的基本思想是先将待排序的序列构建成一个堆，然后依次从堆顶取出最值（最大值或最小值），将其与堆的最后一个元素交换，并将堆的大小减一，然后再通过一系列操作使得剩余的元素重新构建成一个堆。重复执行此过程，直到堆为空，从而得到一个有序的序列。\n堆排序的主要步骤如下：\n\n构建初始堆：将待排序序列构建成一个初始堆，即满足堆的特性。\n交换和调整：将堆顶元素与堆的最后一个元素交换位置，并将堆的大小减一。然后通过向下调整（或向上调整）操作，使剩余元素重新构建成一个堆。\n重复执行步骤2，直到堆为空。\n\n由于完全二叉树的性质，堆排序可以高效地在数组中进行操作，因为堆的结构可以直接映射到数组的索引上，不需要显式使用指针。2i, 2i+1\n垂直遍历leetcode.314\n   3  /\\ /  \\ 9  20    /\\   /  \\  15   7  输入： {3,9,20,#,#,15,7}输出： [[9],[3,15],[20],[7]]public List&lt;List&lt;Integer&gt;&gt; verticalOrder(TreeNode root) {        // Write your code here        List&lt;List&lt;Integer&gt;&gt; results = new ArrayList&lt;&gt;();        if (root == null) {            return results;        }        Map&lt;Integer, List&lt;Integer&gt;&gt; map = new TreeMap&lt;Integer, List&lt;Integer&gt;&gt;();        Queue&lt;Integer&gt; qCol = new LinkedList&lt;&gt;();        Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;();        queue.offer(root);        qCol.offer(0);        while(!queue.isEmpty()) {            TreeNode curr = queue.poll();            int col = qCol.poll();            if(!map.containsKey(col)) {                map.put(col, new ArrayList&lt;Integer&gt;(Arrays.asList(curr.val)));            } else {                map.get(col).add(curr.val);            }            if(curr.left != null) {                queue.offer(curr.left);                qCol.offer(col - 1);            }            if(curr.right != null) {                queue.offer(curr.right);                qCol.offer(col + 1);            }        }        for(int n : map.keySet()) {            results.add(map.get(n));        }        return results;    }\n前缀和构造多叉树\n前缀和处理数组区间问题，快速得到某个子数组的和\nTrie（发音类似 “try”）或者说 前缀树 字典树是一种树形数据结构，用于高效地存储和检索字符串数据集中的键。这一数据结构有相当多的应用情景，例如自动补完和拼写检查。\n前缀和是一种重要的预处理，能大大降低查询的时间复杂度。我们可以简单理解为“数列的前 n 项的和”。这个概念其实很容易理解，即一个数组中，第 n 位存储的是数组前 n 个数字的和。\n通过一个例子来进行说明会更清晰。题目描述：有一个长度为 N 的整数数组 A，要求返回一个新的数组 B，其中 B 的第 i 个数 B[i]是原数组 A 前 i 项和。\n2559. 统计范围内的元音字符串数\n难度中等9收藏分享切换为英文接收动态反馈\n给你一个下标从 0 开始的字符串数组 words 以及一个二维整数数组 queries 。\n每个查询 queries[i] = [li, ri] 会要求我们统计在 words 中下标在 li 到 ri 范围内（包含 这两个值）并且以元音开头和结尾的字符串的数目。\n返回一个整数数组，其中数组的第 i 个元素对应第 i 个查询的答案。\n**注意：**元音字母是 'a'、'e'、'i'、'o' 和 'u' 。\nhttps://leetcode.cn/problems/count-vowel-strings-in-ranges/\n2575. 找出字符串的可整除数组\n难度中等9收藏分享切换为英文接收动态反馈\n给你一个下标从 0 开始的字符串 word ，长度为 n ，由从 0 到 9 的数字组成。另给你一个正整数 m 。\nword 的 可整除数组 div  是一个长度为 n 的整数数组，并满足：\n\n如果 word[0,...,i] 所表示的 数值 能被 m 整除，div[i] = 1\n否则，div[i] = 0\n\n返回 **word 的可整除数组。\n思路： 如何想到递归求模\nfirst：\n\n(a + b) % p = (a % p + b % p) % p （1）\n(a - b) % p = (a % p - b % p) % p （2）\n(a * b) % p = (a % p * b % p) % p （3）\na ^ b % p = ((a % p)^b) % p （4）\n\nsecond：\n\n记N[i]为word[0 ~ i]表示的值。\n记n[i]为word[i]表示的数。\n不难得出 : N[i] = N[i - 1] * 10 + n[i]\n在此假设 : N[i - 1] = p * m + q(即余数是q)\n那么 : N[i] % m = (p * m * 10) % m + (q * 10 + n[i ] ) % m\n其中 : (p * m * 10) % m必能整除, 因此只要看后半部分\n\n平方根\n历史上至少有过两个问题，它们看起来非常困难，非常不像 P 问题，但在人们的不懈努力之下，最终还是成功地加入了 P 问题的大家庭。其中一个是线性规划（linear programming），它是一种起源于二战时期的运筹学模型。1947 年，乔治·丹齐格（George Dantzig）提出了一种非常漂亮的算法叫作“单纯形法”（simplex algorithm），它在随机数据中的表现极为不错，但在最坏情况下却需要耗费指数级的时间。因此，很长一段时间，人们都在怀疑，线性规划是否有多项式级的算法。直到 1979 年，人们才迎来了线性规划的第一个多项式级的算法，它是由前苏联数学家列昂尼德·哈奇扬（Leonid Khachiyan）提出的。另外一个问题则是质数判定问题（primality test）：判断一个正整数是否是质数（prime），或者说判断一个正整数是不是无法分成两个更小的正整数之积。人们曾经提出过各种质数判定的多项式级算法，但它们要么是基于概率的，要么是基于某些假设的，要么是有一定适用范围的。2002 年，来自印度理工学院坎普尔分校的阿格拉瓦尔（M. Agrawal）、卡亚勒（N. Kayal）和萨克斯泰纳（N. Saxena）发表了一篇重要的论文《PRIMES is in P》，给出了第一个确定性的、时间复杂度为多项式级别的质数判定算法，质数判定问题便也归入了 P 问题的集合。很容易看出，找出一个多项式级的答案验核算法，再怎么也比找出一个多项式级的答案获取算法更容易。\n为了练习函数与循环，判断一个数是否为质数：我们来实现一个平方根函数：用牛顿法实现平方根函数。\n\n法一：牛顿迭代法的本质是借助泰勒级数，从初始值开始快速向零点逼近。\n计算机通常使用循环来计算 x 的平方根。从某个猜测的值 z 开始，我们可以根据 z² 与 x 的近似度来调整 z，产生一个更好的猜测：\nz -= (z*z - x) / (2*z)    long c=x;        while(c*c&gt;x){            c = (c+x/c)/2;//怎么得出来的？        }    return (int)c;\n法二：二分查找逼近\n在数学中，数根(又称位数根或数字根Digital root)是自然数的一种性质，换句话说，每个自然数都有一个数根。\n数根是将一正整数的各个位数相加（即横向相加），若加完后的值大于10的话，则继续将各位数进行横向相加直到其值小于十为止[1]，或是，将一数字重复做数字和，直到其值小于十为止，则所得的值为该数的数根。\n例如54817的数根为7，因为5+4+8+1+7=25，25大于10则再加一次，2+5=7，7小于十，则7为54817的数根。\n然后是它的用途。\n数根可以计算模运算的同余，对于非常大的数字的情况下可以节省很多时间。\n数字根可作为一种检验计算正确性的方法。例如，两数字的和的数根等于两数字分别的数根的和。\n另外，数根也可以用来判断数字的整除性，如果数根能被3或9整除，则原来的数也能被3或9整除。\n堆的三种操作：\n1删除堆顶元素的方法： 常见操作是用数组尾部元素替换堆顶，这里不直接删除堆顶，因为所有的元素会向前移动一位，会破坏了堆的结构\n然后进行下移操作，将新的堆顶和它的子节点进行交换，直到子节点大于等于这个新的堆顶，删除堆顶的时间复杂度为O(logk)\n2堆化：就是将任意数组调整为堆的结构。\n\n任意数组都可以看做一颗完全二叉树\n从当前这个完全二叉树的最后一个非叶子节点开始进行元素下沉（siftDown）操作，逐步将这颗二叉树调整为堆结构     buildHeap 第二种 从堆的顶部（数组的开头）开始，并对每个项目调用 siftUp。\n\n3 插入\n\n法一：交换法\n法三:哨兵法\n\nimport java.util.ArrayList;public class Heap {  private ArrayList&lt;Integer&gt; data;  private boolean isMaxHeap;  public Heap(boolean isMaxHeap) {      data = new ArrayList&lt;&gt;();      this.isMaxHeap = isMaxHeap;  }// 建堆public void buildHeap(int[] arr) {    for (int num : arr) {        data.add(num);    }    for (int i = parent(data.size() - 1); i &gt;= 0; i--) {        siftDown(i);    }}// 插入 在堆中插入新元素后维护堆的性质public void insert(int num) {    data.add(num);    siftUp(data.size() - 1);}// 删除最大值public int delete() {    int res = data.get(0);    data.set(0, data.get(data.size() - 1));    data.remove(data.size() - 1);    siftDown(0);    return res;}// 查询最大/小值public int peek() {    return data.get(0);}private void siftUp(int i) {    while (i &gt; 0 &amp;&amp; data.get(parent(i)).compareTo(data.get(i)) &gt; 0) {        swap(i, parent(i));        i = parent(i);    }}private void siftDown(int i) {    int maxIndex = i;    int left = leftChild(i);    if (left &lt; data.size() &amp;&amp; compare(data.get(left), data.get(maxIndex)) &gt; 0) {        maxIndex = left;    }    int right = rightChild(i);    if (right &lt; data.size() &amp;&amp; compare(data.get(right), data.get(maxIndex)) &gt; 0) {        maxIndex = right;    }    if (i != maxIndex) {        swap(i, maxIndex);        siftDown(maxIndex);    }}对于任意节点 i，其左子节点的索引为 2i+1，右子节点的索引为 2i+2，而父节点的索引为 floor((i-1)/2)。private int parent(int i) {    return (i - 1) / 2;}private int leftChild(int i) {    return 2 * i + 1;}private int rightChild(int i) {    return 2 * i + 2;}private void swap(int i, int j) {    int temp = data.get(i);    data.set(i, data.get(j));    data.set(j, temp);}private int compare(int a, int b\nQuicksort基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。\n三路快排 当序列中有大量的重复元素，二路排序虽然会均衡的分配到两个序列中，但是重复元素仍然参与到分割序列中，带来无谓的性能损耗。\n     public void sort(int[] args, int l, int r) {        if (l &gt;= r) {            return;        }    int target = args[l];    int lt = l, gt = r + 1, i = l + 1;    while (i &lt; gt) {        if (args[i] &lt; target) {            swap(args, i, lt + 1);            lt++;            i++;        } else if (args[i] &gt; target) {            swap(args, i, gt - 1);            gt--;        } else {            i++;        }    }    swap(args, l, lt);    sort(args, l, lt - 1);    sort(args, gt, r);    }//快速选择算法public int findKthLargest(int[] nums, int k) {        int left = 0;        int right = nums.length - 1;        int pivot = partition(nums, left, right);        // 从小到大排序，倒数第k个就是第k个最大元素        int targetIndex = nums.length - k;        while (true) {            if (pivot == targetIndex) {                return nums[pivot];            } else if (pivot &gt; targetIndex) {                right = pivot - 1;            } else {                left = pivot + 1;            }            pivot = partition(nums, left, right);        }    }    int partition(int[] nums, int left, int right) {        // 基准值索引        int pivot = left;        // 预放值索引        int index = left + 1;        // 将比基准值小的都紧挨着基准值        for (int i = index; i &lt;= right; i++) {            if (nums[i] &lt; nums[pivot]) {                swap(nums, i, index);                index++;            }        }        // 把基准值移动到比基准值小的数列的最右边        swap(nums, pivot, index - 1);        return index - 1;    }\n快速选择算法过一次遍历，确定某一个元素在排序以后的位置，这个算法叫「快速选择」。要理解「快速选择」算法，必须先理解「快速排序」的「partition」。 快速排序会递归处理划分的两边，而选择只处理划分一边。\n\nLRUcache\n解法\n1.LinkedHashmap。伪头部（dummy head）和伪尾部（dummy tail）标记界限，这样在添加节点和删除节点的时候就不需要检查相邻的节点是否存在。 当向缓存中添加新项时，如果达到容量限制，将删除链表头部（最少使用）的节点，并更新哈希表。\n使用双向链表加哈希表维护get 和 put  O(1) 平均时间复杂度\n2.自己实现LinkedHashmap\n\nLRU 的功能可以使用双向链表实现，访问到的节点移动到头部，超出容量的从尾部删除。\n要实现O(1)得使用HashMap，里面储存 key 与 链表节点即可，这样可以快速定位节点，然后删除它，将它移动到链表头部。\n\n3.O（n）解法 好写\n\n使用HashMap来存储键值对，其中键是缓存的键，值是对应的缓存项。\n使用ArrayList来维护最近访问的顺序，最近访问的项位于列表的末尾。\n当访问缓存项时，将其移到ArrayList的末尾，以表明它是最近访问的。\n当缓存达到容量限制时，淘汰最近最少使用的项（即ArrayList的头部项）。\n\n• ArrayList 中的移除和添加操作是线性时间复杂度 O(n)。但由于在缓存中存储的是键，因此在 ArrayList 中查找和移除元素的复杂度可以视为 O(n)。\nLFU 的缓存污染问题：\nLFU 通常根据缓存中条目的访问频率来替换最不经常使用的条目。但是，LFU 在某些情况下可能出现“缓存污染”问题：\n新数据问题： 当一些数据被频繁访问但实际上不是常用数据时，它们的频率计数会增加，导致 LFU 将其视为常用数据，长时间占据缓存空间。\n突发事件问题： 在短时间内，某些数据可能会因为特定事件而被频繁访问，这些数据的频率计数可能会暂时性地高于实际常用数据。\nLRU 的长环模式问题：\nLRU 根据最近最少使用的原则进行缓存替换，但存在一个“长环模式”问题：\n周期性访问模式： 如果存在一组数据被周期性地访问（例如，每隔一段时间访问一次），而且这组数据的访问顺序与 LRU 的缓存替换顺序一致，就会导致这些数据在缓存中形成长环。\n替换不及时： 长环中的数据虽然可能在某段时间内并不频繁使用，但由于周期性访问模式，它们的位置始终位于 LRU 缓存替换算法的末尾，因此不容易被替换出去。\n解决方法：\n针对这些问题，可以考虑使用一些改进的缓存算法或结合其他策略来提高缓存效率，如：\nLFU 的改进版本： 可以考虑采用 LFU 的变体算法，如动态调整频率计数的算法，以更准确地反映数据的实际热度。\nLRU 的改进版本： 引入时间衰减机制，使得长时间不被访问的数据在一定时间后被逐渐淘汰，而不仅仅依赖于访问顺序。\n混合替换策略： 使用两种或多种不同的缓存替换策略组合，根据具体情况动态选择合适的替换策略。\n在可计算性理论与计算复杂性理论中，所谓的归约是将某个计算问题变换为另一个问题的过程。可用归约法定义某些问题的复杂度类（因变换过程而异）。P NP\n以直觉观之，如果存在能有效解决问题B的算法，也可以作为解决问题A的子程序，则将问题A称为“可归约”到问题B，因此求解A并不会比求解B更困难。\nA(x) = B(f(x)), in other words, for x∈A，there’s f(x)∈B\nWEB开发\nRestful规范\nREST:REpresentational State Transfer 直接翻译：表现层状态转移\nREST建立在 HTTP 协议之上，利用 HTTP 方法和资源来进行通信和交互。\n转移（Transfer）：无论状态是由服务端还是客户端来提供的，“取下一篇文章”这个行为逻辑必然只能由服务端来提供，因为只有服务端拥有该资源及其表征形式。服务器通过某种方式，把“用户当前阅读的文章”转变成“下一篇文章”，这就被称为“表征状态转移”\nURL定位资源，用HTTP描述操作。\n看Url就知道要什么 看http method就知道干什么 看http status code就知道结果如何\n为什么要用RESTful结构呢？\n大家都知道“古代”网页是前端后端融在一起的，比如之前的PHP，JSP等。在之前的桌面时代问题不大，但是近年来移动互联网的发展，各种类型的Client层出不穷，RESTful可以通过一套统一的接口为 Web，iOS和Android提供服务。另外对于广大平台来说，比如Facebook platform，微博开放平台，微信公共平台等，它们不需要有显式的前端，只需要一套提供服务的接口\n6大原则\n1）客户端 - 服务器 - 通过将用户接口问题与数据存储问题分开，我们通过简化服务器组件来提高跨多个平台的用户接口的可移植性并提高可伸缩性。\n2）无状态 - 从客户端到服务器的每个请求都必须包含理解请求所需的所有信息，并且不能利用服务器上任何存储的上下文。因此，会话状态完全保留在客户端上。无状态是 REST 的一条核心原则，部分开发者在做服务接口规划时，觉得 REST 风格的服务怎么设计都感觉别扭，很有可能的一种原因是在服务端持有着比较重的状态。REST 希望服务器不要去负责维护状态，每一次从客户端发送的请求中，应包括所有的必要的上下文信息，会话信息也由客户端负责保存维护，服务端依据客户端传递的状态来执行业 务处理逻辑，驱动整个应用的状态变迁。客户端承担状态维护职责以后，会产生一些新 的问题，譬如身份认证、授权等可信问题，它们都应有针对性的解决方案（这部分内容 可参见“安全架构”的内容）。 但必须承认的现状是，目前大多数的系统都达不到这个要求，往往越复杂、越大型的系统越是如此。服务端无状态可以在分布式计算中获得非常高价值的好处，但大型系统的上下文状态数量完全可能膨胀到让客户端在每次请求时提供变得不切实际的程度，在服务端的内存、会话、数据库或者缓存等地方持有一定的状态成为一种是事实上存在，并将长期存在、被广泛使用的主流的方案。\n3）可缓存 - 缓存约束要求将对请求的响应中的数据隐式或显式标记为可缓存或不可缓存。如果响应是可缓存的，则客户端缓存有权重用该响应数据以用于以后的等效请求。\n4）统一接口 - 通过将通用性的软件工程原理应用于组件接口，简化了整个系统架构，提高了交互的可见性。为了获得统一的接口，需要多个架构约束来指导组件的行为。REST由四个接口约束定义： 资源识别;\n通过陈述来处理资源;\n自我描述性的信息; 并且，超媒体作为应用程序状态的引擎。\n5）分层系统 - 分层系统风格允许通过约束组件行为来使体系结构由分层层组成，这样每个组件都不能“看到”超出与它们交互的直接层。\n6）按需编码（可选） - REST允许通过以小程序或脚本的形式下载和执行代码来扩展客户端功能。这通过减少预先实现所需的功能数量来简化客户端。\n问题\n\n仅支持请求/响应通信： REST 风格的通信模式是基于请求和响应的。这意味着它不适合实时或持续通信，例如实时数据流。对于需要双向通信或服务器推送数据给客户端的应用场景，REST 不是最佳选择。\n可用性受限： REST 通常要求客户端和服务器同时在线才能进行通信。如果其中一方离线，通信将无法进行，这可能降低系统的可用性。\n服务发现的需求： 客户端需要知道服务的 URL 才能进行通信。这意味着客户端必须使用服务发现机制来发现服务实例的位置和地址，可能会增加系统的复杂性。\n获取多个资源的挑战性： 在单个请求中获取多个资源可能会有挑战。在传统的 RESTful 架构中，通常需要多个请求来获取不同资源，并且客户端必须合并和处理这些响应。面向资源的编程思想只适合做 CRUD 抽象资源。所谓的抽象资源是“不直接对应数据库表的资源”。比如“登录/登出”。如果你用RPC式的接口设计，这就很直接：POST /login 和 POST /logout。但是你要硬套RESTful，你就得挖空心思的想出一个抽象资源“会话”（session）。登录 = 创建会话，登出 = 销毁会话。于是你把接口设计成 POST /sessions 和 DELETE /sessions。这就很反直觉，且背离RESTful的设计初衷。 元数据如何返回也是个问题。比如分页查询是很常见的需求。但是当前是第几页，一共多少条记录等，这些元数据放在响应的哪儿呢？如果放在body里，有点不REST，因为REST的响应应该只包含数据；如果放在header里，则又要和客户端商定协议细节，客户端实现也变得更困难。\n多个更新操作的映射困难： 将多个更新操作映射到 HTTP 动词可能会有挑战。例如，使用 HTTP 方法如 PUT 或 PATCH 对多个资源进行原子性更新可能会复杂化，因为 HTTP 方法通常用于单个资源的操作。\n\nREST 与 HTTP 完全绑定，不适合应用于要求高性能传输的场景中\n对于需要直接控制传输，如二 进制细节、编码形式、报文格式、连接方式等细节的场景中，REST 确实不合适，这些场 景往往存在于服务集群的内部节点之间，这也是之前曾提及的，REST 和 RPC 尽管应用 场景的确有所重合，但重合的范围有多大就是见仁见智的事情。 RESTful 和 HTTP 绑的太死，很难用在其他传输协议里，比如你很难把 REST 用在 Redis PubSub 里。相比之下，一个包含指令和数据的RPC包通过HTTP发还是通过Kafka发都没问题\nREST 不利于事务支持\nREST 没有传输可靠性支持 是的，并没有。在 HTTP 中你发送出去一个请求，通常会收到一个与之相对的响应，譬 如 HTTP/1.1 200 OK 或者 HTTP/1.1 404 Not Found 诸如此类的。但如果你没有收到任 何响应，那就无法确定消息到底是没有发送出去，抑或是没有从服务端返回回来，这其 中的关键差别是服务端到底是否被触发了某些处理？应对传输可靠性最简单粗暴的做法 是把消息再重发一遍。这种简单处理能够成立的前提是服务应具有幂等性\n服务发现之所以重要，是因为它解决了微服务架构最关键的问题：如何精准的定位需要调用的服务ip以及端口。无论使用哪种方式来提供服务发现功能，大致上都包含以下三点：\n\nRegister, 服务启动时候进行注册\nQuery, 查询已注册服务信息\nHealthy Check,确认服务状态是否健康\n\n整个过程很简单。大致就是在服务启动的时候，先去进行注册，并且定时反馈本身功能是否正常。由服务发现机制统一负责维护一份正确或者可用的服务清单。因此，服务本身需要能随时接受查下，反馈调用方服务所要的信息。\n在分布式系统中，服务发送心跳信息并不能完全反映其真正的健康状态。以下是一些方法和策略，可以帮助应对这种情况：主动健康检查：针对服务发送心跳信息而未能准确反映其真实状态的问题，需要使用更为全面和准确的健康检查机制，并在设计系统时实施弹性策略和监控机制\nSpring 是分层的企业级应用轻量级开源框架，以 IoC 和 AOP为内核。\n\n用来简化Spring应用的初始搭建以及开发过程，使用特定的方式来进行配置\n创建独立的Spring引用程序main方法运行\n嵌入的tomcat无需部署war文件\n简化maven配置\n自动配置Spring添加对应的功能starter自动化配置\n\nIoC控制反转，就是把原先我们代码里面需要实现的对象创建、依赖的代码，反转给容器来帮忙实现。当应用了IoC，一个对象依赖的其它对象会通过被动的方式传递进来，而不是这个对象自己创建或者查找依赖对象。IoC 的实现方式有依赖注入和依赖查找，由于依赖查找使用的很少，因此 IoC 也叫做依赖注入。依赖注入指对象被动地接受依赖类而不用自己主动去找，对象不是从容器中查找它依赖的类，而是在容器实例化对象时主动将它依赖的类注入给它。假设一个 Car 类需要一个 Engine 的对象，那么一般需要需要手动 new 一个 Engine，利用 IoC 就只需要定义一个私有的 Engine 类型的成员变量，容器会在运行时自动创建一个 Engine 的实例对象并将引用自动注入给成员变量。\nAOP 即面向切面编程，简单地说就是将代码中重复的部分抽取出来，在需要执行的时候使用动态代理的技术，在不修改源码的基础上对方法进行增强。优点是可以减少代码的冗余，提高开发效率，维护方便。常用场景包括权限认证、自动缓存、错误处理、日志、调试和事务等。\n相关注解\n@Aspect：声明被注解的类是一个切面 Bean。\n@Before：前置通知，指在某个连接点之前执行的通知。\n@After：后置通知，指某个连接点退出时执行的通知（不论正常返回还是异常退出）。\nSpring MVC 核心组件\nDispatcherServlet：SpringMVC 中的前端控制器，是整个流程控制的核心，负责接收请求并转发给对应的处理组件。控制器，是整个流程控制的核心，负责接收请求并转发给对应的处理组件。\nHandler：处理器，完成具体业务逻辑，相当于 Servlet 或 Action。\nHandlerMapping：完成URL 到 Controller映射的组件，DispatcherServlet 接收到请求之后，通过 HandlerMapping 将不同的请求映射到不同的 Handler。\nMybatis 是一个实现了数据持久化的 ORM 框架，简单理解就是对 JDBC 进行了封装。\nORM全称是：Object Relational Mapping(对象关系映射](https://www.zhihu.com/search?q=对象关系映射&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={“sourceType”%3A“article”%2C“sourceId”%3A“27188788”}))，其主要作用是在编程中，把面向对象的概念跟数据库中表的概念对应起来。举例来说就是，我定义一个对象，那就对应着一张表，这个对象的实例，就对应着表中的一条记录。\nSpring 启动时读取应用程序提供的Bean配置信息，并在Spring容器中生成一份相应的Bean配置注册表，然后根据这张注册表实例化Bean，装配好Bean之间的依赖关系，为上层应用提供准备就绪的运行环境。\nDatabase\n\n对于数据库而言，重要的不是数据量，而是当数据量增加时运算如何增加。\n\n时间复杂度用来检验某个算法处理一定量的数据要花多长时间，时间复杂度不会给出确切的运算次数，但是给出的是一种理念。\nNoSQL指非关系型数据库，与关系型数据库存在显著差异。其中最重要之处在于NoSQL数据库不适用SQL语言。其数据存储不需要固定的表格模式，一般都有水平可扩展性。NoSQL数据库分为以下几类：\n①Key/Value键值存储。这种数据存储通常都无数据结构，被当作字符串或者二进制数据，但是数据加载快，典型应用于高并发和日志系统场景。如Redis。\n②列存储数据库。列存储数据库功能相对局限，但是查找速度快，易分布式扩展。一般用于分布式文件系统。如Hbase、Cassandra\n③文档型数据库。和键值对数据库类似，也没有严格数据格式。不需要预先创建表结构，数据格式更加灵活，一般用户Web应用。如MongoDB。\n④图形数据库。图形数据库专注于构建关系图谱，如社交网络、推荐系统等等。这类网络有Neo4j、DEX。\nNoSQL种类繁多，Spring Boot支持绝大多数。我们主要介绍常见的Redis和MongoDB。\nDBMS: XML,file system\nMysql\nMySQL 的架构共分为两层：Server 层和存储引擎层，\n\nServer 层负责建立连接、分析和执行 SQL。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。\n存储引擎层负责数据的存储和提取。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。\n\n执行一条 SQL 查询语句，期间发生了什么？\n\n连接器：建立连接，管理连接、校验用户身份；\n查询缓存：查询语句如果命中查询缓存则直接返回，否则继续往下执行。MySQL 8.0 已删除该模块；\n解析 SQL，通过解析器对 SQL 查询语句进行词法分析、语法分析，然后构建语法树，方便后续模块读取表名、字段、语句类型；\n执行 SQL：执行 SQL 共有三个阶段：\n\n预处理阶段：检查表或字段是否存在；将 select * 中的 `` 符号扩展为表上的所有列。\n优化阶段：基于查询成本的考虑， 选择查询成本最小的执行计划；\n执行阶段：根据执行计划执行 SQL 查询语句，从存储引擎读取记录，返回给客户端；\n\n\n\n索引\n数据库索引是对数据库表的一列或者多列的值进行排序一种结构，使用索引可以快速访问数据表中的特定信息。\n索引的优缺点？\n优点：\n\n大大加快数据检索的速度。\n将随机I/O变成顺序I/O(因为B+树的叶子节点是连接在一起的)\n加速表与表之间的连接\n\n缺点：\n\n从空间角度考虑，建立索引需要占用物理空间\n从时间角度 考虑，创建和维护索引都需要花费时间，例如对数据进行增删改的时候都需要维护索引。\n\n索引的实现通常使用Hash索引和B+树（MySQL常用的索引就是B+树）。除了数据之外，数据库系统还维护为满足特定查找算法的数据结构，这些数据结构以某种方式引用数据，这种数据结构就是索引。简言之，索引就类似于书本，字典的目录。以下是一些关键概念和要点与数据库索引相关：\n\n索引类型：\n\nB树索引：是最常见的索引类型，适用于大多数数据库系统。B树索引在树结构中进行数据划分，使得在平均情况下，检索时间复杂度为O(log n)。\n哈希索引：基于哈希算法，适用于等值查询。但是，哈希索引不适用于范围查询，且不适合于有序数据。\n全文索引：用于文本数据的高效搜索，支持关键词搜索、模糊查询等。\n空间索引：用于地理空间数据的索引，可以支持地理坐标查询和范围查询。\n\n\n\n什么时候不需要创建索引？\n\nWHERE 条件，GROUP BY，ORDER BY 里用不到的字段，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用物理空间的。\n字段中存在大量重复数据，不需要创建索引，比如性别字段，只有男女，如果数据库表中，男女的记录分布均匀，那么无论搜索哪个值都可能得到一半的数据。在这些情况下，还不如不要索引，因为 MySQL 还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描。\n表数据太少的时候，不需要创建索引；\n经常更新的字段不用创建索引，比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于要维护 B+Tree的有序性，那么就需要频繁的重建索引，这个过程是会影响数据库性能的。\n\nMySQL 索引\n索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现。InnoDB将磁盘数据建立索引并储存在B+树中\n\nB+Tree 索引\n是大多数 MySQL 存储引擎的默认索引类型。\n\nB树是一种多路平衡查找树\nB+ 树查询效率更稳定。因为 B+ 树每次只有访问到叶子节点才能找到对应的数据，而在 B 树中，非叶子节点也会存储数据，这样就会造成查询效率不稳定的情况，有时候访问到了非叶子节点就可以找到关键字，而有时需要访问到叶子节点才能找到关键字。\n其次，B+ 树的查询效率更高，这是因为通常 B+ 树比 B 树更矮胖（阶数更大，深度更低），查询所需要的磁盘 I/O 也会更少。同样的磁盘页大小，B+ 树可以存储更多的节点关键字。\nB+Tree 只在叶子节点存储数据，而 B 树 的非叶子节点也要存储数据，所以 B+Tree 的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点。 另外，B+Tree 叶子节点采用的是双链表连接，适合 MySQL 中常见的基于范围的顺序查找，而 B 树无法做到这一点。\n2、B+Tree vs Hash， Hash 在做等值查询的时候效率贼快，搜索复杂度为 O(1)。 但是 Hash 表不适合做范围查询，它更适合做等值的查询，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因。\n红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，这是因为使用 B+ 树访问磁盘数据有更高的性能。\nWhy use b+ tree as index?\n二叉搜索树查询效率无疑是最高的，因为平均来说每次比较都能缩小一半的搜索范围  所以表面上来看我们使用 B、B+ 树没有 二叉查找树效率高，但是实际上由于 B、B+ 树降低了树高，减少了磁盘 IO 次数，反而大大提升了速度。\n（一）B+ 树比其他Avl树有更低的树高\n平衡树的树高 O(h)=O(log_dN)，其中 d 为每个节点的出度。红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 h 很明显比 B+ Tree 大非常多。\n（二）磁盘访问原理\nB树被作为实现索引的数据结构被创造出来，是因为它能够完美的利用“局部性原理”。\n局部性原理：软件设计要尽量遵循“数据读取集中”与“使用到一个数据，大概率会使用其附近的数据”，这样磁盘预读能充分提高磁盘IO；\n操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。\nB+Tree 相比于 B 树和二叉树来说，最大的优势在于查询效率很高，因为即使在数据量很大的情况，查询一个数据的磁盘 I/O 依然维持在 3-4次。\n（三）磁盘预读特性\n为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。并且可以利用预读特性，相邻的节点也能够被预先载入。\n\n聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据 聚簇索引的叶子节点就是实际的数据行\n非聚簇索引：一个表可以有多个非聚簇索引，这些索引在逻辑上只是指向数据行的指针。 将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因\n\n区别就在于叶子节点存放的是什么数据：\n\n聚簇索引的叶子节点存放的是实际数据，所有完整的用户记录都存放在聚簇索引的叶子节点；\n二级索引的叶子节点存放的是主键值，而不是实际数据。\n\n如果某个查询语句使用了二级索引，但是查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获得数据行，这个过程就叫作「回表」，也就是说要查两个 B+ 树才能查到数据。不过，当查询的数据是主键值时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作「索引覆盖」，也就是只需要查一个 B+ 树就能找到数据。\nRedis\nRedis（Remote Dictionary Server )，即远程字典服务。C语言开发的一个开源的（遵从BSD协议）高性能键值对（key-value）的内存数据库，可以用作数据库、缓存、消息中间件等。它是一种NoSQL（not-only sql，泛指非关系型数据库）的数据库。\nRedis的基本概念和用法：\n\n高性能：Redis完全存储在内存中，这使得它能够以非常高的速度执行读写操作。\n多种数据结构：除了键值对，Redis还支持字符串、哈希表、列表、集合、有序集合等多种数据结构，允许更灵活的数据操作。\n持久化：Redis支持将数据持久化到硬盘，以便在重启后恢复数据。它提供了两种持久化方式：快照（Snapshotting）和AOF（Append-Only File）。\n发布/订阅：Redis提供了发布（publish）和订阅（subscribe）的功能，允许不同部分之间通过消息进行通信。\n事务：Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化，Redis 在事务失败时不进行回滚，而是继续执行余下的命令。Redis官方认为事务是原子性的：所有的命令，要么全部执行（但不一定成功），要么全部不执行。\n缓存：Redis常用于作为缓存层，将频繁访问的数据存储在内存中，以提高读取性能。\n计数器和排行榜：Redis适用于计数器和排行榜等需要快速处理增减操作的场景。\n分布式锁：Redis可以用来实现分布式锁，以确保在多个节点上的操作不会互相干扰。\n消息队列：使用Redis的列表数据结构，可以实现简单的消息队列，用于异步处理任务。\n\nRedis这么快？\n\n第一：Redis完全基于内存，绝大部分请求是纯粹的内存操作，非常迅速，数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度是O(1)。第二：数据结构简单，对数据操作也简单。第三：采用单线程，避免了不必要的上下文切换和竞争条件，不存在多线程导致的CPU切换，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有死锁问题导致的性能消耗。虽然 Redis 的主要工作（网络 I/O 和执行命令）一直是单线程模型，但是在 Redis 6.0 版本之后，也采用了多个 I/O 线程来处理网络请求，这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 I/O 的处理上。\n\n第四：使用多路复用IO模型，非阻塞IO。\nString、Hash、List、Set、SortedSet。\n\nHashes: Hashes are maps between string fields and string values. They are useful for representing objects with multiple fields, such as user profiles.\nString data up to 512 MB 。String类型是二进制安全的(其本质是将操作输入作为原始的、无任何特殊字符意义的数据流)，意思是 redis 的 string 可以包含任何数据。如数字，字符串，jpg图片或者序列化的对象。\nLists: 双向链表实现Lists are ordered collections of strings. You can add elements to the head (left) or tail (right) of a list. Lists can be used for task queues, message queues, and more.\nSets: Sets are unordered collections of unique strings. They are used for storing unique values. You can perform set operations like union, intersection, and difference.\nSorted Sets (ZSETs): Similar to sets, but each member has a score associated with it. Sorted sets are used for tasks that require ordering by score, such as leaderboards.\nBitmaps: Bitmaps are used for bit-level operations. They are often used to represent sets with a large number of unique items in a very memory-efficient way.\n\n\nRedis 的字符串表示为 sds(simple dynamic string) ，而不是 C 字符串（以 \\0 结尾的 char*）。\n对比 C 字符串， sds 有以下特性：\n\n可以高效地执行长度计算（strlen）；\n可以高效地执行追加操作（append）；\n二进制安全；\n\n\nsds 会为追加操作进行优化：加快追加操作的速度，并降低内存分配的次数，代价是多占用了一些内存，而且这些内存不会被主动释放。\n\nstruct sdshdr{\nint len;//buf数组中已经使用的字节的数量，也就是SDS字符串长度\nint  free;//buf数组中未使用的字节的数量\nchar buf[];//字节数组，字符串就保存在这里面\n};\nRedis需要对数据设置过期时间，并采用的是惰性删除+定期删除两种策略对过期键删除。Redis对过期键的策略+持久化\n\n数据过期清除策略：\n\n定期删除：Redis会定期（默认每隔100ms）随机抽取一些设置了过期时间TTL 的键，检查其是否过期，如果有过期的键就删除。这个策略确实是为了避免全局扫描而引入的，以减少CPU负载。\n惰性删除：当你尝试访问某个键时，Redis会检查该键是否已过期，如果已过期，则删除它。这个策略确保在获取键的时候处理过期数据，称为\"惰性\"，因为它不主动扫描键。\n\n\n内存淘汰策略：\n\nRedis是一个内存数据库，当内存用尽时，需要根据一定的策略来释放一些键以腾出内存空间。这个策略称为内存淘汰策略（Memory Eviction Policy）。\nRedis提供了不同的内存淘汰策略，常见的策略包括：\n\nLRU（Least Recently Used）：删除最近最少使用的键。\nLFU（Least Frequently Used）：删除最不频繁使用的键。\nRandom（随机淘汰）：随机选择一个键进行删除。\nTTL（Time To Live）：根据键的TTL来删除过期的键。\n\n\n你可以根据需要在Redis配置文件中选择适合你的内存淘汰策略，默认使用LRU策略。\n\n\n\n如果缓存数据设置的过期时间是相同的，并且Redis恰好将这部分数据全部删光了。这就会导致在这段时间内，这些缓存同时失效，全部请求到数据库中。\n跳表是可以实现二分查找的有序链表。\n\nMulti开始事务。\n命令入队。\nExec执行事务。\n\n持久化\nReids 是先执行写操作命令后，才将该命令记录到 AOF 日志里的，这么做其实有两个好处。\n\n避免额外的检查开销：因为如果先将写操作命令记录到 AOF 日志里，再执行该命令的话，如果当前的命令语法有问题，那么如果不进行命令语法检查，该错误的命令记录到 AOF 日志里后，Redis 在使用日志恢复数据时，就可能会出错。\n不会阻塞当前写操作命令的执行：因为当写操作命令执行成功后，才会将命令记录到 AOF 日志。\n\nRDB 快照：将某一时刻的内存数据，以二进制的方式写入磁盘\n所以，RDB 快照就是记录某一个瞬间的内存数据，记录的是实际数据   因此在 Redis 恢复数据时， RDB 恢复数据的效率会比 AOF 高些，因为直接将 RDB 文件读入内存就可以，不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据。\n\nRDB 做快照时会阻塞线程吗？\n\nRedis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，他们的区别就在于是否在「主线程」里执行：\n\n执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，会阻塞主线程；\n执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以避免主线程的阻塞；\n\nRedis 还可以通过配置文件的选项来实现每隔一段时间自动执行一次 bgsave 命令，默认会提供以下配置：\nsave 900 1 save 300 10 save 60 10000\n别看选项名叫 save，实际上执行的是 bgsave 命令，也就是会创建子进程来生成 RDB 快照文件。 只要满足上面条件的任意一个，就会执行 bgsave，它们的意思分别是：\n\n900 秒之内，对数据库进行了至少 1 次修改；\n300 秒之内，对数据库进行了至少 10 次修改；\n60 秒之内，对数据库进行了至少 10000 次修改。\n\n这里提一点，Redis 的快照是全量快照，也就是说每次执行快照，都是把内存中的「所有数据」都记录到磁盘中。所以执行快照是一个比较重的操作，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多。\nAOF 是一种持久化方式，它通过将 Redis 所有的写操作（包括写命令）以追加（append）的方式记录到一个文件中。AOF 文件包含了可以重放（replay）重建数据集的写命令序列，因此，它记录了数据库状态的完整变更历史。\n优点：\n可读性强：AOF 文件是一个文本文件，易于理解和查看。\n数据完整性：AOF 文件中包含了对数据的操作记录，可以用来恢复数据。\n灵活性：AOF 文件支持不同的同步策略，如每秒同步、每个命令同步等，提供了一定程度的灵活性。\n缺点：\n文件较大：由于记录了所有写操作，AOF 文件可能会比 RDB 文件更大。\n恢复速度相对较慢：在恢复时，可能会因为需要重新执行大量写操作而导致恢复速度较慢。\n缓存\n\n缓存雪崩：\n\nRedis挂掉了，请求全部走数据库。\n对缓存数据设置相同的过期时间，导致某段时间内缓存失效，请求全部走数据库。\n\n缓存雪崩如果发生了，很可能就把我们的数据库搞垮，导致整个服务瘫痪！\n如何解决缓存雪崩？\n\n解决方法：考虑用加锁或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上；还有一个解决方案，原有的失效时间基础上增加一个随机值，这样就会大幅度的减少缓存在同一时间过期。\n\n对于“Redis挂掉了，请求全部走数据库”这种情况，我们可以有以下的思路：\n\n事发前：实现Redis的高可用(主从架构+Sentinel 或者Redis Cluster)，尽量避免Redis挂掉这种情况发生。\n事发中：万一Redis真的挂了，我们可以设置本地缓存(ehcache)+限流(hystrix)，尽量避免我们的数据库被干掉(起码能保证我们的服务还是能正常工作的)\n事发后：redis持久化，重启后自动从磁盘上加载数据，快速恢复缓存数据。\n\n可以发现缓存击穿跟缓存雪崩很相似，如果缓存中的某个热点数据过期了，此时大量的请求直接访问数据库就是 缓存击穿\n应对缓存击穿可以采取前面说到两种方案：\n\n互斥锁方案，保证同一时间只有一个业务线程更新缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。\n不给热点数据设置过期时间，由后台异步更新缓存，或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间；\n\n缓存穿透\n是指查询一个一定不存在的数据。由于缓存不命中，并且出于容错考虑，如果从数据库查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，失去了缓存的意义。\n缓存穿透也有两种方案：\n\n由于请求的参数是不合法的(每次都请求不存在的参数)，于是我们可以使用布隆过滤器(BloomFilter)或者压缩filter提前拦截，不合法就不让这个请求到数据库层！\n当我们从数据库找不到的时候，我们也将这个空对象设置到缓存里边去。下次再请求的时候，就可以从缓存里边获取了。\n这种情况我们一般会将空对象设置一个较短的过期时间。\n\n缓存与数据库双写一致\n只要我们设置了键的过期时间，我们就能保证缓存和数据库的数据最终是一致的。因为只要缓存数据过期了，就会被删除。随后读的时候，因为缓存里没有，就可以查数据库的数据，然后将数据库查出来的数据写入到缓存中。\n除了设置过期时间，我们还需要做更多的措施来尽量避免数据库与缓存处于不一致的情况发生。分布式环境下非常容易出现缓存和数据库间数据一致性问题，针对这一点，如果项目对缓存的要求是强一致性的，那么就不要使用缓存。我们只能采取合适的策略来降低缓存和数据库间数据不一致的概率，而无法保证两者间的强一致性。合适的策略包括合适的缓存更新策略，更新数据库后及时更新缓存、缓存失败时增加重试机制。\n两种策略各自有优缺点：\n\n先删除缓存，再更新数据库\n在高并发下表现不如意，在原子性被破坏时表现优异\n先更新数据库，再删除缓存(Cache Aside Pattern设计模式)\n\n缓存更新的策略\n\nCache Aside（旁路缓存）策略；\nRead/Write Through（读穿 / 写穿）策略；\nWrite Back（写回）策略；\n\n主要分为两类 Cache-Aside 和 Cache-As-SoR。 SoR 即「System Of Record，记录系统」，表示数据库。 实际开发中，Redis 和 MySQL 的更新策略用的是 Cache Aside，另外两种策略主要应用在计算机系统里\n\nCache Aside 策略：应用程序直接与数据库和缓存交互，并负责维护缓存的一致性。这种策略简单易用，但是需要维护缓存和数据库的一致性，可能出现缓存穿透或缓存雪崩的问题，一般采用延迟双删来保证最终一致性\n\n\n查询：先查询缓存，如果缓存中没有，则查询数据库，并将结果写入缓存\n更新：先更新数据库，然后删除缓存或者更新缓存\n\nCache-As-SoR **可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的Cache。**数据库由缓存代理，缓存未命中时由缓存加载数据库数据然后应用从缓存读，写数据时更新完缓存后同步写数据库。应用只感知缓存而不感知数据库。\n\nRead/Write Through 策略：应用程序只和缓存交互，使用缓存与数据库交互\n\n\n查询：先查询缓存，如果缓存中没有，则缓存从数据库中加载数据，并写入缓存\n更新：先更新缓存，再由缓存同步更新数据库\n\n\nWrite Behind 策略：应用程序只和缓存交互。回写式，数据写入缓存即可返回，缓存内部会异步的去更新数据源，这样好处是写操作特别快，因为只需要更新缓存。并且缓存内部可以合并对相同数据项的多次更新，但是带来的问题就是数据不一致，可能发生写丢失。\nRefresh-Ahead 策略：应用程序只和缓存交互，由后台服务与数据库交互\n\n\n查询：只查询缓存\n更新：由后台服务自动从数据库中查询最新的数据，并将数据写入缓存中，\n\n性能\nCache Aside 的性能较高，它只在缓存未命中时才访问数据库\nRead/Write Through 的性能较低，它在每次读写时都需要访问数据库\nWrite Behind Caching 的性能最高，它只在缓存未命中时才访问数据库，而写入操作是异步的\nRefresh-Ahead 的性能介于 Cache Aside 和 Write Behind Caching 之间，它只在即将过期时才访问数据库，并且写入操作也是异步的\n数据一致性\nCache Aside 的数据一致性较低，它只在缓存未命中时才更新缓存，而写入操作则是直接更新数据库，并将缓存中的数据删除或更新\nRead/Write Through 的数据一致性最高，它在每次读写时都更新数据库和缓存\nWrite Behind Caching 的数据一致性最低，它只在缓存未命中时才更新缓存，而写入操作则是先更新缓存，并在异步更新数据库，有较大的延迟。\nRefresh-Ahead 的数据一致性介于 Read/Write Through 和 Cache Aside 之间，它保证了缓存中的数据总是最新的，但是有一定的延迟\nRedis 持久化策略，其实就是为了减少服务宕机后数据丢失，以及快速恢复数据，也算是支持高可用的一种实现。 除此之外，Redis 还提供了其它几种方式来保证**系统高可用，**业务中最常用的莫过于主从同步（也称作主从复制）、Sentinel 哨兵机制以及 Cluster 集群。\n主从复制\nRedis 同时支持主从复制和读写分离：一个 Redis 实例作为主节点 Master，负责写操作。其它实例（可能有 1 或多个）作为从节点 Slave，负责复制主节点的数据。\n主节点 Master 数据更新：Master 负责处理所有的写操作，包括写入、更新和删除等。 数据同步：写操作在 Master 上执行，然后 Master 将写操作的结果同步到所有从节点 Slave 上。 从节点 Slave 数据读取：Slave 负责处理读操作，例如获取数据、查询等。 数据同步：Slave 从 Master 复制数据，并在本地保存一份与主节点相同的数据副本。\n2.2 为什么要读写分离 1）防止并发2）易于扩展 我们都知道，大部分使用 Redis 的业务都是读多写少的。所以，我们可以根据业务量的规模来确定挂载几个从节点 Slave，当缓存数据增大时，我们可以很方便的扩展从节点的数量，实现弹性扩展。 同时，读写分离还可以实现数据备份和负载均衡，从而提高可靠性和性能。 3）高可用保障 不仅如此，Redis 还可以手动切换主从节点，来做故障隔离和恢复。\n主从复制过程\n\n从服务器连接主服务器，发送SYNC命令；主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令；主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令；从服务器收到快照文件后丢弃所有旧数据，载入收到的快照；主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令；从服务器完成对快照的载入，开始接受命令请求，并执行来自主服务器缓冲区的写命令；（从服务器初始化完成）主服务器每执行一个写命令就会向从服务器发送相同的写命令，从服务器接收并执行收到的写命令（从服务器初始化完成后的操作）\n一般主从配置可以缓解请求压力，做读写分离，写服务器不开启持久化，从服务器开启，从服务器还负责读取的操作，而且从服务器可以是多个，可以有效缓解主服务器的压力；但是坏处在于，如果主服务器宕机，无法自动切换恢复；\n\nbgsave 过程中，Redis 依然可以继续处理操作命令的，也就是数据是能被修改的，关键的技术就在于**写时复制技术（Copy-On-Write, COW）。**执行 bgsave 命令的时候，会通过 fork() 创建子进程\n写入时复制（英语：Copy-on-write，简称COW）是一种计算机程序设计领域的优化策略。其核心思想是，如果有多个调用者（callers）同时请求相同资源（如内存或磁盘上的数据存储），他们会共同获取相同的指针指向相同的资源，直到某个调用者试图修改资源的内容时，系统才会真正复制一份专用副本（private copy）给该调用者，而其他调用者所见到的最初的资源仍然保持不变。这过程对其他的调用者都是透明的。此作法主要的优点是如果调用者没有修改该资源，就不会有副本（private copy）被建立，因此多个调用者只是读取操作时可以共享同一份资源。\nRedis的持久化策略\n有两种：1、RDB：快照形式是直接把内存中的数据保存到一个dump的文件中，定时保存，保存策略。2、AOF：把所有的对Redis的服务器进行修改的命令都存到一个文件里，命令的集合。Redis默认是快照RDB的持久化方式。当Redis重启的时候，它会优先使用AOF文件来还原数据集，因为AOF文件保存的数据集通常比RDB文件所保存的数据集更完整。你甚至可以关闭持久化功能，让数据只在服务器运行时存。\n集群原理\n在Redis集群中，所有Redis节点彼此互联，节点内部使用二进制协议优化传输速度和带宽。当一个节点宕机后，集群中超过半数节点检测失效才认为该节点失效。不同于Tomcat集群需要反向代理服务器，Redis集群中任意节点都可以直接和Java客户端连接。Redis集群上的数据分配采用哈希槽。Redis集群中内置了16384个哈希槽，当有数据要存储时，Redis会首先使用CRC16算法对key进行计算，将计算结果对16384取余，这样每个key都会对应一个取值在16384之间的哈希槽。开发者可根据每个Redis实例性能来调整每个Redis实例上哈希槽的分布范围。\n查询优化\n数据库查询优化是指通过调整数据库查询语句、数据库结构和相关配置，以提高数据库查询性能和效率的过程。在大型应用中，数据库通常是性能瓶颈之一，因此优化查询可以显著提升整体应用性能。以下是一些常见的数据库查询优化技巧：\n\n选择合适的索引：索引是数据库中加速查询的关键。确保在经常查询的字段上创建适当的索引，但也不要过度创建索引，因为过多的索引可能会降低写操作的性能。\n优化查询语句：\n\n使用适当的条件：在查询中使用WHERE子句限制返回的行数，避免全表扫描。\n避免使用SELECT *：只选择需要的列，减少数据传输和处理成本。\n使用JOIN优化：合理使用INNER JOIN、LEFT JOIN等连接，确保查询返回所需的数据，避免笛卡尔积和不必要的数据拉取。\n\n\n分批处理：对于大量数据的查询，可以使用分页或者批量处理技术，避免一次性处理过多数据。\n避免子查询：尽量避免在查询中使用复杂的子查询，因为它们可能导致性能下降。可以考虑使用连接或者临时表来代替子查询。\n使用EXPLAIN：数据库提供了EXPLAIN语句，可以帮助你理解查询是如何执行的，以及是否使用了索引等。通过分析EXPLAIN的输出，可以发现潜在的性能问题并进行优化。\n定期维护和优化数据库：定期进行数据库的优化和维护操作，如重新组织表、更新统计信息等，以保持数据库的健康状态。\n缓存：使用缓存技术来存储常用查询的结果，减少数据库查询的次数。但需要注意及时更新缓存，以保证数据的一致性。\n硬件和服务器优化：数据库所运行的服务器的硬件性能也会影响查询性能。确保服务器的CPU、内存、存储等方面都能满足数据库的需求。\n数据库规范化与反规范化：在设计数据库结构时，要考虑数据的规范化程度。有时候，为了提高查询性能，可以考虑适当的反规范化。\n使用合适的数据库引擎：不同的数据库引擎有不同的优势和特点。根据应用的需求选择适合的数据库引擎，如MySQL、PostgreSQL、MongoDB等。\n\nDELETE FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH);\n\n分解大连接查询\n将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联，这样做的好处有：\n\n缓存之所以能够大幅提高系统的性能，关键在于数据的访问具有局部性，也就是二八定律：「百分之八十的数据访问是集中在 20% 的数据上」。这部分数据也被叫做热点数据。\n让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。\n分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。\n减少锁竞争；\n在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可伸缩。\n查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。\n聚簇索引：将数据和索引放到一起存储，索引结构的叶子节点保留了数据行。 非聚簇索引：将数据进和索引分开存储，索引叶子节点存储的是指向数据行的地址。\nCREATE INDEX ,ALTER TABLE\n最基本的分页方式：\nSELECT ... FROM ... WHERE ... ORDER BY ... LIMIT ...\n在中小数据量的情况下，这样的 SQL 足够用了，唯一需要注意的问题就是确保使用了索引。\n举例来说，如果实际 SQL 类似下面语句，那么在 category_id, id 两列上建立复合索引比较好。\nSELECT * FROM articles WHERE category_id = 123 ORDER BY id LIMIT 50, 10\n\nDBMS\n一般业务刚上线的时候，直接使用单机数据库就够了，但是随着用户量上来之后，系统就面临着大量的写操作和读操作，单机数据库处理能力有限，容易成为系统瓶颈。\n由于存在读写锁冲突，并且很多大型互联网业务往往读多写少，读操作会首先成为数据库瓶颈，我们希望消除读写锁冲突从而提升数据库整体的读写能力。\n那么就需要采用读写分离的数据库集群方式，一主多从，主库会同步数据到从库。写操作都到主库，读操作都去从库。\n读写分离的主要思想是将数据库的读和写操作分开处理，从而减轻主数据库的负担，提高整个系统的并发能力和性能。\n常见的读写分离架构包括：\n\n主从复制：通过在主数据库上进行写操作，并将写操作同步到多个从数据库（只读副本）。读操作则可以在从数据库上执行，分担了主数据库的读压力。\nSharding分片：按照某种规则（例如按照用户ID、地理位置等）将数据分散存储在不同的数据库节点上，每个节点负责一部分数据。这样可以水平扩展数据库系统，提高了系统整体的读写能力。\n\nMysql表格设计？\n平衡范式与冗余(效率优先；往往牺牲范式)拒绝3B(拒绝大sql语句：big sql、拒绝大事务：big transaction、拒绝大批量：big batch);\n第一范式（1NF）是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性。如果出现重复的属性，就可能需要定义一个新的实体，新的实体由重复的属性构成，新实体与原实体之间为一对多关系。在第一范式（1NF）中表的每一行只包含一个实例的信息。简而言之，第一范式就是无重复的列。\n说明：在任何一个关系数据库中，第一范式（1NF）是对关系模式的基本要求，不满足第一范式（1NF）的数据库就不是关系数据库。\n第一范式存在问题：冗余度大、会引起修改操作的不一致性、数据插入异常、数据删除异常。\n\n冗余度大：\n\n1NF要求每列都是原子性的，这可能导致数据的冗余。当数据重复存储在不同的表中，会增加存储空间，增加了数据不一致性的风险。\n\n\n修改操作的不一致性：\n\n如果某个信息在多个地方存储，当需要修改这个信息时，需要在所有存储位置进行同样的修改。如果有一个位置的信息修改了而其他位置未修改，数据就会不一致。\n\n\n数据插入异常：\n\n数据插入异常指的是由于表中的部分列被留空或无法插入新行，而导致无法插入需要的数据。在1NF中，如果表中的某列是必填项，而其他列不是，当插入新行时，可能会出现无法插入数据的情况。\n\n\n数据删除异常：\n\n当从表中删除数据时，可能会意外删除其他相关数据，从而造成数据丢失。这种情况在存在多个依赖于相同键的表时较为常见。如果删除某个主键相关的信息，可能会导致其他数据不完整或不一致。\n\n\n\n第二范式，强调记录的唯一性约束，数据表必须有一个主键，并且没有包含在主键中的列必须完全依赖于主键，而不能只依赖于主键的一部分。\n举例：\n学生信息（学号，身份证，姓名）学号-》姓名 ，学号，身份证-》姓名 所以姓名部分依赖于（学号，身份证）该表不是2NF。\n2.3 3NF 第三范式\n第三范式，强调数据属性冗余性的约束，也就是非主键列必须直接依赖于主键。也就是消除了非主属性对码的传递函数依赖的2NF。\nBCNF消除主键的某一列会依赖于主键的其他列\n4NF 第四范式\n非主属性不应该有多值。如果有多值就违反了第四范式。4NF是限制关系模式的属性间不允许有非平凡且非函数依赖的多值依赖。\n举例：用户联系方式表(用户id，固定电话，移动电话)，其中用户id是主键，这个满足了BCNF,但是一个用户有可能会有多个固定电话或者多个移动电话，那么这种设计就不合理，应该改为(用户id，联系方式类型，电话号码)。\n说明：如果只考虑函数依赖，关系模式规范化程度最高的范式是BCNF;如果考虑多值依赖则是4NF。\n先建立索引或者分区，然后再查询\n事务\n要保证交易正常可靠地进行，数据库就得解决上面的四个问题，这也就是事务诞生的背景\n事务是为了保证数据的一致性\n事务（transaction）指一组 SQL 语句；每一个MySQL语句都是相互依赖的。而整个单独单元作为一个不可分割的整体， 如果单元中某条SQL语句一旦执行失败或产生错误，整个单元将会回滚。 所有受到影响的数据将返回事务开始以前的状态;如果单元中的所有SQL语句均执行成功， 则事务被顺利执行。 保留点（savepoint）指事务处理中设置的临时占位符（placeholder），你可以对它发布回退（与回退整个事务处理不同）。\n创建\n隐式事务:\n比如insert、update、delete语句。\n显式事务:\n事务具有明显的开启和结束标记。前提: 必须先设置自动提交功能为禁用。set autocommit=0;\n事务四大特性，数据库特性\n\n\nA (Atomicity) 原子性：整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样\n\n\nC (Consistency) 一致性：在事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。A账户和B账户之间相互转账，无论如何操作，A、B账户的总金额都必须是不变的。数据库只有一个状态，不存在未确定状态\n\n\n隔离性（Isolation）：隔离性是当多个用户 并发的 访问数据库时，如果操作同一张表，数据库则为每一个用户都开启一个事务，且事务之间互不干扰，如果A在转账1亿给B（T1），同时C又在转账3亿给A（T2），不管T1和T2谁先执行完毕，最终结果必须是A账户增加2亿，而不是3亿，B增加1亿，C减少3亿。4个隔离级别\n持久性（Durability）：持久性就是指如果事务一旦被提交，数据库中数据的改变就是永久性的，即使断电或者宕机的情况下，也不会丢失提交的事务操作。\n\n\n\n只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并行执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 如何保证原子性\n作为接口的调用方，对于接口调用的结果，一般会返回成功、失败和超时。对于成功和失败，都是明确的状态，调用方可以根据结果做相应的处理，而超时则是未知状态，由于不确定是否成功请求了，作为调用方来说，所以一般都会选择重试。而重试就会出现定义中描述的多次执行。我们转账超时的时候，如果下游转账系统做好幂等控制，我们发起重试，那即可以保证转账正常进行，又可以保证不会多转一笔。\n可以从下面这个例子中加深一下理解：A账户向B账号汇钱\n使用BEGIN开启一个事务，使用COMMIT提交一个事务，这种事务被称为显式事务，例如，把上述的转账操作作为一个显式事务：\nbegin; -- 开始一个事务update table set A = A - 1亿; -- 伪sql，仅作示意update table set B = B + 1亿;-- 其他读写操作commit; -- 提交事务\n要保证上面操作的原子性， 就得等begin和commit之间的操作全部成功完成后，才将结果统一提交给数据库保存，如果途中任意一个操作失败，就撤销前面的操作，且操作不会提交数据库保存,这样就保证了同生共死。\nbegin; -- 开始一个事务update table set A = A - 1亿; -- 伪sql，仅作示意update table set B = B + 1亿;-- 其他读写操作commit; -- 提交事务\n要保证上面操作的原子性， 就得等begin和commit之间的操作全部成功完成后，才将结果统一提交给数据库保存，如果途中任意一个操作失败，就撤销前面的操作，且操作不会提交数据库保存,这样就保证了同生共死。\n如何保证隔离性\n原子性的问题解决了，但是如果有另外的事务在同时修改数据A怎么办呢？ 虽然可以保证事务的同生共死，但是数据一致性会被破坏。 此时需要引入数据的隔离机制，确保同时只能有一个事务在修改A，一个修改完了，另一个才来修改。 这需要对数据A加上互斥锁\n问题：ACID中哪个特性最重要？\nConsistency 一致性, AID都是为了保证C，但注意的是ACID里的一致性和CAP定理里的一致性不是一回事！这里的一致性是指系统从一个正确的状态,迁移到另一个正确的状态.什么叫正确的状态呢?就是当前的状态满足预定的约束就叫做正确的状态\n然而微服务架构下，每个微服务都有自己的数据库，导致微服务架构的系统不能简单地满足 ACID，我们就需要寻找微服务架构下的数据一致性解决方案。\n微服务架构的系统本身是一种分布式系统，而本文讨论的问题其实也就是分布式事务之数据一致性的问题，我们来聊聊分布式系统的 CAP 理论和 BASE 理论。\n如果不考虑事务的隔离性，会发生的几种问题：\n1，脏读\n脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据。\n当一个事务正在多次修改某个数据，而在这个事务中这多次的修改都还未提交，这时一个并发的事务来访问该数据，就会造成两个事务得到的数据不一致。例如：用户A向用户B转账100元，对应SQL命令如下\n`update account set money=money+**100** where name=’B’;  (此时A通知B)\n\nupdate account set money=money - **100** where name=’A’;`\n\n当只执行第一条SQL时，A通知B查看账户，B发现确实钱已到账（此时即发生了脏读），而之后无论第二条SQL是否执行，只要该事务不提交，则所有操作都将回滚，那么当B以后再次查看账户时就会发现钱其实并没有转。\n不可重复读和幻读到底有什么区别呢？\n(1) 不可重复读是读取了其他事务更改的数据，针对update操作\n解决：使用行级锁，锁定该行，事务A多次读取操作完成后才释放该锁，这个时候才允许其他事务更改刚才的数据。\n(2) 幻读是读取了其他事务新增的数据，针对insert和delete操作 （前后多次读取，数据总量不一致）\n解决：使用表级锁，锁定整张表，事务A多次读取数据总量之后才释放该锁，这个时候才允许其他事务新增数据。\n\n隔离级别\n并发环境中事务的隔离级别一共分为四种，安全性分别如下：\n\n序列化（SERIALIZABLE）：如果隔离级别为序列化，则用户之间通过一个接一个顺序地执行当前的事务，这种隔离级别提供了事务之间最大限度的隔离。\n可重复读（REPEATABLE READ）：这是MySQL的默认事务隔离级别，它确保**同一事务的多个实例在并发读取数据时，会看到同样的数据行。**不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。\n**提交读（READ COMMITTED）**这是大多数数据库系统的默认隔离级别（Oracle、PostgreSQL、SQL Server默认模式）。它满足了隔离的简单定义：**一个事务只能看见已经提交事务所做的改变。**这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。\n未提交读（READ UNCOMMITTED）处于这个隔离级的事务可以读到其他事务还没有提交的数据 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。\n\n\n一个事务A（txnId=100）修改了数据X，使得X=1，并且commit了\n另外一个事务B（txnId=101）开始尝试读取X，但是还X=1。但B没有提交。\n第三个事务C（txnId=102）修改了数据X，使得X=2。并且提交了\n事务B又一次读取了X。这时\n\n\n如果事务B是Read Committed。那么就读取X的最新commit的版本，也就是X=2\n如果事务B是Repeatable Read。那么读取的就是当前事务（txnId=101）之前X的最新版本，也就是X被txnId=100提交的版本，即X=1。\n\n注意，这里B不论是Read Committed，还是Repeatable Read，都不会被锁，都能立刻拿到结果。这也就是MVCC存在的意义。\n在基于MVCC的数据库实现中，根本就不需要出现Read Uncommitted这种情况。Read Uncommitted是早期数据库，读写都基于锁进行实现的产物。在实际业务中Read Uncommitted毫无意义\n当多个用户/进程/线程同时对数据库进行操作时，会出现3种冲突情形：\n\n读-读，不存在任何问题\n读-写，有隔离性问题，可能遇到脏读（会读到未提交的数据） ，幻影读等。\n写-写，可能丢失更新\n\n要解决冲突，一种办法是是锁，即基于锁的并发控制，比如2PL，这种方式开销比较高，而且无法避免死锁。\n多版本并发控制（MVCC）是一种用来解决读-写冲突的无锁并发控制，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 这样在读操作不用阻塞写操作，写操作不用阻塞读操作的同时，避免了脏读和不可重复读\n乐观并发控制（OCC）是一种用来解决写-写冲突的无锁并发控制，认为事务间争用没有那么多，所以先进行修改，在提交事务前，检查一下事务开始后，有没有新提交改变，如果没有就提交，如果有就放弃并重试。乐观并发控制类似自选锁。乐观并发控制适用于低数据争用，写冲突比较少的环境。\n多版本并发控制可以结合基于锁的并发控制来解决写-写冲突，即MVCC+2PL，也可以结合乐观并发控制来解决写-写冲突。\nMVCC\nMVCC（Multi-Version Concurrency Control，多版本并发控制）是数据库系统中一种常见的并发控制机制，用于处理多个事务同时对同一数据进行读写操作时的并发性。\nMVCC 的工作原理如下：\n\n版本控制： 每当对数据库进行写操作（如插入、更新或删除）时，系统会为每个操作创建一个新版本的数据，而不是直接覆盖原始数据。\n版本号： 每个数据行都有一个唯一的版本号或时间戳来标识。这些版本号可以表示数据的创建时间或事务的提交时间。\n读取操作： 当进行读取操作时，事务会根据其开始时间或事务 ID 查看可见的版本。这意味着事务只能看到在其开始之前提交的版本。这样，即使其他事务正在修改数据，读操作也不会受到影响，因为它们会查看之前已提交的版本。\n写操作： 对于写操作，事务会创建新版本的数据，并在提交时更新系统中的版本控制信息。其他正在进行的事务可以继续访问旧版本的数据，直到新版本的数据被完全提交。\n\nMySQL的InnoDB存储引擎实现MVCC的策略就是使用这种方法来提高读写事务控制的、他大大提高了读写事务的并发性能，原因是MVCC是一种不采用锁来控制事物的方式，是一种非堵塞\n一般解决幻读的方法是增加范围锁RangeS，锁定检索范围为只读，这样就避免了幻读。\nMVCC最大的优势：读不加锁，读写不冲突。读写不冲突是非常重要的，极大的增加了系统的并发性能。MVCC机制也是乐观锁的一种体现。\n分布式事务由于网络不可靠的问题\n二阶段锁（Two-Phase Locking，简称2PL）是一种并发控制机制，用于确保并发执行的事务不会导致数据不一致或丢失更新的问题。这种机制通过两个阶段来实现对共享资源的锁定和释放。\n1. 两个阶段\na. 加锁阶段（Growing Phase）\n在这个阶段，事务可以获取锁，但不能释放任何锁。事务可以根据需要动态地请求和获取锁，但一旦释放了锁，就不能再获取新的锁。这个阶段的目标是获取所有需要的锁。\nb. 解锁阶段（Shrinking Phase）\n在这个阶段，事务可以释放已经持有的锁，但不能再获取新的锁。一旦事务释放了一个锁，就不能再获取锁，这样可以确保事务不会在释放锁之后再对其他资源请求锁，从而防止死锁的发生。\n2. 特性\na. 死锁避免\n通过强制事务按照两个阶段的顺序获取和释放锁，可以有效地避免死锁的发生。在解锁阶段，事务不再获取新的锁，从而减少了死锁的可能性。\nb. 严格一致性\n二阶段锁保证了事务的严格一致性，即事务在提交之前持有的锁不会被释放，确保了事务在整个执行过程中对数据的一致性要求。\nc. 并发性\n尽管二阶段锁可以保证一致性，但它可能限制了系统的并发性。因为在加锁阶段，事务需要获取所有需要的锁，这可能导致一些事务等待其他事务释放锁，从而降低了系统的并发性能。\nDistributed System\n分布式事务是指涉及多个**独立参与者（如数据库、消息队列、服务等）**的事务操作。在分布式环境中，确保多个参与者之间的事务操作具有原子性、一致性、隔离性和持久性（ACID属性）是复杂的。\nZooKeeper（ZooKeeper分布式协调服务）是一个开源的分布式协调服务，提供了一个高性能、高可靠性的分布式应用协调和管理的平台。它主要用于解决分布式系统中的一些数据一致性共识问题，比如选举master、分布式锁、配置管理等。底层基于paxos改造，目前kafka,Hbase,Hadoop都通过zookeeper实现。\n\n协调服务：ZooKeeper 提供了分布式系统中的一致性和协同服务，包括统一命名服务、状态同步等。\n分布式锁：ZooKeeper 提供了基于临时节点的分布式锁，可以用于协调多个节点的访问。\n选举机制：ZooKeeper 提供了一些原语，可以用来实现选主机制，确保在集群中只有一个节点拥有特定的角色。\n\nZookeeper是集群部署，只要集群中超过半数节点存活，即可提供服务，例如一个由3个节点的Zookeeper，允许1个Zookeeper节点宕机，集群仍然能提供服务；一个由５个节点的Zookeeper，允许2个节点宕机。\n但Zookeeper的设计是CP模型，即要保证数据的强一致性，必然在可用性方面做出牺牲。\nZookeeper不同于raft只有leader follower\nleader向所有follower发送定期心跳，心跳返回不过半则导致leader退位。follower没有收到来自leader的心跳会导致进入looking状态\n\n分布式锁基本使用两个解决方案：\n（1）基于ZooKeeper的分布式锁，适用于高可靠（高可用）而并发量不是太大的场景；\n（2）基于Redis的分布式锁，适用于并发量很大、性能要求很高的、而可靠性问题可以通过其他方案去弥补的场景。\nZooKeeper 分布式锁的实现依赖于有序节点和事件监听机制，通过客户端创建有序节点、比较节点顺序和监听节点变化事件，确保在分布式环境下实现了简单而有效的分布式锁：\n创建临时顺序节点：客户端尝试在 ZooKeeper 中创建一个临时顺序节点，表示自己请求锁。\n获取所有顺序节点：客户端获取所有与锁相关的节点列表，并确定自己的节点序号。\n判断是否获得锁：客户端检查自己创建的节点是否为当前所有节点中最小的节点序号，如果是，则表示获得了锁。\n否则等待：如果客户端的节点不是当前序号最小的节点，客户端将监听其前一个节点的变化事件，并进入等待状态。\n释放锁：当持有锁的客户端完成操作后，会删除对应的节点，释放锁，此时监听该节点的其他等待锁的客户端会收到通知，重新检查节点序号以尝试获取锁。\nKafka 使用 ZooKeeper：\nApache Kafka 是一个分布式流处理平台，用于处理实时数据流。Kafka 使用 ZooKeeper 来进行多个方面的管理，包括：\nBroker 管理：ZooKeeper 负责跟踪 Kafka 集群中的所有 Broker 的元数据，比如 Broker 的状态、位置等信息。\nTopic 和 Partition 管理：ZooKeeper 存储了 Kafka Topic 和 Partition 的元数据信息，帮助 Kafka Broker 和 Consumer 定位和管理消息。\nLeader 选举：ZooKeeper 协助 Kafka 进行 Leader 的选举和管理，确保 Partition 在集群中的高可用性。\nHBase 使用 ZooKeeper：\nApache HBase 是一个分布式、面向列的 NoSQL 数据库，运行在 Hadoop 上。HBase 也使用 ZooKeeper 来进行诸多管理：\nRegionServer 管理：ZooKeeper 跟踪 HBase 中的 RegionServer 的状态和位置信息，协助 HBase 进行 Region 的负载均衡。\nMaster 和备份 Master 选举：ZooKeeper 用于 Master 节点的选举，并协助备份 Master 在主节点失效时接管管理。\nHadoop 使用 ZooKeeper：\nApache Hadoop 是一个用于分布式存储和处理大规模数据的框架。在 Hadoop 中，ZooKeeper 主要用于一些管理和协调任务，例如：\nNameNode 高可用：ZooKeeper 用于协助 Hadoop 实现 NameNode 的高可用性，确保在 NameNode 故障时可以快速切换到备用节点。\n作业协调：ZooKeeper 在 Hadoop 集群中用于作业协调，比如记录作业的状态、分配任务等。\n在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者的组件来统一掌控所有节点（称作参与者）的操作结果并最终指示这些节点是否要把操作结果进行真正的提交（比如将更新后的数据写入磁盘等等）。因此，二阶段提交的算法思路可以概括为： 参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。\n2PC 的过程分为两个阶段：\n\n投票阶段（Voting Phase）：\n\n协调者（Coordinator）向所有参与者发送一个请求以确认是否可以提交事务。\n参与者在确认自身可以提交事务的情况下，返回“同意”消息，否则返回“中止”消息。\n\n\n提交/中止阶段（Commit/Abort Phase）：\n\n如果所有参与者都返回“同意”，协调者发送一个全局提交消息。\n如果有任何参与者返回“中止”，协调者发送一个全局回滚消息。\n\n\n\n2PC 的优点是可以确保分布式事务的一致性，使得所有参与者要么都提交要么都回滚。然而，它也有一些缺点，包括：\n\n阻塞：在第一个阶段，协调者会阻塞等待所有参与者的回应，这可能导致整个系统的阻塞。\n单点故障：协调者成为了单点故障，一旦协调者出现问题，整个协议可能失败。\n\n由于这些限制，一些分布式系统可能会选择其他协议来解决分布式事务的问题，以减少阻塞、提高性能和降低单点故障的影响。\nPaxos与分布式事务有相似之处，但并不相同。而在分布式数据库中，分布式事务是用来保证跨节点事务原子性的，而Paxos协议则是保证数据多副本一致性的。\n这里要理解到，即使没有多副本，分布式事务一样是需要的；即使没有分布式事务，多副本一致性也是需要的。因此这两者有关联，但各自处理场景不同。\n而Paxos在分布式数据库中多副本一致性，保证的到底是什么呢？大多数人会认为主要是保证有多数副本节点写入成功，而实际上，更重要的，其实是为了保证大量并发写入，在各个节点上的写入顺序是一致的，即必须严格保证各个副本节点，关于并发操作的写入顺序一致，这一点是需要注意的。\n在分布式系统中，缓存数据一致性是一个重要的问题，因为不同的缓存节点可能会存储相同的数据副本，而当数据更新时，需要确保所有缓存节点上的数据保持一致。以下是一些常见的方法和策略，用于解决缓存数据一致性问题：\n\n缓存失效策略：\n当数据发生变化时，及时使缓存失效。这样，下次请求到达时，缓存将会被更新。这种方法简单有效，但可能导致\"缓存雪崩\"问题，即大量失效缓存导致数据库压力过大。\n缓存更新策略：\n当数据更新时，不仅使缓存失效，而是先更新数据库，然后更新缓存。这种方式可以避免缓存雪崩，但可能会带来一些延迟。\n写-through 缓存：\n当数据写入数据库时，同时更新缓存。这确保了缓存和数据库的一致性，但也可能导致写操作的延迟。\n写-behind 缓存（也称为异步写缓存）：\n当数据写入数据库时，首先更新缓存。然后，异步地将数据写入数据库。这可以提高写操作的性能，但可能会带来数据不一致的风险。\n分布式缓存：\n使用分布式缓存系统，如 Redis 或 Memcached，它们具有内置的机制来处理数据一致性。这些系统通常提供了复制、分片和一致性哈希等特性，以确保数据在缓存集群中保持一致。\n版本控制：\n在数据中引入版本控制，每次更新都伴随着版本号的增加。缓存节点可以存储数据的版本号，以便在比较版本号时检查数据是否一致。\n发布/订阅模式：\n使用发布/订阅模式，当数据更新时，发布一个消息通知所有缓存节点更新数据。这种方式可以确保所有节点在数据更新时保持同步。\n一致性哈希：\n一致性哈希算法可以用于在缓存集群中确定每个数据项应该存储在哪个节点上。这有助于在缓存节点发生变化时，最小化数据的迁移。\n\n单点故障( SPOF ) 是系统的一部分，如果发生故障，整个系统将停止工作\nSystems can be made robust by adding redundancy in all potential SPOFs. Redundancy can be achieved at various levels.\nThe assessment of a potential SPOF involves identifying the critical components of a complex system that would provoke a total systems failure in case of malfunction. Highly reliable systems should not rely on any such individual component.\n设计一个避免单点故障的分布式系统：\n\n冗余节点：\n引入冗余节点是一种常见的方法。将系统的功能分布到多个节点上，每个节点都能独立处理请求。如果一个节点发生故障，其他节点可以继续处理请求。这可以通过负载均衡来实现，确保请求被平均分配到各个节点上。\n主从备份：\n对于存储数据的场景，使用主从备份架构可以帮助避免单点故障。主节点负责处理写操作，从节点负责复制主节点的数据，并提供读操作。如果主节点发生故障，可以将一个从节点提升为新的主节点。\n集群化架构：\n将系统划分为多个相互独立的集群，每个集群都有自己的节点和资源。这种架构可以减少单个集群发生故障对整体系统的影响。同时，通过使用跨集群的通信机制来实现跨集群的协调。\n使用分布式存储：\n分布式存储系统可以确保数据分布在多个节点上，从而避免单点存储故障。一些分布式存储技术如HDFS（Hadoop Distributed File System）、Ceph等可以提供高度可靠的数据存储。\n自动扩展和缩放：\n采用自动扩展和缩放机制可以根据负载情况动态调整节点数量。当负载增加时，系统可以自动添加新节点来分担负载，当负载减少时，可以自动缩减节点数量。\n监控和警报系统：\n设置监控和警报系统来实时监测系统的状态。一旦发现故障或异常，系统可以自动触发警报，从而能够迅速采取行动来处理问题。\n\nCAP原则又称CAP定理，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可兼得。\n\n一致性（Consistency）：一致性要求分布式系统的所有节点在同一时刻对于相同的数据有相同的视图。换句话说，如果一个写操作成功完成后，所有后续的读操作都应该返回最新的写入结果。\n可用性（Availability）：可用性要求分布式系统在有请求时能够提供响应，即系统保持活跃状态，不会出现无响应的情况。可用性意味着在任何时刻，系统都至少能够响应一部分请求，即使某些节点或组件失败。\n分区容忍性（Partition Tolerance）：分区容忍性是指分布式系统能够在节点之间发生通信故障或网络分区的情况下继续正常运行。网络分区是指系统中的节点被分隔成多个不能相互通信的子集。\n\n\n以电商网站为例，会员登录、个人设置、个人订单、购物车、搜索用AP，因为这些数据短时间内不一致不影响使用；后台的商品管理就需要CP，避免商品数量的不一致；支付功能需要CA，保证支付功能的安全稳定\n\n提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。\n然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。\n总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。\nBASE理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。\n\nBasically Available（基本可用） 响应时间上的损失：正常情况下，处理用户请求需要0.5s返回结果，但是由于系统出现故障，处理用户请求的时间变成3s。 系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面\n软状态 软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时\n最终一致性 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。\n\n\n分布式： 一个业务分拆多个子业务，部署在不同的服务器上\n集群： 同一个业务，部署在多个服务器上。比如之前做电商网站搭的redis集群以及solr集群都是属于将redis服务器提供的缓存服务以及solr服务器提供的搜索服务部署在多个服务器上以提高系统性能、并发量解决海量存储问题。\n\nThe consensus problem requires agreement among a number of processes (or agents) for a single data value. Some of the processes (agents) may fail or be unreliable in other ways, so consensus protocols must be fault tolerant or resilient. The processes must somehow put forth their candidate values, communicate with one another, and agree on a single consensus value.\nThe consensus problem is a fundamental problem in control of multi-agent systems. One approach to generating consensus is for all processes (agents) to agree on a majority value. In this context, a majority requires at least one more than half of available votes (where each process is given a vote). However, one or more faulty processes may skew the resultant outcome such that consensus may not be reached or reached incorrectly.\n解决共识问题的协议旨在处理数量有限的错误进程Protocols that solve consensus problems are designed to deal with limited numbers of faulty processes. These protocols must satisfy a number of requirements to be useful. For instance, a trivial protocol could have all processes output binary value 1. This is not useful and thus the requirement is modified such that the output must somehow depend on the input. That is, the output value of a consensus protocol must be the input value of some process. Another requirement is that a process may decide upon an output value only once and this decision is irrevocable. A process is called correct in an execution if it does not experience a failure. A consensus protocol tolerating halting failures must satisfy the following properties.[1]\nOLTP vs OLAP\n\nMapReduce\n1TB的数据，如何统计单词数？如何建立倒排索引？这其实是很复杂的过程，如果有10TB，或者1PB数据怎么办呢？这时候单机就没办法做，变得很复杂需要想办法来解决。很多人说这些可以自己单机写，单机写的过程甚至构建了一个MapReduce的系统\n是面向大数据并行处理的计算模型、框架和平台，它隐含了以下三层含义：\n1）MapReduce是一个基于集群的高性能并行计算平台（Cluster Infrastructure）。它允许用市场上普通的商用服务器构成一个包含数十、数百至数千个节点的分布和并行计算集群。\n2）MapReduce是一种编程模型和相关实现，用于在集群上使用并行分布式算法处理和生成大数据集将数据分布存储、数据通信、容错处理等并行计算涉及到的很多系统底层的复杂细节交由系统负责处理，大大减少了软件开发人员的负担。\n3）MapReduce是一个并行程序设计模型与方法（Programming Model &amp; Methodology）。它借助于函数式程序设计语言Lisp的设计思想，提供了一种简便的并行程序设计方法，用Map和Reduce两个函数编程实现基本的并行计算任务，提供了抽象的操作和并行编程接口，以简单方便地完成大规模数据的编程和计算处理  A MapReduce program is composed of a map\n procedure, which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce\n method, which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The “MapReduce System” (also called “infrastructure” or “framework”) orchestrates the processing by marshalling\n the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy\n and fault tolerance\n.\nHadoop实现了一个分布式文件系统（ Distributed File System），其中一个组件是HDFS（Hadoop Distributed File System）。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算\n信息检索\n正向索引: 当用户发起查询时（假设查询为一个关键词），搜索引擎会扫描索引库中的所有文档，找出所有包含关键词的文档，这样依次从文档中去查找是否含有关键词的方法叫做正向索引。增加效率，搜索引擎会把正向索引变为反向索引（倒排索引）即把“文档→单词”的形式变为“单词→文档”的形式，分出来的词集合称为Term Dictionary。\n倒排索引具体机构如下: 单词1→文档1的ID；文档2的ID；文档3的ID…\n倒排索引(Inverted Index)：倒排索引是实现**“单词-文档矩阵”**的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”。\n单词词典(Lexicon)：搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向“倒排列表”的指针。 倒排列表(PostingList)：倒排列表记载了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息，每条记录称为一个倒排项(Posting)。根据倒排列表，即可获知哪些文档包含某个单词。 倒排文件(Inverted File)：所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件即被称之为倒排文件，倒排文件是存储倒排索引的物理文件。\nTF(term frequency): 单词在文档中出现的次数。 Pos: 单词在文档中出现的位置。\n这个表格展示了更加复杂的倒排索引，前两列不变，第三列倒排索引包含的信息为(文档ID，单词频次，&lt;单词位置&gt;)，比如单词“乔布斯”对应的倒排索引里的第一项(1;1;&lt;1&gt;)意思是，文档1包含了“乔布斯”，并且在这个文档中只出现了1次，位置在第一个。\nMQ（Message Queue）\n是基础数据结构中“先进先出”的一种数据结构。一般用来解决应用解耦，异步消息，流量削峰等问题，实现高性能，高可用，可伸缩和最终一致性架构\n缺点：\n系统复杂性\n本来蛮简单的一个系统，我代码随便写都没事，现在你凭空接入一个中间件在那，我是不是要考虑去维护他，而且使用的过程中是不是要考虑各种问题，比如消息重复消费、消息丢失、消息的顺序消费等等，反正用了之后就是贼烦。\n数据一致性\n这个其实是分布式服务本身就存在的一个问题，不仅仅是消息队列的问题，但是放在这里说是因为用了消息队列这个问题会暴露得比较严重一点。\n可用性\n你搞个系统本身没啥问题，你现在突然接入一个中间件在那放着，万一挂了怎么办？我下个单MQ挂了，优惠券不扣了，积分不减了，这不是杀一个程序员能搞定的吧，感觉得杀一片。\n消息队列中间件\n消息队列的核心思想就是把同步的操作变成异步处理\n在实际应用中包括如下四个场景：\n\n应用耦合：多应用间通过消息队列对同一消息进行处理，避免调用接口失败导致整个过程失败；\n异步处理：多应用对消息队列中同一消息进行处理，应用间并发处理消息，相比串行处理，减少处理时间；\n限流削峰：广泛应用于秒杀或抢购活动中，避免流量过大导致应用系统挂掉的情况；\n消息驱动的系统：系统分为消息队列、消息生产者、消息消费者，生产者负责产生消息，消费者(可能有多个)负责对消息进行处理；发布/订阅模式下包括三个角色：角色主题（Topic）发布者(Publisher)订阅者(Subscriber)\n\n目前在市面上比较主流的消息队列中间件主要有，Kafka、ActiveMQ、RabbitMQ、RocketMQ 等这几种。\nElasticsearch\nElasticsearch 是一个分布式、可扩展、实时的搜索与数据分析引擎。 它能从项目一开始就赋予你的数据以搜索、分析和探索的能力，可用于实现全文搜索和实时数据统计。\n微服务\n熔断、隔离、降级\n服务隔离、降级和熔断的产生背景 tomcat底层都会共享一个线程池（自己创建的例外），当某个方法(服务)访问非常慢造成响应延迟，会造成大多数线程阻塞，导致整个线程池被占用甚至拖垮。 线程名定义：线程池名称+线程ID\n2.服务隔离解决思路 2.1 线程池隔离\n不同的http服务使用不同的线程池，当自己的资源用完，直接返回失败而不是占用别人的资源 优点：可提高并发性 缺点：增加CPU调度开销 使用场景：第三方应用或接口；并发量大\n2.2 信号量隔离\n原子计数器方式记录当前运行的线程数，超过则拒绝，不超过则+1，返回则-1 使用场景：内部应用或中间件；并发需求不大\n区别：信号量可动态调整，但线程池不可以调整\n3.服务降级 当服务不可用（服务正在等待、链接超时、网络延迟、服务器响应慢等），客户端一直等待时，调用fallback方法给客户端返回一个错误提示，不让客户端继续等待。 目的：提高用户体验，防止雪崩效应。\n4.服务熔断 熔断和保险丝一样，当访问请求过多的时候，达到一个阈值（自己设置）就直接拒绝访问，可以保护当前服务，让服务不会被挂掉，需要和服务降级一起使用。\nRPC\nRPC 的全称是 Remote Procedure Call 是一种进程间通信方式。它允许程序调用另一个地址空间(通常是共享网络的另一台机器上)的过程或函数，而不用程序员显式编码这个远程调用的细节。即无论是调用本地接口/服务的还是远程的接口/服务，本质上编写的调用代码基本相同。\nRPC 是一个请求响应模型 会隐藏底层的通讯细节(不需要直接处理Socket通讯或Http通讯)。客户端发起请求，服务器返回响应(类似于Http的工作方式).RPC 在使用形式,最初目的是像调用本地函数(或方法)一样去调用远程的函数(或方法)。\n除 RPC 之外，常见的多系统数据交互方案还有分布式消息队列、HTTP 请求调用、数据库和分布式缓存等。 其中 RPC 和 HTTP 调用是没有经过中间件的，它们是端到端系统的直接数据交互。\n在RPC框架中主要有三个角色：提供者、消费者和注册中心。如下图所示：\n!https://img-blog.csdnimg.cn/3236c96f76d343e4811384df4d06f2d1.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5MzkwNTQ1,size_16,color_FFFFFF,t_70#pic_center\n在这里插入图片描述\n\n提供者: 暴露服务的服务提供方。\n调用者: 调用远程服务的服务消费方。\n注册中心: 服务注册与发现的注册中心。\n\n4步：建立通信，服务寻址，网络传输，服务调用\n3、网络传输\n3.1、序列化\n客户端怎么把参数值传给远程的函数呢？在本地调用中，我们只需要把参数压到栈里，然后让函数自己去栈里读就行。但是在远程过程调用时，客户端跟服务端是不同的进程，不能通过内存来传递参数。甚至有时候客户端和服务端使用的都不是同一种语言（比如服务端用C++，客户端用Java或者Python）。这时候就需要客户端把参数先转成一个字节流，传给服务端后，再把字节流转成自己能读取的格式。这个过程叫序列化和反序列化。同理，从服务端返回的值也需要序列化反序列化的过程。\n当A机器上的应用发起一个RPC调用时，调用方法和其入参等信息需要通过底层的网络协议如TCP传输到B机器，由于网络协议是基于二进制的，所有我们传输的参数数据都需要先进行序列化（Serialize）或者编组（marshal）成二进制的形式才能在网络中进行传输。然后通过寻址操作和网络传输将序列化或者编组之后的二进制数据发送给B机器。\n3.2、反序列化 B接收到请求后，二进制信息恢复为内存中的表达方式\n网络传输。\n远程调用往往用在网络上，客户端和服务端是通过网络连接的。所有的数据都需要通过网络传输，因此就需要有一个网络传输层。网络传输层需要把Call ID和序列化后的参数字节流传给服务端，然后再把序列化后的调用结果传回客户端。只要能完成这两者的，都可以作为传输层使用。因此，它所使用的协议其实是不限的，能完成传输就行。尽管大部分RPC框架都使用TCP协议，但其实UDP也可以，而gRPC干脆\n通信方式\n分布式运算8宗罪\n\\1. The network is reliable —— 网络是可靠的。 \\2. Latency is zero —— 延迟是不存在的。 \\3. Bandwidth is infinite —— 带宽是无限的。 \\4. The network is secure —— 网络是安全的。 \\5. Topology doesn’t change —— 拓扑结构是一成不变的。 \\6. There is one administrator —— 总会有一个管理员。 \\7. Transport cost is zero —— 不必考虑传输成本。 \\8. The network is homogeneous —— 网络是同质化的\nRPC发展为RMI（Sun/Oracle）、Thrift（Facebook/Apache）、Dubbo（阿里巴巴/Apache）、gRPC（Google）、Motan1/2（新浪）、Finagle（Twitter）、brpc（百度/Apache）、.NET Remoting（微软）、Arvo（Hadoop）、JSON-RPC 2.0 （公开规范，JSON-RPC 工作组）……等等难以穷举的协议和框架。最近几年，RPC 框架有明显的朝着更高层次（不仅仅负责调用远程服务，还管理远程 服务）与插件化方向发展的趋势，不再追求独立地解决 RPC 的全部三个问题（表示数据、 传递数据、表示方法）\n并发\n虽然一些编程语言的框架在不断地提高多核资源使用效率，例如 Java 的 Netty 等，但仍然需要开发人员花费大量的时间和精力搞懂这些框架的运行原理后才能熟练掌握。\n作为程序员，要开发出能充分利用硬件资源的应用程序是一件很难的事情。现代计算机都拥有多个核，但是大部分编程语言都没有有效的工具让程序可以轻易利用这些资源。编程时需要写大量的线程同步代码来利用多个核，很容易导致错误。\nGo语言正是在多核和网络化的时代背景下诞生的原生支持并发的编程语言。Go语言从底层原生支持并发，无须第三方库，开发人员可以很轻松地在编写程序时决定怎么使用 CPU 资源。\nGo语言的并发是基于 goroutine 的，goroutine 类似于线程，但并非线程。可以将 goroutine 理解为一种虚拟线程。Go语言运行时会参与调度 goroutine，并将 goroutine 合理地分配到每个 CPU 中，最大限度地使用 CPU 性能。\n多个 goroutine 中，Go语言使用通道（channel）进行通信，通道是一种内置的数据结构，可以让用户在不同的 goroutine 之间同步发送具有类型的消息。这让编程模型更倾向于在 goroutine 之间发送消息，而不是让多个 goroutine 争夺同一个数据的使用权。\n","categories":["计算机工程"],"tags":["计算机基础","编程","web","计算机网络","网络安全"]},{"title":"日本麻将（日麻）策略分析","url":"/2024/11/07/Riichi/","content":"日本麻将（日麻）策略与基本规则\n基本规则\n日本麻将是一种四人游戏，使用136张牌。每位玩家开始时有13张牌，目标是通过摸牌和打牌来完成一个有效的和牌组合。\n日本麻将（日麻）在规则和策略上与中国麻将有一些差异。以下是一些主要的日麻策略，以帮助你在游戏中做出更好的判断：\n\n基本策略：\n追求速度与效率：日麻中的目标通常是尽快听牌（进入只差一张就能和牌的状态）。为了达到这个目标，你需要根据听牌的速度调整手牌，尽量去做最有效率的组合，比如三面听、两面听，避免做边张和嵌张等较难摸到的听口。\n优先两面搭子：两面搭子（例：34等可以和2或5完成顺子）在游戏中最为常见，也是最容易形成有效听牌形态的，因此在整理手牌时，优先保留两面搭子，提高听牌概率。\n役的意识：\n追求基本役：在日麻中，至少要有一个“役”才能胡牌，因此你在整理手牌时，需要始终考虑如何达成至少一个“役”。常见的基本役包括立直（リーチ）、断幺九（タンヤオ）、平和（ピンフ）等。立直是最简单且最稳定的役之一，只要手牌接近听牌且达到门清（没有吃、碰、杠），可以选择立直来增加胜率。\n利用役牌：如果手上有较多的场风牌、门风牌或三元牌（白、发、红中），尽量保留，形成“役牌”组合（役牌雀头），这是稳定得分的手段。\n判断手牌的价值：\n考虑点数：在日麻中，不仅仅是胡牌，点数（和牌的价值）也是非常重要的。在决定是否吃、碰、杠或立直时，需要根据当前手牌的情况评估点数。例如，只有立直+门清平和可能得点较低，如果有机会组合出高分役（如一气通贯、混一色、对对和等），可以适当放弃一些较快的听牌路线。\n追求满贯及以上的役：当起手较好时，可以尝试追求大役（例如清一色、混全带幺九、七对子等）。这时，需要稳住心态，不急于碰牌或立直，减少暴露手牌的机会，增加隐蔽性。\n防守策略：\n学会观察舍牌：观察对手的出牌，尤其是在对手立直的情况下，尽量不要轻易打出他们可能需要的牌。例如，如果对手舍出字牌（白、发、中等），通常表示这些字牌不在他们的听牌或役牌之中，这些字牌往往是安全牌。\n牌河判断：观察对手的舍牌序列，特别注意其连续打出的牌，寻找其可能听的牌的线索。例如，对手突然打出一张生张（没人打过的牌），可能表明他们已经听牌，尤其是对一张牌的犹豫，往往能说明手牌状态。\n灵活变通：\n根据场况调整策略：如果你在比赛的前半段得分领先，可以偏向防守，避免打出危险牌，放弃高风险高回报的打法。相反，如果你的点数较少甚至面临危险，需要积极追求高分和牌，尽量打破对手的节奏。\n熟悉流局规则：了解流局时听牌的奖励，以及四家立直、四家碰、九种九牌等特殊情况，并利用这些规则在特定场合中止对手的攻势或为自己争取时间。\n\n经验之谈\n给日麻新手的建议\n\n东风战，以和牌率为优先；东南战(半庄战)，以做大牌为准则。\n千万、千万不要随便杠牌，尤其是大明杠。\n\n杠牌最大的意义是翻悬赏牌，每个人的手牌机率都是平等的，在1vs3的游戏中宝牌在敌人手上的机率高达75%、在自己手上的机率仅有25%，期望值极低。\n所以加杠基本上也只有一个情况（但比大明杠时机常见很多）：你的手牌牌型较好听牌（或一向听好牌），且其他家还没有威胁性（没有立直或多个副露）\n如果手牌中没有明确的役（例如没有成对的三元牌、风牌），杠牌可能只能带来额外的杠分，而无法大幅提高手牌的得点。这种情况下，与其选择杠牌，不如尽量保持手牌的灵活性，等待形成更高得点的牌型。\n\n确定自己有役，才考虑鸣牌(吃、碰、大明杠)\n\n\n开局的单张自风、场风和三元牌通常可以保留几巡，以增加形成役牌的机会。\n若是非自风、非场风的风牌，或牌面结构明确追求速度时，可以早些舍出这些牌，增加摸牌效率。\n在速攻策略下，为减少无用牌的阻碍，风牌和三元牌也可以在确定无用时提前舍出。\n\n4o\n可以先记得「什么时候不该鸣牌」，等以上几课的能力都熟悉之后再深入学习。\n早巡不鸣。牌越烂越不该鸣。牌越小越不该鸣。\n在4巡前，除非你有确定3-4翻以上的大牌，否则不该鸣牌。因为越早巡牌通常越烂，这时候自摸牌成为有效进张、让向听数前进的机会也很高。摸了几手之后手牌稍微定型，对于手牌的打点与速度判断才会更加地准确。\n牌越烂指的是向听数越多的意思。在3向听以上的手牌，除非这个鸣牌能大幅度的加速手牌的进展，否则应该忍住。因为烂的牌鸣牌之后也只是变得没那么烂，往往速度上还是会输给牌形较好的玩家。若为了速度去过度鸣牌，在对手立直对攻时防守力不足就相当吃亏。\n大幅落后时不鸣。\n因为落后时需要高得点来逆转，而鸣牌会让手牌分数无法再提高。所以除非手牌分数足够大，否则都该忍住去拼立直。\n「该鸣牌的时机」\n后半场(例如东风场的3-4局、南风场的南2-南4)，自己大幅领先，且鸣牌确定有役又能让手牌向听数前进的时候。就是速攻不给别人逆转机会的概念，不过还是要注意鸣牌之后的手牌牌形/向听数不能太差。毕竟速攻总得确保自己有「速」对吧\n倒数三巡，安全牌足够多且有听牌机会时。\n因为流局有不听罚符，所以可以在确信不会放铳时，鸣牌追求形式听牌。\n基本上注意以上三点有关于振听的部分，我在细项说明之后也会提及并解释一下其原理\n以下分项说明：\n\n东风战，以和牌率为优先；东南战(半庄战)，以做大牌为准则。\n\n基本策略\n\n**听牌速度：**尽快形成听牌（听牌指离和牌只差一张牌的状态）。\n**防守意识：**注意其他玩家的打牌，避免放铳（给对手和牌）。\n**役种意识：**了解并争取高分值的役种（特定的和牌组合）。\n**牌效率：**保持手牌的灵活性，避免过早固定牌型。\n\n重要概念\n\n**立直：**宣告听牌并锁定手牌，可以增加得分。\n**副露：**通过吃、碰、杠来快速完成牌型，但会限制可能的和牌组合。\n**宝牌：**特定的增加得分的牌。\n\n进阶技巧\n常用日麻AI： https://mjai.ekyu.moe/zh-cn.html， https://github.com/Equim-chan/Mortal\n牌效率：https://euophrys.itch.io/mahjong-efficiency-trainer?fbclid=IwAR3w3WZnfmSrx4_Zi0kb38jSmWgSFRyZbAMeDQD9rZFFF4Asw1oz63n4gi0\n\n**读牌：**根据其他玩家的打牌推测他们的手牌。\n**河底读牌：**分析已经打出的牌来判断安全牌。\n**场势判断：**根据当前局势决定是否应该攻击或防守。\n\n记住，麻将是一种需要长期练习和经验积累的游戏。持续学习和实践是提高技能的关键。\n","categories":["随谈"],"tags":["概率论","组合数学","决策理论"]},{"title":"我们是被怎么塑造的？","url":"/2025/03/11/Whoarewe/","content":"Lately, I’ve been reflecting on how deeply our upbringing, culture, and self-perception influence our present. Many of our struggles—perfectionism, anxiety, or even feeling out of place—can often be traced back to these invisible forces shaping us from childhood.\n我们登上的并非我们所选择的舞台，演出并非我们所选择的剧本.We don’t choose our starting point. Parents, school, and society hand us a blueprint for how to think, behave, and define success. Some people grow up with constant pressure to achieve, while others are raised in environments that encourage self-expression and risk-taking. These early experiences shape our core beliefs—ideas we hold about ourselves and the world, often without questioning them.\nIf you grew up in an environment where failure was unacceptable, you might internalize perfectionism.\nIf you were constantly compared to others, self-worth might feel conditional—only valid when you achieve something.\nIf your culture emphasized collectivism over individualism, personal desires might feel selfish, making decisions harder.\nAsian cultures (and many others with strong traditional values) often emphasize duty, hierarchy, and stability over self-expression. This is a double-edged sword:\n✅ It fosters discipline, responsibility, and strong family bonds.\n❌ It can also create rigid expectations, where success is narrowly defined (grades, career, financial stability).\nIn contrast, Western cultures—especially in English-speaking and Protestant European countries—tend to prioritize self-expression, independence, and questioning authority. Neither is inherently better, but conflicts arise when someone raised in a more traditional setting starts questioning these values.\nFor example:\nWanting to change careers or take risks might feel like betraying expectations.\nPrioritizing mental well-being over external success might seem selfish.\nExpressing emotions openly might feel unnatural or weak.\nThe good news? Your beliefs are not fixed. Cognitive Behavioral Therapy (CBT) teaches that thoughts create emotions, and emotions shape behavior. By questioning negative core beliefs, you can reframe your self-perception and break free from limiting patterns.\nFor example:\nInstead of “I must be perfect to be worthy,” try “I am valuable beyond my achievements.”\nInstead of “I can’t fail, or I’ll disappoint everyone,” try “Failure is proof that I’m trying something worthwhile.”\nInstead of “I must follow the safest path,” try “Exploring my own path is an act of self-respect.”\nDefining Life Values: What Truly Matters? At some point, we have to decide: Whose values are we living by?\nAre we chasing success defined by society, family, or our true selves?\nAre we making choices based on fear or genuine desire?\nAre we prioritizing what looks good externally or what feels right internally?\nFinding inner peace comes not from external achievements but from aligning life with authentic values. Some people find it in creative work, relationships, impact, or even embracing uncertainty. There’s no single right answer—only the one that resonates with you.\nFinal Thoughts\nCulture and upbringing shape us, but they don’t have to define us forever. Recognizing the patterns, questioning them, and rewriting our own narrative is part of growth. The goal isn’t to reject where we came from but to integrate it with who we choose to become.\nIf free will means being completely uncaused by prior events, it probably doesn’t exist. But if it means acting based on your own reasoning, emotions, and desires (even if those are influenced by prior causes), then we can still meaningfully say we have free will.\nThe real question is: Do you need “ultimate” free will to feel like your choices matter?\nWould love to hear from others—have you ever questioned the values you were raised with? What helped you redefine them?\n","categories":["感想"]},{"title":"WebAPP怎么保存状态？ Token设计与选型","url":"/2025/01/07/Token/","content":"认证系统的设计与选型：JWT到长短生命周期的Token管理\n大约在 2010 年，大多数Web服务器都使用session来存储状态信息，用户将执行用户/密码登录，后端将为该session ID创建SessionID。 人们开始琢磨一个新点子：用 令牌 (token) 进行身份认证。JWT (JSON Web Token) 一战成名，成为这个时代的宠儿！在此之前，每次用户访问系统，服务器都得去数据库查一查：“嘿，这哥们儿还算是登录状态吗？” 这不仅拖慢了系统，还让分布式身份认证变得异常麻烦。\n而 JWT 的出现，彻底改变了游戏规则。\n👉 服务器不需要查询数据库，只需验证 JWT 的签名 就能知道用户是否登录。\n这不仅提升了性能，还让那些酷炫的分布式认证系统成为可能。而分布式认证系统之所以变得重要，是因为那时候全世界都陷入了微服务（Microservice）**的热潮。\n不过，令牌也有个问题：唯一重要的是令牌本身。没有数据库来确认会话的有效性，这意味着如果有人偷了你的令牌，他们就可以在令牌有效期内假装成你。因此，令牌的生命周期应该很短。以前会话可以持续几个小时，而令牌的生命周期可能只有几分钟。\n短命令牌的烦恼：用户得反复登录\n但如果令牌的生命周期太短，用户就会觉得很烦，因为他们需要一次又一次地登录。更糟糕的是，令牌可能在他们正用着应用的时候突然“死掉”。于是，**刷新令牌（Refresh Token）**应运而生。\n有了刷新令牌，你可以在令牌快要过期的时候，偷偷发个请求去续命。每个刷新令牌只能用一次，而且当你用它的时候，你还会得到一个新的刷新令牌。这样一来，如果有人试图重复使用同一个刷新令牌，我们就知道出事了，然后可以把所有跟这个被盗令牌相关的刷新令牌都废掉。最终，所有使用该账号的用户都会被强制登出。\n工程师的永恒饭碗：自己制造问题，再解决问题！\n现在你明白为什么工程师的工作永远干不完了吧？因为我们总是先发明一些新问题，然后再发明一些解决方案，而这些解决方案又会带来更多的问题！😄\n在现代的认证与授权机制中，Token扮演着关键角色，用于标识和验证用户的身份。\nToken是一个令牌，客户端访问服务器时，验证通过后服务端会为其签发一张令牌，之后客户端就可以携带令牌访问服务器，服务端只需要验证令牌的有效性即可。一句话概括；访问资源接口（API）时所需要的资源凭证\nJWT（JSON Web Token）常常是一个热门选择。下面我将从设计原则、优势与劣势、以及实际应用场景等角度详细说明 JWT 的设计与选型问题。JWT 的标准结构由三部分组成：\nHeader（头部）：通常包含令牌的类型（通常为 JWT）以及所使用的签名算法（如 HMAC SHA256 或 RSA）。\nPayload（载荷）：包含声明（Claims），如注册声明（如 iss、sub、aud、exp、nbf、iat 和 jti）以及自定义数据。\nSignature（签名）：通过对 Header 和 Payload 进行编码后，使用指定的算法和密钥（或私钥）进行签名，确保数据未被篡改。\n然而，如何设计Token，尤其是在选择使用Access Token和Refresh Token的组合（简称AT&amp;RT方案）还是单Token方案时，往往需要根据业务需求和安全性做权衡。本文将从原理、场景和安全性三个角度入手，深入探讨Token设计中的抉择。\n\nAccess Token 与 Refresh Token 的角色分工\nAccess Token 和 Refresh Token 的设计理念源于 OAuth 2.0 协议，而 OAuth 2.0 是由 Internet Engineering Task Force (IETF) 在 2012年 正式发布的（RFC 6749）。虽然 OAuth 2.0 是这一设计的主要推动者，但它的灵感可以追溯到早期的身份验证和授权机制。Access Token（简称AT）和 Refresh Token（简称RT）的设计，背后的核心思想是将授权与资源访问的行为分离。\n\n\nAccess Token：用于快速访问受保护的资源。它通常有较短的有效期，直接携带用户授权信息（如权限、身份等），在API请求中传递给服务器进行验证。\n\n特点：短时有效、高频使用。\n风险：一旦被窃取，可能导致短期内资源被恶意访问。\n\n\n\nRefresh Token：用于刷新Access Token的有效期。它通常只在认证服务器（Authorization Server）和客户端之间交互，不会直接参与资源访问。\n\n特点：长时有效、低频使用。\n风险：被窃取后可能允许恶意用户长期续签Access Token。\n\n\n\n通过将两者分工，AT&amp;RT方案将获取授权这一高危、低频的操作与获取资源这一低危、高频的操作分离开，从而降低安全风险。\n\nAT&amp;RT 与单Token方案的核心区别\nAT&amp;RT方案与单Token方案的区别，可以总结为一个核心理念：控制风险的暴露面。\n\n\n单Token方案：\n\n单Token同时承担授权和资源访问功能。\n一旦泄漏，恶意用户可以直接访问资源，且可能长时间有效。\n适用场景：小型应用、内网服务、对安全性要求较低的系统。\n\n\n\nAT&amp;RT方案：\n\n将授权功能剥离到Refresh Token，Access Token则专注于资源访问。\n好处：\n\n如果Access Token泄漏，由于其短时效性，攻击者只能在有限时间内使用。\n即使Refresh Token泄漏，也可以通过令牌轮换机制（Token Rotation）降低风险。\n\n\n适用场景：安全敏感、用户访问频繁的场景，如银行、社交媒体、云服务等。\n\n\n\n\n设计Token时需要考虑的关键问题\n在实际系统中，选择单Token还是AT&amp;RT方案，取决于以下几个因素：\n\n\n安全性需求：\n\n如果系统中涉及敏感数据或存在较高的Token泄露风险（如公网环境、多终端接入），推荐使用AT&amp;RT方案。\n单Token方案适用于风险较低的场景，如开发测试环境或不涉及关键数据的内部服务。\n\n\n\n使用频率：\n\n高频访问的API更倾向于AT&amp;RT方案，因为Access Token的短时效性可以有效减少潜在攻击窗口。\n如果访问频率较低，单Token方案可能更简洁高效。\n\n\n\n复杂度与成本：\n\nAT&amp;RT方案需要维护多种令牌状态（如失效、刷新、轮换），实现复杂度较高。\n单Token方案实现简单，适合中小型团队或初创项目。\n\n\n\nToken有效期：\n\nAccess Token通常设置为几分钟到几小时的有效期，降低泄漏风险。\nRefresh Token的有效期则更长，通常为数天到数月。\n\n\n\n\nAT&amp;RT方案的安全机制\n为了进一步提高安全性，AT&amp;RT方案通常配合以下技术：\n\n\n令牌轮换（Token Rotation）：\n\n每次使用Refresh Token时，返回新的Refresh Token并使旧的失效。\n即使Refresh Token被窃取，也只能使用一次，降低攻击风险。\n\n\n\n短期有效Access Token：\n\n将Access Token有效期控制在较短时间内（如5分钟）。\n即便被窃取，攻击窗口也极为有限。\n\n\n\n设备绑定与IP限制：\n\n将Refresh Token绑定到特定设备或IP，防止跨设备滥用。\n\n\n\n安全存储：\n\n客户端应妥善存储Refresh Token（如使用Secure Storage或HttpOnly Cookie）。\n\n\n\n\n实践中的选型建议\n\n\n\n场景\n推荐方案\n原因\n\n\n\n\n内网微服务通信\n单Token方案\n安全风险较低，实现简单，且内部环境对Token泄漏容忍度高。\n\n\n移动端应用（电商、社交媒体等）\nAT&amp;RT方案\n需要支持多终端、频繁访问资源且对安全性要求较高。\n\n\n低频后台管理系统\n单Token方案\n后台系统操作频率低，可通过严格控制用户行为降低风险。\n\n\n高敏感性业务（银行、金融等）\nAT&amp;RT方案+Token轮换\n涉及敏感数据，需最大限度减少Token泄漏后的攻击可能性。\n\n\n\n\n总结\nAT&amp;RT方案的核心在于用双Token机制分隔高频和高危操作，从而降低风险的暴露面。而单Token方案则以简洁性取胜，适合对安全性要求较低的场景。在实际应用中，开发者需要根据业务特点、用户行为模式以及系统复杂度，选择最合适的方案。\nToken设计没有“银弹”，但对风险的正确理解，才是高效与安全并行的基石。\n背景知识\n在身份验证和授权领域，“有状态”（stateful）和\"无状态\"（stateless）通常用来描述系统处理用户认证信息的方式。\n有状态（Stateful）：\n有状态的认证系统在服务器端会维护用户的会话状态信息。这意味着服务器端会存储关于用户认证状态的信息，比如用户的登录状态、会话信息、权限等。\n在有状态的认证系统中，服务器端在处理用户请求时，会根据存储的会话状态来验证用户的身份和权限，通常会使用会话标识（如 Session ID）来识别用户的会话状态。\n典型的例子包括传统的基于会话的认证方式，用户在登录后会获得一个会话标识（Session ID），服务器端会在会话管理中保持用户的登录状态，并在后续请求中使用该会话标识来验证用户的身份。\n使用有状态令牌（Stateful Token）在特定场景中具有显著优势，特别是在需要服务器端管理会话状态和即时控制会话生命周期的情况下。以下是一些适合使用有状态令牌的场景：\n1. 高度安全性要求的应用\n\n即时会话撤销：在高安全性应用中，例如金融服务、在线支付和医疗系统，可能需要立即撤销某些会话。服务器端存储会话状态，可以在需要时立即无效化令牌。\n敏感操作：对于涉及敏感数据和操作的系统，服务器端管理可以提供额外的控制和审计能力。\n\n2. 单一服务器或共享存储的系统\n\n集中式会话管理：在单服务器应用或使用集中式共享存储的环境中，服务器端存储和管理会话状态较为简单和高效。\n小规模系统：对于小规模系统或负载较低的系统，管理会话状态的开销相对较低。\n\n3. 需要复杂的会话控制,需长时间会话管理的应用\n\n细粒度权限控制：服务器端可以根据用户会话状态进行细粒度的权限控制，例如临时提升权限、基于活动时间段的权限控制等。\n动态会话属性：在会话期间可能需要动态调整用户的会话属性（如角色、权限等），服务器端管理更加灵活。\n长期会话：在需要长时间保持用户会话的应用中，例如企业内部应用或需要频繁交互的应用，服务器端可以更好地管理和维护会话状态。\n会话恢复：支持会话恢复，用户可以在不同设备和会话中无缝切换。\n\n4. 严格的会话一致性需求,不需要跨域\n\n强一致性：需要确保会话状态的一致性，如电商购物车、多人协作工具等，服务器端可以更好地保证一致性和数据同步。\n内部系统：主要用于内部系统，不需要跨域、跨平台访问，这类系统更适合使用有状态令牌。\n\n无状态（Stateless）：\n无状态的认证系统不会在服务器端维护用户的会话状态信息。每个请求都是独立的，服务器不会存储任何关于用户的会话状态。\n在无状态的认证系统中，服务器端不需要存储任何会话状态，所有的认证信息都被包含在每个请求中，通常是在请求的头部（Header）或参数中。\n典型的例子包括基于令牌（Token）的认证方式，比如 JSON Web Token（JWT）。在这种方式下，用户在登录后会获得一个令牌（Token），服务器端会使用令牌中的信息来验证用户的身份和权限，而不需要存储任何会话状态。\n即使使用 JWT 进行用户认证，后端数据库仍然需要存储用户的账户和密码信息。这是因为：\n\n用户注册和登录：在用户注册时，需要存储他们的账户信息（如用户名、邮箱）和密码（通常是经过哈希处理的密码）。\n身份验证：在用户登录时，后端需要验证用户输入的密码是否正确，这需要与数据库中存储的密码进行比对。\n\nJWT 是在用户成功登录后生成的令牌，通常包含用户的身份信息和权限信息。其生成和使用过程如下：\n\n用户登录：用户提交用户名和密码。\n身份验证：服务器验证用户名和密码。如果验证成功，服务器生成一个 JWT，包含用户的身份信息（如用户ID）和其他必要的声明（claims）。\n返回 JWT：服务器将生成的 JWT 返回给客户端。\n后续请求：客户端在后续请求中携带 JWT，服务器通过验证 JWT 的签名和有效期来确认用户身份。\n\nJWT 在用户认证和授权方面具有以下优势：\n\n\n无状态性：服务器不需要存储用户的会话信息，每次请求都可以通过验证 JWT 来确认用户身份。\n\n\n扩展性：适用于分布式系统和微服务架构，因为令牌是自包含的，不需要共享会话状态。\n\n\n安全性：通过加密和签名，确保令牌的完整性和可信度。\nCookie-Based Session：\n\n\n优点：简单、安全性高（服务器端存储）。\n\n\n缺点：扩展性差，服务器压力大。\n\n\nToken-Based Authentication：\n\n优点：无状态、扩展性好。\n缺点：客户端存储安全性较差。\n\nJWT：\n\n优点：无状态、自包含、适合分布式系统。\n缺点：安全性依赖于客户端存储和令牌签名。\n\nCookie-Based Session：适用于简单的 Web 应用，特别是单服务器或小规模应用。\nToken-Based Authentication：适用于需要跨多个客户端（如移动端、单页应用）的应用。\nJWT：适用于分布式系统和微服务架构，提供更高的扩展性和灵活性。\n如果是在一个只有帐号跟密码的世界上，我就必须要把我的 Google 帐号和密码告诉给这第三方服务行事历服务，它才有办法取得这些资讯。但如果我把帐号密码告诉了第三方，它可能就可以在暗地里窃取行事历以外的资讯，像是在 Gmail 里的机密资讯。这时候就出现了使用 Access Token 来解决这个问题的协议，OAuth 2.0。\n在 OAuth 2.0 中，使用短期的 Access Token 有几个主要的优势和原因：\n\n安全性： 短期的 Access Token 有效期较短，如果被盗或泄露，攻击者可利用的时间窗口较小，减少了被滥用的风险。相比之下，长期有效的 Token 更容易成为攻击者的攻击目标。\n可控制资源的访问权限（最小权限原则\n强制刷新： 通过设定较短的有效期，强制客户端定期刷新 Access Token。这有助于保持令牌的有效性和安全性，并且可以促使客户端与认证服务器交互，更新并重新授权访问权限。\n可撤销性: 您可以随时撤销第三方服务的访问权限。\n\n但同时也要权衡安全性和效率。 Access Token应该维持在较短有效期，过长不安全，过短也会影响用户体验，因为频繁去刷新带来没有必要的网络请求。可以参考我们常常在某些网站停止操作一段时间之后就会掉线，这个时间是Refresh Token的有效期，Access Token不应长过这个时间。\nRefresh Token的有效期就是允许用户在多久时间内不用重新登录的时间，可以很长，视业务而定。我们在使用某些APP的时候，即使一个月没有开过也是登录状态的，这6y就是Refresh Token决定的。授权服务在接到Refresh Token的时候还要进一步做客户端的验证，尽可能排除盗用的情况。\nA refresh token can help you balance security with usability. Since refresh tokens are typically longer-lived, you can use them to request new access tokens after the shorter-lived access tokens expire.\nIt’s a common practice for some JWT frameworks or authentication systems to use a combination of JWT access tokens and opaque refresh tokens. This hybrid approach aims to leverage the advantages of both token types while mitigating their respective drawbacks.\n通过引入 refresh token，系统设计者实现了一种混合模型：保持了大部分请求的无状态性（通过 JWT），同时通过 refresh token 实现必要的有状态管理（如 token 的更新和撤销）。这种方法有效地在用户体验、安全性和系统性能之间取得了平衡。\n你的总结确实很好地解释了为什么在现代架构中，尽管追求无状态（stateless），但还是需要一些有状态（stateful）元素来解决实际问题。这样的设计既保持了分布式系统的优势，又提供了必要的安全控制和用户体验优化。\n所有token应该保管在private的地方，也就是只能客户端自己使用，所有token都应该在TLS信道下发送（比如HTTPS）。\nIn an SSO/Auth system, the token you design should store information essential for secure and efficient authentication and authorization. Here’s a breakdown of the key information that should be included in your token:\nv.Set(\"scope\", \"iauth_token\")  v.Set(\"response_type\", \"code\")  v.Set(\"client_id\", testService.ServiceID)  v.Set(\"state\", uuid.NewString())  v.Set(\"redirect_uri\", \"https://test.hotstar.com:9999/callback\")  tokenClaims := &amp;iauth.TokenClaims{}    parser := jwt.Parser{}    _, _, err = parser.ParseUnverified(result.AccessToken, tokenClaims)    assert.Nil(t, err)    assert.Equal(t, tokenClaims.SubjectID, testUserId)    assert.Equal(t, tokenClaims.ExpiresAt, int64(result.ExpiresAt))    assert.Equal(t, tokenClaims.ExpiresAt-12*3600-2, tokenClaims.IssuedAt) //we set 2 second offset in issueAt    assert.True(t, tokenClaims.IssuedAt == tokenClaims.NotBefore)    assert.NotNil(t, tokenClaims.Id)\nEssential Information:\n\nUser Identifier (UID): A unique identifier for the user, typically a numerical ID. This is crucial for identifying the user associated with the token.\nIssuer (iss): Identifies the system that issued the token. This helps verify the token’s origin and prevent forgery.\nAudience (aud): Specifies the intended recipient or service for the token. This ensures the token is used only by the intended service.\nExpiration Time (exp): A timestamp indicating when the token expires. This limits the token’s validity and reduces the risk of unauthorized access if compromised.\nIssued At Time (iat): A timestamp indicating when the token was issued. This helps track token age and detect potential issues.\nToken Type (typ): Specifies the type of token being used (e.g., JWT, opaque token). This helps the receiving service correctly handle the token.\n\nOptional but Recommended Information:\n\nScope (scp): Defines the permissions granted to the token holder. This allows for fine-grained control over what resources the user can access.\nNonce (jti): A unique identifier for the token, preventing replay attacks where a stolen token is used multiple times.\nSession ID: A unique identifier for the user’s session. This allows for session management and revocation.\nUser Roles/Groups: Information about the user’s roles or groups within the system. This can be used for authorization decisions.\nRefresh Token ID: If using a refresh token strategy, this identifies the associated refresh token for seamless token renewal.\n\nConsiderations:\n\nSecurity: Prioritize security by using strong encryption algorithms and proper key management for token generation and validation.\nPerformance: Balance the amount of information stored in the token with performance considerations. Larger tokens can impact performance.\nPrivacy: Only include necessary information to minimize the risk of exposing sensitive user data.\nToken Format: Choose a suitable token format like JWT (JSON Web Token) for its standardized structure and ease of parsing.\n\n参考文献与数据来源：\n\nRFC 7519 - JSON Web Token (JWT)\nJWT Best Practices\nOWASP JWT Cheat Sheet\nNIST Guidelines on Authentication\n\n","categories":["计算机工程"],"tags":["计算机基础","分布式系统","网络安全"]},{"title":"重新认识操作系统","url":"/2024/09/17/Linux0/","content":"操作系统OS\n在学习操作系统的过程中，我们会接触到众多复杂的概念，如内核、进程管理、内存管理、文件系统等。为了更好地掌握这些内容，建议采取循序渐进的学习路线，从理论到实践、从基础到进阶，逐步深入。\n学习路线\n\n操作系统的基本概念与历史\n学习操作系统的基础知识，了解其核心功能和发展历史。\n操作系统的起源（如 UNIX 的发展、GNU 项目的影响）\n现代操作系统的类型及其关系（如 Linux、Windows、macOS）\n操作系统的核心技术\n掌握内核、进程、线程、内存管理、文件系统等关键概念。\n内核的设计与功能（特别是 Linux 内核的架构）\n进程与线程的管理（理解 fork()、进程调度等）\n内存管理的机制:MMU 和虚拟内存: 深入理解 MMU 和虚拟内存的工作原理，以及分页技术和地址翻译机制。\n页面置换算法: 了解常用的页面置换算法，如 LRU、FIFO 和 LFU。\n写时复制 (COW): 掌握 COW 技术的概念和应用，例如在 fork() 中的使用。\n字节序 (Endianness): 了解大端序和小端序的区别，以及它们在数据存储和网络传输中的重要性。\n内存泄漏: 理解内存泄漏的概念和危害，并学习如何避免内存泄漏。\n文件系统的结构与实现（从文件的存储到系统调用）\n进阶专题：并发与多任务处理\n理解并发、并行、以及多任务处理在操作系统中的应用。\n推荐学习内容：\n不同的文件系统类型（FAT、NTFS、ext4、Btrfs）、使用的数据结构（索引节点、目录条目）、日志记录、磁盘调度算法。\n进程间通信（IPC）、信号量、锁机制\n多线程编程与并发控制（如 pthread 库的使用）\n\n第四阶段：系统管理和工具\n系统管理工具: 熟悉常用的 Linux 系统管理工具，如 systemd、cron、top/htop、ps、journalctl 和 dmesg。\n文件管理工具: 掌握常用的 Linux 文件管理工具，如 ls、cp、mv、rm、find、grep、tar、gzip、bzip2、chmod 和 chown。\n文本处理工具: 学习常用的 Linux 文本处理工具，如 vim、nano、sed、awk、cat、less 和 more。\n网络工具: 熟悉常用的 Linux 网络工具，如 ifconfig/ip、ping、traceroute、ssh、curl、wget、netstat、ss、iptables 和 nftables。\n第五阶段：高级主题\nDMA 零拷贝: 了解 DMA 零拷贝技术及其在提高文件传输性能方面的作用。\n空分复用和时分复用: 理解空分复用和时分复用的概念及其在操作系统中的应用。\n多线程/多进程: 学习多线程和多进程的概念，以及它们在解决阻塞问题和提高程序并发性方面的应用。\n协程: 了解协程的概念和优势，以及它们与线程的区别\n实践为主: 学习 Linux 最重要的是实践，建议你在学习过程中多动手操作，例如在虚拟机或云服务器上安装 Linux 系统 or 实现一个多线程程序，理解线程调度与同步并尝试使用各种命令和工具。\n硬件基础\n\n地址空间（address space）表示任何一个计算机实体所占用的内存大小\n程序局部性原理：是指程序在执行时呈现出局部性规律，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域，具体来说，局部性通常有两种形式：时间局部性和空间局部性。\n**时间局部性：**被引用过一次的存储器位置在未来会被多次引用（通常在循环中）。\n**空间局部性：**如果一个存储器的位置被引用，那么将来他附近的位置也会被引用。\n存储器抽象\n在计算机中，每个设备以及进程都被分配了一个地址空间。处理器的地址空间由其地址总线以及寄存器决定。地址空间可以分为Flat——表示起始空间位置为0；或者Segmented——表示空间位置由偏移量决定。在一些系统中，可以进行地址空间的类型转换。至于IP地址空间，IPV4协议并没有预见到IP地址的需求量如此之大，32位的地址空间已经无法满足需求了。因此，开发了IPV6协议，支持128位的地址空间 [1] 。\n暴露问题\n把物理地址暴露给进程会带来下面几个严重问题。第一，如果用户程序可以寻址内存的每个字节，它们就可以很容易地（故意地或偶然地）破坏操作系统，从而使系统慢慢地停止运行。即使在只有一个用户进程运行的情况下，这个问题也是存在的。第二，使用这种模型，想要同时（如果只有一个CPU就轮流执行）运行多个程序是很困难的。在个人计算机上，同时打开几个程序是很常见的（一个文字处理器，一个邮件程序，一个网络浏览器，其中一个当前正在工作，其余的在按下鼠标的时候才会被激活）。在系统中没有对物理内存的抽象的情况下，很难做到上述情景，因此，我们需要其他办法。\n为了解决这些问题，现代操作系统使用虚拟内存技术，将物理地址空间抽象成虚拟地址空间，并通过内存管理单元（MMU）来实现地址转换，使得每个进程只能访问自己的虚拟地址空间，从而提高系统的安全性、隔离性和稳定性。\nMMU管理\nData fetching\nOS使用内存从磁盘中取数据的过程通常称为“数据加载”或“页面调度”，涉及操作系统和硬件的协作，以下是这一过程的详细解释：\n\nCPU与内存的关系\n\nCPU：中央处理器（CPU）无法直接从磁盘读取数据，所有要执行的指令和数据都必须先加载到内存（RAM）中。\n\n磁盘与内存之间的数据交换机制\n\n分页机制（Paging）：现代操作系统通常采用虚拟内存管理，其中包含分页机制。当某个程序需要访问的数据不在内存中时，操作系统会将所需的数据从磁盘加载到内存。\n\n数据从磁盘加载到内存的过程\n\n3.1 虚拟内存与页表\n虚拟内存：操作系统将每个进程分配一个虚拟地址空间，这些虚拟地址并不直接对应物理内存，而是通过页表（Page Table）进行映射。\n页表：页表记录了虚拟地址与物理内存地址之间的映射关系。当CPU需要访问一个内存地址时，它首先检查页表以找到对应的物理内存地址。\n3.2 页错误（Page Fault）\n页错误：如果程序访问的内存地址在当前的页表映射中未找到（即不在内存中），则发生页错误。页错误并非错误，而是一个提示操作系统需要从磁盘中加载数据的信号。\n处理页错误：当页错误发生时，操作系统会暂停当前进程，并将相应的数据页从磁盘加载到内存，然后更新页表以反映新的映射关系。\n3.3 磁盘I/O操作\nI/O请求：操作系统向磁盘发出I/O请求以读取特定的数据块。\n数据读取：磁盘控制器负责将所需的数据从磁盘读取到内存缓冲区中。这一过程涉及磁盘旋转定位（如果是HDD）或闪存寻址（如果是SSD）。\nDMA传输：直接内存访问（DMA）控制器可能用于加速数据传输过程，将数据从磁盘直接加载到内存，而不经过CPU，释放CPU用于其他任务。\n3.4 内存更新\n内存填充：当数据从磁盘读取到内存后，操作系统将其放入指定的内存位置，并更新页表以反映这一变化。\n进程恢复：在内存更新完成后，操作系统恢复原先的进程，重新执行之前因页错误而暂停的指令。\n\n缓存与预读取\n\n磁盘缓存：操作系统可能会将从磁盘读取的数据暂时存储在内存中（缓存），以加速后续的访问。若程序在短时间内再次请求同一数据，操作系统可以直接从内存中获取数据，而无需重新访问磁盘。\n预读取（Prefetching）：操作系统有时会预先从磁盘读取一系列连续的数据块到内存中，预计程序可能会需要这些数据，从而减少后续的I/O操作次数。\n\n文件系统和块设备管理\n\n文件系统：操作系统的文件系统负责将文件映射到磁盘上的物理块地址，决定如何将文件内容组织成块并存储在磁盘上。\n块设备管理：块设备（如磁盘）的管理程序处理低级别的读写操作，包括将逻辑块地址转换为物理块地址并执行读写。\n\n总结\n\n当CPU需要访问不在内存中的数据时，操作系统会通过页表映射、页错误处理、I/O请求、DMA传输、以及缓存等机制，从磁盘中读取数据并加载到内存。这一过程确保程序在物理内存不足以容纳所有数据时，依然能够顺利执行，从而实现虚拟内存的扩展。\n虚拟内存为每个进程提供了一个私有的地址空间 每个进程拥有一片连续完整的内存空间 数据不断换入换出外存实现宏观上的大于实际内存的虚拟内存。\n\n虚拟地址空间和物理地址空间： 每个应用程序看到的是一组虚拟地址，而不是真正的物理内存地址。操作系统负责将这些虚拟地址映射到物理内存中的实际位置。\n分页技术： 操作系统将虚拟内存划分成固定大小的页面（通常是4KB），同时也将物理内存划分成相同大小的页框。虚拟页被映射到物理页框，但不一定要将所有虚拟页都加载到物理内存中。\n页面置换： 当应用程序需要访问一个虚拟页，但该页不在物理内存中时，会触发页面置换。操作系统会根据一定的算法，将一个当前不太可能访问的物理页替换出去，然后将需要的虚拟页加载到这个物理页中。\n页表： 操作系统维护一个页表，记录虚拟页与物理页的映射关系。当应用程序访问虚拟地址时，操作系统会查询页表，找到对应的物理页。\n页面调度算法： 操作系统使用不同的算法来决定哪些页应该被替换出物理内存，以便为新的虚拟页腾出空间。一些常见的页面调度算法包括最近最少使用（LRU）、先进先出（FIFO）、最不常用（LFU）等。\n\n虚拟内存的主要优点包括：\n\n允许运行比物理内存更大的应用程序。\n提供了更好的内存管理和分配灵活性。\n能够使多个应用程序同时运行，而不会发生内存冲突。\n提供了更好的内存保护，防止一个应用程序影响到其他应用程序的内存空间。\n\n然而，虚拟内存也有一些缺点，比如访问虚拟内存可能会引入一定的性能开销，因为涉及到物理内存和硬盘之间的数据交换。如果系统中同时运行的应用程序过多，可能会导致频繁的页面置换，从而影响性能。\n总之，虚拟内存是现代操作系统中重要的内存管理技术，它通过将物理内存和硬盘空间结合起来，使得计算机系统能够更有效地管理内存资源，并支持运行多个应用程序。\n一台计算机具有16位地址意味着它可以寻址2^16个不同的地址，因此它的地址总空间为64K（64 kilobytes）\n然而，物理内存只有32KB（32 kilobytes）。这意味着，实际上，计算机在物理内存中只能同时存储32KB的数据。\n虚拟内存技术可以提供一种机制，允许计算机运行比其物理内存更大的程序。这是通过将部分程序数据存储在磁盘上的虚拟内存中实现的。\n以下是一个简单的解释：\n\n虚拟内存：\n\n计算机上运行的程序可能比物理内存大。为了解决这个问题，操作系统使用虚拟内存技术。程序的地址空间可以分为多个部分，其中一部分被加载到物理内存中，而另一部分存储在磁盘上的虚拟内存中。\n当程序访问在虚拟内存中的部分时，操作系统会将相应的数据块加载到物理内存中。这种方式允许运行大于物理内存的程序，但可能会导致性能损失，因为频繁的磁盘I/O会比内存访问慢得多。\n\n\n页面交换：\n\n当程序访问未加载到物理内存中的虚拟内存页面时，发生页面交换。操作系统可能会将一些当前不使用的物理内存页面（通常是最近未使用的页面）写回到磁盘上的虚拟内存，以便腾出空间来加载被请求的虚拟内存页面。\n\n\n地址翻译：\n\n操作系统使用地址翻译机制来将程序中的虚拟地址映射到物理地址。这个映射是由硬件的内存管理单元（MMU）来处理的。MMU负责将虚拟地址转换为物理地址，并确保访问的内存区域是有效的。\n\n\n\n分页系统映射\n内存管理单元（MMU）管理着地址空间和物理内存的转换，其中的页表（Page table）存储着页（程序地址空间）和页框（物理内存空间）的映射表。一个虚拟地址分成两个部分，一部分存储页面号，一部分存储偏移量。CPU 上的内存管理单元（Memory Management Unit，MMU）就是专门用来进行虚拟地址到物理地址的转换的，不过 MMU 需要借助存放在内存中的页表，而这张表的内容正是由操作系统进行管理的。\n页表是一个十分重要的数据结构！\n操作系统为每个进程建立了一张页表。一个进程对应一张页表，进程的每个页面对应一个页表项，每个页表项由页号和块号（页框号）组成，记录着进程页面和实际存放的内存块之间的映射关系。\nARM架构\n\n模式：\n\n用户模式 (usr)：这是应用程序运行的模式。\n系统管理模式 (svc - Supervisor Call)：这是内核模式，系统调用通过SWI (Software Interrupt) 指令进入此模式。\n\n\n系统调用：\n\n在ARM中，系统调用通常是通过执行 SWI (或新版的 SVC) 指令来实现的。这个指令会触发一个软中断，使得CPU从用户模式切换到监督者模式（内核态），并跳转到预定义的中断向量去处理系统调用。\n\n\n\nx86架构\n\nRing级别：\n\nRing 0：内核态，拥有最高权限，可以执行任何指令。\nRing 3：用户态，权限最低，不能执行特权指令。\n\n\n系统调用：\n\n在x86架构上，传统上是通过 int 0x80 中断实现系统调用的。现代x86 CPU（特别是x86-64）通常使用 syscall 指令，这是为了提高系统调用的效率。\nint 0x80 触发一个中断，使得处理器从Ring 3切换到Ring 0，并跳转到中断处理程序，处理系统调用。\n\n\n\nLinux的系统调用和中断处理\n\n系统调用的上下文：\n\n系统调用的执行是在调用进程的上下文中进行的。这意味着内核代码可以访问该进程的地址空间，执行I/O操作，管理文件等。\n\n\n硬件中断的上下文：\n\n硬件中断处理程序运行在中断上下文中，这个上下文与任何特定进程无关。处理中断的代码必须尽快完成，因为中断处理会阻止其他中断的发生（在同一IRQ线上），或者需要特别处理嵌套中断。\n\n\n切换过程：\n\n用户态到内核态：通过系统调用接口（如ARM的 SWI 或 x86的 int 0x80 或 syscall），或者硬件中断。\n内核态到用户态：通过特定的返回指令（如x86的 iret），恢复用户态的执行上下文。\n\n\n\n其他细节\n\n驱动程序：\n\n驱动程序中的函数一部分作为系统调用的一部分执行，另一部分可能在中断处理程序中执行。驱动程序需要精心设计，以处理好在不同上下文中的操作。\n\n\n安全性和稳定性：\n\n这种特权级别的分离确保了即使一个用户态的程序崩溃或被恶意攻击，它也不会直接影响到系统的核心功能，保护了操作系统的稳定性和安全性。\n\n\n\n通过这种方式，Linux有效地管理了系统资源，提供了安全的环境让应用程序运行，同时保持了对硬件的完全控制。\ndma零拷贝\n什么是 DMA 技术？简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。\n\n上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。 其次，还发生了 4 次数据拷贝，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程：\n第一次拷贝，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。 第二次拷贝，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。 第三次拷贝，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。 第四次拷贝，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。\n我们回过头看这个文件传输的过程，我们只是搬运一份数据，结果却搬运了 4 次，过多的数据拷贝无疑会消耗 CPU 资源，大大降低了系统性能。 这种简单又传统的文件传输方式，存在冗余的上文切换和数据拷贝，在高并发系统里是非常糟糕的，多了很多不必要的开销，会严重影响系统性能。\n所以，要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数。\n零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。。\n零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。\n所以，总体来看，零拷贝技术可以把文件传输的性能提高至少一倍以上。\n写时复制 (Copy-on-Write, COW) 是一种重要的资源管理技术，用于高效地实现可修改资源的复制操作，尤其是在虚拟内存管理和数据结构中应用广泛。\nCOW 的核心思想:\n\n延迟复制: COW 不会立即复制整个资源，而是在需要修改资源时才进行复制。\n共享只读数据: 在修改之前，多个进程或数据结构可以共享同一份只读数据，节省内存空间和复制时间。\n按需复制: 只有当某个进程或数据结构需要修改数据时，才会复制一份副本进行修改，其他进程或数据结构不受影响。\n\nCOW 在虚拟内存管理中的应用:\n\nfork() 系统调用: 在类 Unix 操作系统中，fork() 用于创建一个新进程，该进程是父进程的副本。COW 可以显著提高 fork() 的效率。\n\n传统 fork(): 需要复制父进程的整个地址空间，耗时且浪费内存。\nCOW fork(): 父进程和子进程 initially 共享相同的物理内存页面，这些页面被标记为只读。只有当某个进程需要修改页面内容时，才会复制一份副本进行修改。\n\n\n\nWhat is POSIX?\nPOSIX 是一套由 IEEE 制定的标准，旨在维护操作系统之间的兼容性。它定义了 API、命令行 shell 和实用程序接口，以确保软件与各种 Unix 变体和其他操作系统兼容。\nKey Components of POSIX\n\nC API Extensions:\n\nPOSIX greatly extends the ANSI C standard with a wide array of APIs for file operations, process and thread management, networking, memory management, and more.\nFile Operations: Includes functions like mkdir, symlink, stat, poll, etc.\nProcess and Threads: Functions like fork, execl, wait, pipe, sem_*, shm_*, etc., are essential for process creation, inter-process communication, and synchronization.\nNetworking: The socket() API is crucial for network communication.\nMemory Management: Functions like mmap, mlock, mprotect, etc., manage memory allocation, protection, and advice.\n\n\nCLI Utilities:\n\nPOSIX defines a set of standard command-line utilities like cd, ls, echo, mkdir, etc. Many of these utilities are direct shell front-ends for corresponding C API functions.\nMajor implementations include GNU Coreutils, which provides the majority of these utilities on Linux systems.\n\n\nShell Language:\n\nPOSIX standardizes the shell scripting language, which includes variable assignments, command execution, and control structures. The most common implementation in Linux is GNU Bash.\n\n\nEnvironment Variables:\n\nPOSIX specifies standard environment variables like HOME, PATH, which play a crucial role in the shell and other utilities.\n\n\nProgram Exit Status:\n\nPOSIX extends the ANSI C standard by defining specific exit codes, such as 126 (command found but not executable), 127 (command not found), and values greater than 128 indicating termination by a signal.\n\n\nRegular Expressions:\n\nPOSIX defines two types of regular expressions: Basic (BRE) and Extended (ERE). These are used in various CLI utilities, such as grep, and are implemented in C libraries under regex.h.\n\n\nDirectory Structure and Filenames:\n\nPOSIX specifies aspects of the filesystem, such as the path separator (/), special directories (. for current directory, .. for parent directory), and restrictions on filenames.\n\n\nCommand Line Utility API Conventions:\n\nPOSIX outlines conventions for command-line utilities, such as using `` for standard input, - to terminate option parsing, and single-letter flags, although these are not strictly enforced by all implementations (e.g., GNU utilities often use long options).\n\n\nPOSIX ACLs (Access Control Lists):\n\nAlthough originally part of POSIX, ACLs were withdrawn but have been implemented in several operating systems, including Linux.\n\n\n\nPOSIX Compliance and Implementations\n\nCertified Systems:\n\nNot all systems are officially certified as POSIX-compliant due to the cost of certification, but many closely follow the standards. Examples of certified systems include macOS (formerly OS X), AIX, HP-UX, and Solaris.\nMost Linux distributions are very compliant with POSIX, although not officially certified.\n\n\nWindows and POSIX:\n\nWindows had limited POSIX support in some professional editions, but this was deprecated in Windows 8. Since 2016, Microsoft introduced the Windows Subsystem for Linux (WSL), which provides a Linux-like environment with system calls, ELF binary execution, and more, bringing Windows closer to POSIX compliance for developer usage.\n\n\nCygwin and MSYS2:\n\nThese are third-party projects that provide substantial POSIX API functionality on Windows. Cygwin offers a large collection of GNU and Open Source tools that provide functionality similar to a Linux distribution on Windows.\n\n\nAndroid:\n\nWhile Android is based on the Linux kernel, it does not fully comply with POSIX, primarily because it uses its own libraries and runtime environment (Dalvik/ART) rather than standard Linux libraries like glibc.\n\n\n\nPOSIX 在确保不同类 Unix 系统之间的互操作性和可移植性方面发挥着关键作用。虽然 Linux 和 macOS 等一些系统密切遵循 POSIX，但 Windows 等其他系统历来对 POSIX 的支持有限，尽管 WSL 等最近的发展已经改进了这一点。对于想要编写跨不同操作系统的可移植且可互操作的代码的开发人员来说，了解 POSIX 至关重要\nLinux\nLinus Torvalds 创建了前者。后者是全球数百万开发人员之间的协作，涉及GNU 项目, Linux 内核开发团队由 Torvalds 领导， X Window 系统的各种开发人员在过去的 29 年中，以及其他。这就是自由软件基金会要求将使用来自 GNU 项目的软件的完整 Linux 操作系统称为“GNU/Linux”的原因。\nMany open source developers agree that the Linux kernel was not designed but rather evolved through natural selection. Torvalds considers that although the design of Unix served as a scaffolding, “Linux grew with a lot of mutations – and because the mutations were less than random, they were faster and more directed than alpha-particles in DNA.”[73] Eric S. Raymond considers Linux’s revolutionary aspects to be social, not technical: before Linux, complex software was designed carefully by small groups, but “Linux evolved in a completely different way. From nearly the beginning, it was rather casually hacked on by huge numbers of volunteers coordinating only through the Internet. Quality was maintained not by rigid standards or autocracy but by the naively simple strategy of releasing every week and getting feedback from hundreds of users within days, creating a sort of rapid Darwinian selection on the mutations introduced by developers.”[74] Bryan Cantrill, an engineer of a competing OS, agrees that “Linux wasn’t designed, it evolved”, but considers this to be a limitation, proposing that some features, especially those related to security,[75] cannot be evolved into, “this is not a biological system at the end of the day, it’s a software system.”\n全称GNU/Linux，是一种免费使用和自由传播的类UNIX操作系统，其内核由林纳斯·本纳第克特·托瓦兹于1991年10月5日首次发布，它主要受到Minix和Unix思想的启发，是一个基于POSIX的多用户、多任务、支持多线程和多CPU的操作系统。它能运行主要的Unix工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。Linux有上百种不同的发行版，如基于社区开发的debian、archlinux，和基于商业开发的Red Hat Enterprise Linux、SUSE、Oracle Linux等。\nLinus built kernel to solve unix hard-use and lack I/O RPC function.  区分 Linux（内核）和 Linux（操作系统）。\nKernel 是计算机操作系统核心的计算机程序，通常可以完全控制系统中的所有内容**。**[1]它是操作系统代码的一部分，始终驻留在内存中[2]并促进硬件和软件组件之间的交互。完整的内核通过设备驱动程序控制所有硬件资源（例如 I/O、内存、密码） ，仲裁涉及这些资源的进程之间的冲突，并优化公共资源的利用，例如 CPU 和缓存使用、文件系统和网络套接字。在大多数系统上，内核是启动时最先加载的程序之一（在引导加载程序）。它处理其余的启动以及内存、外围设备和来自软件的输入/输出(I/O) 请求，将它们转换为中央处理器的数据处理指令。\n内核是操作系统的核心部分，具有完全控制系统资源的能力。它始终驻留在内存中，促进硬件和软件之间的交互。内核通过设备驱动程序控制硬件资源，仲裁进程之间的冲突，并优化共享资源的利用。在大多数系统上，内核是启动时最先加载的程序。\n内核的关键代码被加载到受保护的内核空间中，与应用程序运行的用户空间分离。这种分离可以防止用户数据和内核数据相互干扰，并提高系统的稳定性和安全性。内核提供了一个低级的接口，进程通过系统调用来请求内核服务。\n内核架构有整体内核和微内核两种设计。整体内核在单个地址空间中运行，以提高速度；微内核在用户空间中运行大部分服务，以提高灵活性和模块化。Linux内核是整体式的，但也支持模块化，可以在运行时加载和卸载内核模块。\n内核负责决定将哪些正在运行的程序分配给处理器，从而执行程序。它是计算机系统的中央组件，控制和管理系统的各个方面。\nLinux 操作系统将其运行环境分为两种状态：用户态和内核态。用户态是指应用程序运行的环境，而内核态是指操作系统内核运行的环境。\n用户态\n用户态（User Mode）是指应用程序运行的环境。在用户态下，应用程序可以使用用户态提供的系统调用接口来请求操作系统服务。用户态的应用程序不能直接访问硬件资源，必须通过系统调用来请求内核态的服务。\n用户态的特点：\n\n应用程序运行在用户态下\n不允许直接访问硬件资源\n必须通过系统调用来请求内核态服务\n\n内核态\n内核态（Kernel Mode）是指操作系统内核运行的环境。在内核态下，内核可以直接访问硬件资源，管理系统资源，并提供服务给用户态的应用程序。\n内核态的特点：\n\n操作系统内核运行在内核态下\n允许直接访问硬件资源\n管理系统资源并提供服务给用户态应用程序\n\n系统调用\n系统调用（System Call）是用户态应用程序请求内核态服务的接口。用户态应用程序通过系统调用来请求内核态的服务，例如读取文件、创建进程等。\n系统调用的过程：\n\n应用程序在用户态下执行\n应用程序需要请求内核态服务\n应用程序通过系统调用接口请求服务\n内核态接收到请求并处理\n内核态将结果返回给应用程序\n\n实践\n$表明是非root用户登录，#表示是root用户登录，它们是终端shell的命令提示符 几种常用终端的命令提示符\nBASH: root账户: # ,非root账户: You can't use 'macro parameter character #' in math mode KSH: root账户: # ,非root账户:  CSH[TCSH]: root账户: % ,非root账户: %\n而/ 是根节点， ~ 是 home如果以root账号登陆   ~ 是 /root/\nProgram: a file containing instructions to be executed, static Process: an instance of a program in execution, live entity\nexecve（执行文件）在父进程中fork一个子进程，在子进程中调用exec函数启动新的程序。exec函数一共有六个，其中execve为内核级系统调用，其他（execl，execle，execlp，execv，execvp）exec(): often used after fork() to load another process\n许多程序需要开机启动。它们在Windows叫做\"服务\"（service），在Linux就叫做\"守护进程\"（daemon）。Linux Daemon（守护进程）是运行在后台的一种特殊进程。它独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。\n\nsyslogd：系统日志守护进程，负责记录系统日志消息。\nhttpd：Web 服务器守护进程，如 Apache 或 Nginx，用于提供 Web 服务。\nsendmail：邮件服务器守护进程，负责处理发送和接收邮件的功能。\nmysqld：数据库服务器守护进程，比如 MySQL 或 MariaDB，用于管理和提供数据库服务。\n\n这些守护进程通常在系统启动时自动启动，并在系统运行期间持续监听、处理或提供服务。它们的运行通常不需要用户的直接干预，而是通过配置文件或其他设置来控制其行为和功能。\nFTP\n主动模式的FTP是指服务器主动连接客户端的数据端口，被动模式的FTP是指服务器被动等待客户端连接自己的数据端口。\n被动模式的FTP通常用在处于防火墙之后的FTP客户访问外界FTP服务器的情况，因为在这种情况下，防火墙通常配置为不允许外界访问防火墙之内的主机，而只允许由防火墙之内的主机发起对外的连接请求。因此，在这种情况下不能使用主动模式的FTP传输，而被动模式的FTP可以良好的工作。\n主动模式需要服务器主动向客户端发起连接，而现在普通的客户端大多位于NAT之后，所以主动模式常常无法进行。即使可以，也需要客户端打开防火墙，允许服务端的20端口访问。所以服务端基本需要支持被动模式，但被动模式需要开放一段端口。为了安全，可以选择一小段端口，然后在防火墙开放这一段端口。\nEverything is a file\nLinux操作系统的设计哲学之一：将所有设备、资源或进程都视为文件或文件类似的对象。\n在Linux中，不仅普通的文本文件、目录、硬件设备和外部设备都被视为文件，甚至系统中的进程、网络连接、硬件接口等也被抽象为文件。这种抽象化使得Linux系统更加统一和灵活，因为它允许对不同资源使用相似的操作方式。\n好处是读写这些资源都可用open()/close()/write()/read()等函数进行处理。屏蔽了硬件的区别，所有设备都抽象成文件，提供统一的接口给用户。虽然类型各不相同，但是对其提供的却是同一套API。更进一步，对文件的操作也可以跨文件系统执行。在Linux系统中有三类文件：普通文件、目录文件和特殊文件。\n网络 I/O 通过网络进行数据传输，通常包括 TCP 或 UDP 等协议。网络 I/O 的延迟较高，取决于网络带宽、路由器、中继设备等，数据包可能需要经过多个中间节点才能到达目标服务器。\n局域网的延迟通常在微秒到毫秒级，而广域网（如互联网）的延迟可能会达到几十毫秒甚至更高。网络 I/O 并发是非常常见的需求，特别是在高并发服务器（如 Web 服务器）中。服务器通常需要同时处理数百甚至数千个网络连接。为了提高性能，使用了 I/O 多路复用（select、poll、epoll）和异步 I/O 来管理大量的并发连接。网络 I/O 带宽受到网络设备（如网卡、路由器）的限制。千兆以太网的带宽是 1 Gbps，大约相当于 125 MB/s，而如今常见的 10 Gbps 网卡能达到 1.25 GB/s，远低于本地存储设备。\n现代服务器（如 Nginx、Node.js）通常使用异步非阻塞 I/O 模型，以最大限度提高 I/O 吞吐量。\n多路复用I/O\nselect()、poll() 和 epoll() 函数在网络编程和操作系统中用于同时监控多个文件描述符（例如，套接字）的可读性、可写性或异常状态。它们的核心思想是通过监听一组文件描述符，来处理多个输入/输出操作，而不需要每个连接都使用一个独立的线程或进程。它们主要用于事件驱动编程，特别是在服务器中同时处理多个连接时。\n1. select()\nselect() 是一个 POSIX 标准函数，用于监控多个文件描述符，等待其中的某个变为就绪（可读、可写或有异常）。主要应用于网络编程中处理多客户端的并发连接。\n用法\n#include &lt;sys/select.h&gt;int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);\n\nnfds: 是需要监控的最大文件描述符加1。\nreadfds: 监控可读事件的文件描述符集合。\nwritefds: 监控可写事件的文件描述符集合。\nexceptfds: 监控异常事件的文件描述符集合。\ntimeout: 超时参数，指定 select() 应等待的时间。\n\n使用步骤\n\n创建一个 fd_set 集合，并通过 FD_SET() 向其中添加文件描述符。\n调用 select()，等待文件描述符就绪。\n检查返回的集合，处理就绪的文件描述符。\n\n例子\nfd_set readfds;FD_ZERO(&amp;readfds);       // 初始化集合FD_SET(sockfd, &amp;readfds); // 添加一个socket到集合中int result = select(sockfd + 1, &amp;readfds, NULL, NULL, NULL);if (result &gt; 0) {    if (FD_ISSET(sockfd, &amp;readfds)) {        // sockfd 可读，处理数据    }}\n使用场景\n\n多客户端 TCP 服务器：在一个进程或线程中，select() 可以同时处理多个客户端的 I/O 请求，而不需要为每个连接创建一个线程。\n事件驱动系统：通过 select() 来处理多个 I/O 事件或定时事件。\n\n2. poll()\npoll() 是 select() 的增强版本，解决了 select() 一些性能上的瓶颈（如文件描述符限制）。它更灵活，适合处理大量文件描述符。\n用法\n#include &lt;poll.h&gt;int poll(struct pollfd *fds, nfds_t nfds, int timeout);\n\nfds: 一个结构体数组，每个结构体表示一个文件描述符及其事件。\nnfds: 要监控的文件描述符数量。\ntimeout: 超时参数，指定等待的时间。\n\n使用步骤\n\n创建一个 pollfd 数组，为每个文件描述符设置感兴趣的事件。\n调用 poll()，等待文件描述符就绪。\n检查返回的结构体数组，处理就绪的文件描述符。\n\n例子\nstruct pollfd fds[1];fds[0].fd = sockfd;fds[0].events = POLLIN;int result = poll(fds, 1, -1); // -1表示无限等待if (result &gt; 0) {    if (fds[0].revents &amp; POLLIN) {        // sockfd 可读，处理数据    }}\n使用场景\n\n高并发系统：poll() 适用于处理大量并发连接，如高性能 HTTP 服务器。\n文件描述符大于 select() 限制：poll() 没有 select() 的最大文件描述符限制，更适合需要监控大量文件描述符的场景。\n\n3. select() vs poll()\n\n性能: poll() 通常比 select() 更快，尤其是在需要监控大量文件描述符时，因为 select() 的文件描述符集合在每次调用时都需要重新填充，而 poll() 使用的 pollfd 数组更高效。\n灵活性: poll() 更灵活，支持更多的事件类型，且不受 select() 最大文件描述符数量的限制。\n兼容性: select() 是较旧的函数，几乎在所有平台上都支持，而 poll() 是较新的替代方案，适合更现代的系统。\n\n4. 现代替代方案\n\nepoll()（Linux）：比 poll() 更加高效，特别是在大量文件描述符同时就绪的场景下。适合高并发场景，如大型网络服务器。\nkqueue()（BSD 系统）：FreeBSD、OpenBSD 和 macOS 等系统中的多路复用函数，类似 epoll()。\n\n总结\n\nselect(): 简单但有文件描述符数量限制，适用于小型并发场景。\npoll(): 更灵活、适合大量并发连接，解决了 select() 的一些局限性。\n\n在现代高并发系统中，通常会更倾向于使用 epoll() 或者 kqueue()，而 select() 和 poll() 更多用于相对简单或跨平台的场景。\nepoll() (Linux-specific)\n\n\nUsage: A more scalable and efficient mechanism designed specifically for high-performance applications (e.g., handling thousands of file descriptors).\n\n\nBenefits:\n\nMore efficient than poll() and select() because it uses an event-based model.\nThe kernel only returns file descriptors that are ready, rather than polling each one.\nAllows edge-triggered and level-triggered modes, offering flexibility.\n\n\n\nDrawbacks:\n\nOnly available on Linux, so it’s not portable to other operating systems like macOS or Windows.\n\nepoll为什么更高效?\n\n高效的事件通知机制\nepoll它的工作原理是只向内核注册一次文件描述符（使用epoll_ctl()）。此后，内核会跟踪哪些文件描述符已就绪。当您调用时epoll_wait()，它仅返回具有事件（即数据就绪）的文件描述符，而不会扫描所有文件描述符。这使得它更加高效，尤其是在监视大量描述符时，因为它消除了重复扫描的需要。\n它不再在每次调用时检查所有文件描述符，epoll而是更像一个“事件驱动”系统：当注册描述符的状态发生变化时，内核会通知您。\nepoll() offers two modes: edge-triggered (ET) and level-triggered (LT).\n\nEdge-triggered: The application is notified only once when the state of a file descriptor changes (e.g., when data becomes available). The application is then responsible for reading/writing all available data. This minimizes system calls, as the application doesn’t need to continuously poll the kernel.\nLevel-triggered: The behavior is more like poll() or select(); it keeps notifying the application as long as the file descriptor remains ready. This provides flexibility, but edge-triggered mode can significantly reduce overhead in high-performance applications.\n\n\n\n3. O(1) Performance with Large Numbers of Descriptors\n\nselect() and poll():\n\nBoth have O(n) complexity, where n is the number of file descriptors being monitored. As the number of descriptors grows, the time taken to check them grows linearly.\n\n\nepoll():\n\nepoll_wait() has O(1) complexity for most operations, meaning it doesn’t degrade in performance as the number of file descriptors increases. This allows it to scale much better when handling thousands (or even tens of thousands) of connections.\n\n\n\n4. Kernel-Space Data Structures\n\nepoll() uses more advanced kernel-space data structures (like red-black trees and linked lists) to efficiently track the file descriptors. Once a file descriptor is registered, it is stored in a tree, allowing quick lookups and updates, further improving performance.\nIn contrast, select() and poll() need to copy all file descriptors from user space to kernel space on every call, and the kernel has to check each one, leading to more overhead.\n\n此外还使用了内存映射（ mmap ）技术\n另一个本质的改进在于 epoll 采用基于事件的就绪通知方式。在 select/poll 中，进程只有在调用一定的方法后，内核才对所有监视的socket描述符进行扫描，而 epoll 事先通过 epoll_ctl() 来注册一个socket描述符，一旦检测到epoll管理的socket描述符就绪时，内核会采用类似 callback 的回调机制，迅速激活这个socket描述符，当进程调用 epoll_wait() 时便可以得到通知，也就是说epoll最大的优点就在于它 只管就绪的socket描述符，而跟socket描述符的总数无关 。\n\n\n\n\n提出一个问题：当在O_APPEND打开后，然后用 lseek移动到其他的位置，然后再用write写，这个时候，请问你数据写到哪里去了？\n\n\n系统调用：\n主要使用 open() 系统调用来打开文件。\n函数原型：int open(const char *pathname, int flags, mode_t mode);\n\n\n常用标志（flags）：\n\nO_RDONLY：只读模式\nO_WRONLY：只写模式\nO_RDWR：读写模式\nO_CREAT：如果文件不存在则创建\nO_APPEND：追加模式\nO_TRUNC：如果文件存在则清空\nO_NONBLOCK：非阻塞模式\n\n\n\n权限模式（mode）：\n当创建新文件时使用，如 0644（所有者可读写，其他人可读）\n\n\n返回值：\n成功返回文件描述符（非负整数），失败返回-1\n\n\n文件描述符：\n\n是一个小的非负整数\n用作I/O操作的句柄\n0, 1, 2 分别预留给标准输入、输出和错误\n\n\n\n打开文件的限制：\n\n每个进程有打开文件的最大数量限制\n系统全局也有打开文件的最大数量限制\n\n\n\n关闭文件：\n使用 close() 系统调用关闭文件描述符\n\n\n错误处理\n使用 errno 来检查具体的错误原因,     在 Linux 中打开和处理文件时，可能会遇到各种问题或错误，这些问题通常与文件系统状态、权限、系统资源限制有关：\n\n文件描述符耗尽：\n\n原因：打开太多文件而不关闭\n症状：open() 调用失败，返回 EMFILE 错误\n解决：及时关闭不用的文件，使用 select()/poll()/epoll() 管理多个文件描述符\n\n\n权限问题：\n\n原因：没有足够的权限访问文件\n症状：open() 返回 EACCES 错误\n解决：检查文件权限和进程权限，必要时调整\n\n\n文件不存在：\n\n原因：尝试打开不存在的文件（未使用 O_CREAT 标志）\n症状：open() 返回 ENOENT 错误\n解决：确保文件存在或使用适当的标志创建\n\n\n磁盘空间不足：\n\n原因：写入数据时磁盘已满\n症状：write() 调用失败，返回 ENOSPC 错误\n解决：清理磁盘空间或使用更大的存储设备\n\n\n文件锁冲突：\n\n原因：多个进程同时访问同一文件\n症状：fcntl() 锁定操作失败\n解决：实现适当的锁定机制和错误处理\n\n\n符号链接循环：\n\n原因：符号链接形成循环\n症状：open() 返回 ELOOP 错误\n解决：检查和修复符号链接结构\n\n\n文件系统只读：\n\n原因：尝试写入只读文件系统\n症状：open() 或 write() 返回 EROFS 错误\n解决：确保文件系统可写或更改操作逻辑\n\n\n\n\n\n在 Linux 和类 Unix 系统中，当一个文件以 O_APPEND 模式打开时，每次进行 write 操作时，文件的偏移量都会自动设置到文件的末尾，即使在此之前使用 lseek 改变了文件位置。因此，O_APPEND 标志会导致所有写操作都追加到文件的末尾。\n因为 O_APPEND打开后，是一个原子操作：移动到末端，写数据。这是O_APPEND打开的作用。中间的插入时无效的\n2.Linux 里利用 grep 和 find 命令查找文件内容\n从文件内容查找匹配指定字符串的行：\n$ grep \"被查找的字符串\" 文件名\n例子：在当前目录里第一级文件夹中寻找包含指定字符串的 .in 文件\ngrep \"thermcontact\" /.in\n从文件内容查找与正则表达式匹配的行：\n$ grep –e \"正则表达式\" 文件名\n查找时不区分大小写：\n$ grep –i \"被查找的字符串\" 文件名\nfork()\nfork() 系统调用，父进程调用fork会创建一个进程副本，代码中还可以通过fork返回值是否为0来区分是子进程还是父进程。子进程与进行 fork() 调用的进程（父进程）同时运行。创建新的子进程后，两个进程都将执行 fork() 系统调用之后的下一条指令。\n子进程使用与父进程相同的 pc（程序计数器）、相同的 CPU 寄存器、相同的打开文件。\n不同线程,父子拥有独立的内存空间\nfork是用来创建子进程的，这个函数的特别之处在于一次调用，两次返回，一次返回到父进程中，一次返回到子进程中，我们可以通过返回值来判断其返回点：   它不接受任何参数并返回一个整数值。下面是 fork() 返回的不同值。\n1. fork() 的工作原理\n当一个进程调用 fork() 时，操作系统会执行以下操作：\n创建子进程：操作系统为子进程分配一个新的进程控制块（PCB），这是操作系统用于管理进程的结构。\n复制父进程的资源：\n\n子进程会获得父进程的几乎完全相同的资源副本，包括进程的代码段、数据段、堆、栈、文件描述符等。\n但这只是“写时复制”（Copy-On-Write，COW）的副本，父子进程最初共享相同的内存区域，直到其中一个进程试图修改数据时才会实际进行内存的复制。\n\n返回值：\n\n在父进程中，fork() 返回子进程的进程ID（PID）。\n在子进程中，fork() 返回 0。\n如果 fork() 调用失败（例如系统资源不足），则会返回 -1，并设置 errno 以指示错误原因。\n\n2. fork() 的使用\n以下是一个简单的示例代码，展示了如何使用 fork() 创建一个子进程：\n`#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\nint main() {\npid_t pid = fork();  // 创建子进程\nif (pid &lt; 0) {  // fork() 调用失败\n    perror(\"fork failed\");\n    return 1;\n} else if (pid == 0) {  // 子进程\n    printf(\"This is the child process. PID: %d\\n\", getpid());\n} else {  // 父进程\n    printf(\"This is the parent process. Child PID: %d, Parent PID: %d\\n\", pid, getpid());\n}\n\nreturn 0;\n\n}`负值：创建子进程不成功。零：返回新创建的子进程。正值：返回给父级或调用者。该值包含新创建的子进程的进程 ID。\n3. fork() 的典型行为\n父子进程的执行顺序：\n\nfork() 之后，父进程和子进程会继续执行 fork() 后的代码，但它们的执行顺序是不确定的。父进程可能先执行，也可能子进程先执行。\n\n变量的独立性：\n\n尽管子进程复制了父进程的内存空间，但它们的变量是独立的。如果子进程修改了某个变量的值，父进程不会看到这个修改，反之亦然。\n\n文件描述符的共享：\n\n子进程复制了父进程的文件描述符表，这意味着父子进程可以访问相同的文件。然而，它们的文件偏移量是共享的；如果一个进程改变了文件的偏移量，另一个进程会受影响。\n\n4. fork() 的典型应用\n创建守护进程：fork() 常用于创建守护进程（daemon）。守护进程通常在后台运行，不与终端关联。\n多进程服务器：在一个多进程服务器中，服务器进程会 fork() 一个子进程来处理每个客户端请求，从而实现并行处理。\n进程隔离：fork() 可以用于创建一个子进程来执行不同的任务，从而实现进程隔离。\n5. fork() 相关的其他系统调用\nexec() 系列：fork() 通常与 exec() 系列函数（如 execl()、execvp()）一起使用。fork() 创建子进程后，子进程可以调用 exec() 来加载一个新的程序。\nwait() 和 waitpid()：父进程通常通过 wait() 或 waitpid() 等系统调用等待子进程完成。这些调用会阻塞父进程，直到子进程终止，并返回子进程的退出状态。\n6. 常见的 fork() 问题\n僵尸进程：当一个子进程结束而父进程尚未调用 wait() 或 waitpid() 来读取其退出状态时，子进程会成为僵尸进程，占用系统资源。可以通过父进程调用 `wait\n现代替代方案\n\npthread 库：与 fork 不同，pthread 提供了真正的多线程支持，允许多个线程共享同一个进程的地址空间，这在需要高效并发的场景中更加实用。\nCreateProcess：在 Windows 上，CreateProcess 是一个更为灵活的进程创建机制，它提供了比 fork 更为丰富的选项和功能。\n\nfork只能创建调用该函数的线程的副本，进程中其他运行的线程，fork不予处理。这就意味着，对于多线程程序而言，寄希望于通过fork来创建一个完整进程副本是不可行的。\n\n不一致性：如果在多线程环境中使用 fork()，子进程可能会进入一个不一致的状态，因为它只继承了一个线程的状态，而其他线程的状态未被复制。比如，父进程中的某个锁可能正在被另一个线程持有，而子进程中没有这个线程，这可能导致死锁或其他同步问题。\n资源竞争：多线程程序中，多个线程可能会竞争同一资源（如文件描述符、网络连接等）。当 fork() 只复制一个线程时，子进程中的资源状态可能与父进程不一致，导致资源竞争问题。\n\nBest practice\n\nfork() 后立即调用 exec()：在多线程程序中，如果必须使用 fork()，通常的做法是在 fork() 之后立即调用 exec() 系列函数来启动一个新的程序。这将完全替换子进程的地址空间，避免了与父进程共享资源或状态的不一致性问题。\n使用 pthread_atfork()：POSIX 提供了 pthread_atfork() 函数，用于在 fork() 调用前后执行特定的准备和恢复操作。通过 pthread_atfork()，可以在 fork() 之前锁定所有全局资源，在子进程中释放这些锁，从而避免潜在的竞争条件。\n\nLibs\n\n系统管理\n\nsystemd: 系统和服务管理器\ncron: 定时任务管理\ntop/htop: 系统资源监控\nps: 进程状态查看\njournalctl: 日志查看\ndmesg: 内核日志查看\n\n\n文件管理\n\nls, cp, mv, rm: 基本文件操作\nfind: 文件搜索\ngrep: 文本搜索\ntar, gzip, bzip2: 文件压缩和归档\nchmod, chown: 文件权限管理\n\n\n文本处理\n\nvim, nano: 文本编辑器\nsed, awk: 文本处理工具\ncat, less, more: 文件查看\n\n\n网络工具\n\nifconfig/ip: 网络接口配置\nping, traceroute: 网络诊断\nssh: 远程登录\ncurl, wget: 文件下载\nnetstat, ss: 网络连接状态\niptables/nftables: 防火墙管理\n\n\n\nSystemd 是许多现代 Linux 发行版中使用的初始化系统和系统管理器。以下是关于 systemd 的一些重要概念和常用命令：\n\n核心概念：\n\nUnit：systemd 管理的基本对象，包括服务、挂载点、设备和网络配置等\nService：最常见的 unit 类型，用于管理守护进程\nTarget：一组 unit 的集合，类似于运行级别\n\n\n\nUnit 文件结构\n一个典型的 .service 单元文件结构如下：\n`[Unit]Description=My Custom ServiceAfter=network.target[Service]ExecStart=/usr/bin/my-serviceExecReload=/bin/kill -HUP $MAINPIDExecStop=/bin/kill $MAINPIDRestart=on-failure[Install]WantedBy=multi-user.target``[Unit]`：描述单元和其依赖关系。`[Service]`：定义服务如何启动、停止，以及重启策略。`[Install]`：定义单元如何被启用或关联到其他目标。\n注意事项\n权限：许多 systemctl 命令需要以 root 权限运行。\n配置修改后重载：修改 .service 文件后，需要运行 systemctl daemon-reload 来重新加载配置。\n安全性：systemd 提供了许多安全选项，可以限制服务的权限和资源使用。\nEndian refers to the order in which bytes are arranged in memory, particularly in multi-byte data types like integers or floating-point numbers. There are two main types of endianess: Big-endian and Little-endian.\n\nBig-endian: In this format, the most significant byte (the one with the highest address) is stored first. That means the leftmost byte is stored at the lowest memory address. It’s like reading a number from left to right.\nLittle-endian: Conversely, in little-endian format, the least significant byte (the one with the lowest address) is stored first. The rightmost byte is stored at the lowest memory address. It’s like reading a number from right to left.\n\nFor example, let’s take a 32-bit integer 0x12345678:\n\nIn Big-endian:\n\nMemory: 12 34 56 78\nAddresses: 0x00 0x01 0x02 0x03\n\n\nIn Little-endian:\n\nMemory: 78 56 34 12\nAddresses: 0x00 0x01 0x02 0x03\n\n\n\nThe choice between big-endian and little-endian is important in systems where data needs to be communicated between different architectures, as interpreting multi-byte data requires knowing the byte order. Some architectures use big-endian (e.g., PowerPC, SPARC), while others use little-endian (e.g., x86, ARM).\n\n**序列化和反序列化：**将数据结构转换为字节流（序列化）或从字节流重建数据结构（反序列化）时，字节顺序可能会变得相关。序列化库或协议可能会隐式或显式指定或处理字节顺序。\n网络传输协议通常定义为bigEndian\n文件读取保存\n**跨平台兼容性：**开发在不同架构上运行的应用程序可能需要注意字节顺序，以确保在系统之间共享数据时数据的一致性和正确性。\n\n许多现代高级编程语言通过提供处理数据表示和转换的标准化方法，在一定程度上抽象了字节顺序问题。例如：\n\nPython、Java 或 C# 等语言提供了显式处理字节排序的库或方法。在Java中，java.nio.ByteBuffer 类可以用于处理字节顺序。它提供了 order() 方法，允许你设置字节顺序为大端或小端。\n序列化库通常在内部处理字节序，将其从开发人员手中抽象出来。\n标准化网络协议定义了字节顺序约定（例如，HTTP 对某些数据使用大端字节序）。\n\n内存泄漏（Memory Leak）：\n内存泄漏是指计算机程序中分配的内存空间在不再需要时未被释放，导致系统中的可用内存持续减少。当程序中的内存泄漏严重时，最终可能会导致系统性能下降、程序崩溃甚至系统崩溃。内存泄漏通常由编程错误或设计问题引起，例如忘记释放动态分配的内存、循环引用、指针问题等。为了解决内存泄漏问题，程序员需要定期检查和清理不再使用的内存，或者使用专门的工具来帮助识别和解决内存泄漏。\n指针本质上可以在整个OS允许的内存块上任意移动，有时候还会跨界到其他内存块上去。本质上它离机器语言太近，能够造成非常巨大的外延性破坏。一个最经典的例子就是内存践踏造成的缓冲区溢出。\nJava语言中没有明确的指针概念，它使用引用（reference）来操作对象。这是为了提高安全性和简化开发，因为指针容易导致内存泄漏、越界访问和悬空指针等问题。Java的对象引用是由Java虚拟机（JVM）管理的，这种方式有助于减少对内存的直接操作，从而提高了安全性和可靠性。\nC++ 利用 智能指针达成的效果是：一旦某对象不再被引用，系统刻不容缓，立刻回收内存\n一个解决空悬指针的办法是，引入一层间接性，让 p1 和 p2 所指的对象永久有 效。\n在Java中，常见的内存泄漏情况包括：\n\n长期持有对象的引用： 如果一个对象被分配了内存，并被存储在某个全局变量、静态变量、集合或缓存中，但在后续的程序执行过程中未释放对该对象的引用，即使该对象不再需要，也无法被垃圾回收。\n监听器未及时移除： 当一个对象作为监听器注册到某个事件上，但在对象不再需要时，未取消注册或者移除对监听器的引用，这可能导致对象无法被垃圾回收。\n未关闭资源： 例如打开了文件、数据库连接、网络连接等资源，但在使用完后未显式地关闭或释放这些资源，会导致资源泄漏，进而可能导致内存泄漏。\n循环引用： 当两个或多个对象相互引用，并且这些对象之间形成了一个环形的引用结构，即使这些对象在外部不再被使用，但由于它们互相引用导致它们之间的引用计数不为零，无法被垃圾回收。\n\n为避免内存泄漏，需要进行良好的内存管理和编程实践：\n\n及时释放不再需要的对象引用，可以手动将对象引用置为null。\n在使用完资源后及时关闭文件、数据库连接、网络连接等。\n避免循环引用，使用弱引用或软引用来避免形成永久性的对象引用。\n对于监听器等注册的对象，及时取消注册或移除引用。\n\nGolang:\nvar s0 string // 一个包级变量// 一个演示目的函数。func f(s1 string) {    // s0 s1 共享同一块内存空间，s0正在被使用所以不会    //释放，我们只使用了50字节的内存还有2^20-50未被使用，造成很大的内存泄漏    s0 = s1[:50]    // Solution: So，有较大范围的内存泄漏时  创建s1的一个副本的子串，而不是直接引用s1的子串。    s0 = string([]byte(s1[:50]))}func demo() {    s := createStringWithLengthOnHeap(1 &lt;&lt; 20) // 1M bytes    f(s)}func createStringWithLengthOnHeap(length int) string {    // Create a byte slice of the specified length    strBytes := make([]byte, length)    fillChar := 'a'    for i := range strBytes {        strBytes[i] = byte(fillChar)    }    // Convert the byte slice to a string and return    return string(strBytes)}\n进程间通信IPC\n可选用的6种方法\n\n\n管道 Linux管道是一种特殊的文件描述符的「|」竖线就是一个管道，它的功能是将前一个命令（ps auxf）的输出，作为后一个命令（grep mysql）的输入，从这功能描述，可以看出管道传输数据是单向的，如果想相互通信，我们需要创建两个管道才行。管道是通过调用 pipe 函数创建的，fd[0] 用于读，fd[1] 用于写。Channel是Go语言中的一种特殊类型，用于在Goroutine之间进行通信和同步。\n\n\n消息队列（Message Queue）：以上三种方式只适合传递传递少量信息，POSIX 标准中 定义了消息队列用于进程间数据量较多小数据的通信。进程可以向队列添加消息，被赋予读权 限的进程则可以从队列消费消息。消息队列克服了信号承载信息量少，管道只能用于无 格式字节流以及缓冲区大小受限等缺点，但实时性相对受限。\n\n\n信号（Signal）：信号用于通知目标进程有某种事件发生，除了用于进程间通信外，进 程还可以发送信号给进程自身。信号的典型应用是 kill 命令  kill -l运行在 shell 终端的进程，我们可以通过键盘输入某些组合键的时候，给进程发送信号。例如Ctrl+C 产生 SIGINT 信号，表示终止该进程； - Ctrl+Z 产生 SIGTSTP 信号，表示停止该进程，但还未结束； 如果进程在后台运行，可以通过 kill 命令的方式给进程发送信号，但前提需要知道运行中的进程 PID 号，\n例如： - kill -9 1050 ，表示给 PID 为 1050 的进程发送 SIGKILL 信号，用来立即结束该进程； 所以，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令）。 信号是进程间通信机制中唯一的异步通信机制\n\n\n信号量（Semaphore）：为了防止多进程竞争共享资源，互斥和同步问题。信号量就实现了这一保护机制。PV操作，信号量用于两个进程之间同步协作手段，它相当于操作系统提 供的一个特殊变量，程序可以在上面进行 wait() 和 notify() 操作。进程 A 在访问共享内存前，先执行了 P 操作，由于信号量的初始值为 1，故在进程 A 执行 P 操作后信号量变为 0，表示共享资源可用，于是进程 A 就可以访问共享内存。 若此时，进程 B 也想访问共享内存，执行了 P 操作，结果信号量变为了 -1，这就意味着临界资源已被占用，因此进程 B 被阻塞。 直到进程 A 访问完共享内存，才会执行 V 操作，使得信号量恢复为 0，接着就会唤醒阻塞中的线程 B，使得进程 B 可以访问共享内存，最后完成共享内存的访问后，执行 V 操作，使信号量恢复到初始值 1。\n\n\n共享内存（Shared Memory）：允许多个进程访问同一块公共的内存空间，这是效率最高的进程间通信形式。原本每个进程的内存地址空间都是相互隔离的，但操作系统提供了让进程主动创建、映射、分离、控制某一块内存的程序接口。当一块内存被多进程共享时，各个进程往往会与其它通信机制，譬如信号量结合使用，来达到进程间同步及互斥的协调操作。\n\n\n套接字接口（Socket）RPc：消息队列和共享内存只适合单机多进程间的通信，套接字接口 是更为普适的进程间通信机制，可用于不同机器之间的进程通信。套接字（Socket）起 初是由 UNIX 系统的 BSD 分支开发出来的，现在已经移植到所有主流的操作系统上。\n\n\n当仅限于本机进程间通信时，套接字接口是被优化过的，不会经过网络协议栈，不需要打包拆包、计算校验和、维护序号和应答等操作，只是简单地将应用层数 据从一个进程拷贝到另一个进程，这种进程间通信方式有个专名的名称：UNIX Domain Socket，又叫做 IPC Socket。\n\n只支持半双工通信： 意味着数据传输是单向交替的，通信的一方既是发送者也是接收者，但同一时间只能进行一种操作。\n仅限于父子进程或兄弟进程之间使用： 由于UNIX Domain Socket是本地通信的一种方式，因此通常用于同一台机器上的进程间通信，比如父子进程或兄弟进程之间。\n\n\n空分复用技术（Swapping）： 中。这种技术能够有效地扩展可用内存的大小。空分复用需要快速的内存地址映射，使得操作系统能够迅速将虚拟内存地址转换为实际物理内存地址。这种映射是由CPU中的存储器管理单元（Memory Management Unit，MMU）负责完成的。\n时分复用技术（Time Division Multiplexing，TDM）： 时分复用是一种通信技术，用于多路复用，让多个用户或信号共享同一个通信信道或资源。在操作系统中，时分复用也可以被理解为在单个CPU上执行多个进程的技术。这种技术会分割时间成为多个时间片（Time Slice），每个时间片分配给不同的进程，使得它们轮流占用CPU并执行。这种技术可以使系统中的多个进程表现出并发性，尽管实际上在同一时间点只有一个进程在执行。这种并发性是通过快速的进程切换和调度实现的。\n\n多线程/多进程解决了阻塞问题\n线程是抢占式，而协程是非抢占式的，所以需要用户自己释放使用权来切换到其他协程，因此同一时间其实只有一个协程拥有运行权，相当于单线程的能力。\n协程并不是取代线程, 而且抽象于线程之上, 线程是被分割的CPU资源, 协程是组织好的代码流程, 协程需要线程来承载运行, 线程是协程的资源, 但协程不会直接使用线程, 协程直接利用的是执行器(Interceptor), 执行器可以关联任意线程或线程池, 可以使当前线程, UI线程, 或新建新程.。\n线程是协程的资源。协程通过Interceptor来间接使用线程这个资源。\n内核态和用户态\n内核kernel是程序，它需要运行，就必须被分配 CPU。因此，CPU 上会运行两种程序，一种是操作系统的内核程序（也称为系统程序），一种是应用程序。前者完成系统任务，后者实现应用任务。两者之间有控制和被控制的关系，前者有权管理和分配资源，而后者只能向系统申请使用资源。\n您对内核态（Kernel Mode）和用户态（User Mode）的描述非常准确。以下是进一步的详细解释和一些补充信息：\n内核态（Kernel Mode）\n\n权限：在内核态中，运行的代码拥有最高的权限，可以访问系统的所有资源，包括硬件和所有内存空间。这意味着内核可以执行任何CPU指令，管理系统的所有硬件资源，如I/O设备、CPU、内存等。\n用途：\n\n系统调用：当用户态的程序需要执行需要高权限的操作（如I/O操作、进程管理等），它通过系统调用陷入内核态。\n硬件中断处理：硬件中断发生时，系统会切换到内核态来处理中断。\n内存管理：内核负责管理虚拟内存到物理内存的映射，保护内存空间不被非法访问。\n\n\n风险：由于内核态的代码有如此高的权限，任何错误或漏洞都可能导致系统崩溃或安全性问题。\n\n用户态（User Mode）\n\n权限：在用户态，程序的执行权限受到限制。它们只能访问由内核分配和管理的内存空间，并且只能通过系统调用来请求内核执行高权限操作。\n用途：\n\n应用程序执行：所有用户应用程序如浏览器、文本编辑器等都在用户态运行。\n安全性：限制用户态程序的权限可以保护系统的整体稳定性和安全性，防止一个程序恶意或错误地影响整个系统。\n\n\n内存访问：用户态下的程序只能访问那些在其进程的虚拟地址空间中被映射的内存页。试图访问未映射或无权限的内存会导致保护错误（如段错误）。\n系统调用：当用户态程序需要进行需要内核权限的操作时（例如读取文件、网络通信等），它必须通过系统调用接口。这时，CPU会切换到内核态，执行相应的内核代码，完成操作后再返回到用户态。\n\n切换机制\n\n从用户态到内核态：主要通过以下方式：\n\n系统调用（如 fork, exec, read, write 等）。\n中断：如时钟中断，硬件设备中断。\n异常：如页面错误（page fault），除零错误等。\n\n\n从内核态返回用户态：当内核完成其任务后，通过特定的CPU指令（如在x86架构上的 iret 或 sysret）返回到用户态，恢复之前的执行上下文。\n\n这种分离的设计有助于系统的稳定性和安全性，因为它限制了用户程序直接操作硬件的能力，从而保护了系统的核心功能不受用户程序的错误或恶意行为影响。\nLinux\nLinus Torvalds 创建。GNU/LInux是全球数百万开发人员之间的协作，涉及GNU 项目, Linux 内核开发团队由 Torvalds 领导， X Window 系统的各种开发人员在过去的 29 年中，以及其他。这就是自由软件基金会要求将使用来自 GNU 项目的软件的完整 Linux 操作系统称为“GNU/Linux”的原因。\nMany open source developers agree that the Linux kernel was not designed but rather evolved through natural selection. Torvalds considers that although the design of Unix served as a scaffolding, “Linux grew with a lot of mutations – and because the mutations were less than random, they were faster and more directed than alpha-particles in DNA.”[73] Eric S. Raymond considers Linux’s revolutionary aspects to be social, not technical: before Linux, complex software was designed carefully by small groups, but “Linux evolved in a completely different way. From nearly the beginning, it was rather casually hacked on by huge numbers of volunteers coordinating only through the Internet. Quality was maintained not by rigid standards or autocracy but by the naively simple strategy of releasing every week and getting feedback from hundreds of users within days, creating a sort of rapid Darwinian selection on the mutations introduced by developers.”[74] Bryan Cantrill, an engineer of a competing OS, agrees that “Linux wasn’t designed, it evolved”, but considers this to be a limitation, proposing that some features, especially those related to security,[75] cannot be evolved into, “this is not a biological system at the end of the day, it’s a software system.”\n全称GNU/Linux，是一种免费使用和自由传播的类UNIX操作系统，其内核由林纳斯·本纳第克特·托瓦兹于1991年10月5日首次发布，它主要受到Minix和Unix思想的启发，是一个基于POSIX的多用户、多任务、支持多线程和多CPU的操作系统。它能运行主要的Unix工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。Linux有上百种不同的发行版，如基于社区开发的debian、archlinux，和基于商业开发的Red Hat Enterprise Linux、SUSE、Oracle Linux等。\nLinus built kernel to solve unix hard-use and lack I/O RPC function.  区分 Linux（内核）和 Linux（操作系统）。\nKernel 是计算机操作系统核心的计算机程序，通常可以完全控制系统中的所有内容**。**[1]它是操作系统代码的一部分，始终驻留在内存中[2]并促进硬件和软件组件之间的交互。完整的内核通过设备驱动程序控制所有硬件资源（例如 I/O、内存、密码） ，仲裁涉及这些资源的进程之间的冲突，并优化公共资源的利用，例如 CPU 和缓存使用、文件系统和网络套接字。在大多数系统上，内核是启动时最先加载的程序之一（在引导加载程序）。它处理其余的启动以及内存、外围设备和来自软件的输入/输出(I/O) 请求，将它们转换为中央处理器的数据处理指令。\n内核是操作系统的核心部分，具有完全控制系统资源的能力。它始终驻留在内存中，促进硬件和软件之间的交互。内核通过设备驱动程序控制硬件资源，仲裁进程之间的冲突，并优化共享资源的利用。在大多数系统上，内核是启动时最先加载的程序。\n内核的关键代码被加载到受保护的内核空间中，与应用程序运行的用户空间分离。这种分离可以防止用户数据和内核数据相互干扰，并提高系统的稳定性和安全性。内核提供了一个低级的接口，进程通过系统调用来请求内核服务。\n内核架构有整体内核和微内核两种设计。整体内核在单个地址空间中运行，以提高速度；微内核在用户空间中运行大部分服务，以提高灵活性和模块化。Linux内核是整体式的，但也支持模块化，可以在运行时加载和卸载内核模块。\n内核负责决定将哪些正在运行的程序分配给处理器，从而执行程序。它是计算机系统的中央组件，控制和管理系统的各个方面。\nLinux 操作系统将其运行环境分为两种状态：用户态和内核态。用户态是指应用程序运行的环境，而内核态是指操作系统内核运行的环境。\n用户态\n用户态（User Mode）是指应用程序运行的环境。在用户态下，应用程序可以使用用户态提供的系统调用接口来请求操作系统服务。用户态的应用程序不能直接访问硬件资源，必须通过系统调用来请求内核态的服务。\n用户态的特点：\n\n应用程序运行在用户态下\n不允许直接访问硬件资源\n必须通过系统调用来请求内核态服务\n\n内核态\n内核态（Kernel Mode）是指操作系统内核运行的环境。在内核态下，内核可以直接访问硬件资源，管理系统资源，并提供服务给用户态的应用程序。\n内核态的特点：\n\n操作系统内核运行在内核态下\n允许直接访问硬件资源\n管理系统资源并提供服务给用户态应用程序\n\n系统调用\n系统调用（System Call）是用户态应用程序请求内核态服务的接口。用户态应用程序通过系统调用来请求内核态的服务，例如读取文件、创建进程等。\n系统调用的过程：\n\n应用程序在用户态下执行\n应用程序需要请求内核态服务\n应用程序通过系统调用接口请求服务\n内核态接收到请求并处理\n内核态将结果返回给应用程序\n\n实践\nLibs\n\n系统管理\n\nsystemd: 系统和服务管理器\ncron: 定时任务管理\ntop/htop: 系统资源监控\nps: 进程状态查看\njournalctl: 日志查看\ndmesg: 内核日志查看\n\n\n文件管理\n\nls, cp, mv, rm: 基本文件操作\nfind: 文件搜索\ngrep: 文本搜索\ntar, gzip, bzip2: 文件压缩和归档\nchmod, chown: 文件权限管理\n\n\n文本处理\n\nvim, nano: 文本编辑器\nsed, awk: 文本处理工具\ncat, less, more: 文件查看\n\n\n网络工具\n\nifconfig/ip: 网络接口配置\nping, traceroute: 网络诊断\nssh: 远程登录\ncurl, wget: 文件下载\nnetstat, ss: 网络连接状态\niptables/nftables: 防火墙管理\n\n\n\nSystemd 是许多现代 Linux 发行版中使用的初始化系统和系统管理器。以下是关于 systemd 的一些重要概念和常用命令：\n\n核心概念：\n\nUnit：systemd 管理的基本对象，包括服务、挂载点、设备和网络配置等\nService：最常见的 unit 类型，用于管理守护进程\nTarget：一组 unit 的集合，类似于运行级别\n\n\n\nUnit 文件结构\n一个典型的 .service 单元文件结构如下：\n`[Unit]Description=My Custom ServiceAfter=network.target[Service]ExecStart=/usr/bin/my-serviceExecReload=/bin/kill -HUP $MAINPIDExecStop=/bin/kill $MAINPIDRestart=on-failure[Install]WantedBy=multi-user.target``[Unit]`：描述单元和其依赖关系。`[Service]`：定义服务如何启动、停止，以及重启策略。`[Install]`：定义单元如何被启用或关联到其他目标。\n注意事项\n权限：许多 systemctl 命令需要以 root 权限运行。\n配置修改后重载：修改 .service 文件后，需要运行 systemctl daemon-reload 来重新加载配置。\n安全性：systemd 提供了许多安全选项，可以限制服务的权限和资源使用。\n$表明是非root用户登录，#表示是root用户登录，它们是终端shell的命令提示符 几种常用终端的命令提示符\nBASH: root账户: # ,非root账户: You can't use 'macro parameter character #' in math mode KSH: root账户: # ,非root账户:  CSH[TCSH]: root账户: % ,非root账户: %\n而/ 是根节点， ~ 是 home如果以root账号登陆   ~ 是 /root/\nProgram: a file containing instructions to be executed, static Process: an instance of a program in execution, live entity\nexecve（执行文件）在父进程中fork一个子进程，在子进程中调用exec函数启动新的程序。exec函数一共有六个，其中execve为内核级系统调用，其他（execl，execle，execlp，execv，execvp）exec(): often used after fork() to load another process\n许多程序需要开机启动。它们在Windows叫做\"服务\"（service），在Linux就叫做\"守护进程\"（daemon）。Linux Daemon（守护进程）是运行在后台的一种特殊进程。它独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。\n\nsyslogd：系统日志守护进程，负责记录系统日志消息。\nhttpd：Web 服务器守护进程，如 Apache 或 Nginx，用于提供 Web 服务。\nsendmail：邮件服务器守护进程，负责处理发送和接收邮件的功能。\nmysqld：数据库服务器守护进程，比如 MySQL 或 MariaDB，用于管理和提供数据库服务。\n\n这些守护进程通常在系统启动时自动启动，并在系统运行期间持续监听、处理或提供服务。它们的运行通常不需要用户的直接干预，而是通过配置文件或其他设置来控制其行为和功能。\nFTP\n主动模式的FTP是指服务器主动连接客户端的数据端口，被动模式的FTP是指服务器被动等待客户端连接自己的数据端口。\n被动模式的FTP通常用在处于防火墙之后的FTP客户访问外界FTP服务器的情况，因为在这种情况下，防火墙通常配置为不允许外界访问防火墙之内的主机，而只允许由防火墙之内的主机发起对外的连接请求。因此，在这种情况下不能使用主动模式的FTP传输，而被动模式的FTP可以良好的工作。\n主动模式需要服务器主动向客户端发起连接，而现在普通的客户端大多位于NAT之后，所以主动模式常常无法进行。即使可以，也需要客户端打开防火墙，允许服务端的20端口访问。所以服务端基本需要支持被动模式，但被动模式需要开放一段端口。为了安全，可以选择一小段端口，然后在防火墙开放这一段端口。\nEverything is a file\nLinux操作系统的设计哲学之一：将所有设备、资源或进程都视为文件或文件类似的对象。\n在Linux中，不仅普通的文本文件、目录、硬件设备和外部设备都被视为文件，甚至系统中的进程、网络连接、硬件接口等也被抽象为文件。这种抽象化使得Linux系统更加统一和灵活，因为它允许对不同资源使用相似的操作方式。\n好处是读写这些资源都可用open()/close()/write()/read()等函数进行处理。屏蔽了硬件的区别，所有设备都抽象成文件，提供统一的接口给用户。虽然类型各不相同，但是对其提供的却是同一套API。更进一步，对文件的操作也可以跨文件系统执行。在Linux系统中有三类文件：普通文件、目录文件和特殊文件。\n网络 I/O 通过网络进行数据传输，通常包括 TCP 或 UDP 等协议。网络 I/O 的延迟较高，取决于网络带宽、路由器、中继设备等，数据包可能需要经过多个中间节点才能到达目标服务器。\n局域网的延迟通常在微秒到毫秒级，而广域网（如互联网）的延迟可能会达到几十毫秒甚至更高。网络 I/O 并发是非常常见的需求，特别是在高并发服务器（如 Web 服务器）中。服务器通常需要同时处理数百甚至数千个网络连接。为了提高性能，使用了 I/O 多路复用（select、poll、epoll）和异步 I/O 来管理大量的并发连接。网络 I/O 带宽受到网络设备（如网卡、路由器）的限制。千兆以太网的带宽是 1 Gbps，大约相当于 125 MB/s，而如今常见的 10 Gbps 网卡能达到 1.25 GB/s，远低于本地存储设备。\n现代服务器（如 Nginx、Node.js）通常使用异步非阻塞 I/O 模型，以最大限度提高 I/O 吞吐量。\n","categories":["计算机科学"],"tags":["计算机基础","操作系统"]},{"title":"How blockchain evolves to a new world order?","url":"/2025/06/08/blockchain-overview/","content":"A new world order\nIn the world of crypto, few figures have shaped its philosophical and technical trajectory as deeply as Vitalik Buterin, the co-founder of Ethereum. Beyond the whitepaper that birthed the world’s most programmable blockchain, Vitalik’s prolific blog output over the years has become a canon for developers, economists, and visionaries in the decentralized ecosystem.\nFor builders—especially those designing agent platforms, decentralized governance models, or crypto-powered coordination systems—these writings form a strategic blueprint. Build systems that are not only redundant in infrastructure but also politically and logically anti-fragile. Consider multisig governance, on-chain voting, and modular designs.\nTotal Value Locked (TVL) on Ethereum stands at $61.3 billion, dwarfing competitors—Ethereum holds ~45% of DeFi capital versus Solana’s ~5% (source).\nLiquid staking (LST) on Ethereum locks 13.7 million ETH (~$34.4B).\nETH chain records ~$676K/day in fees and nearly $1 billion in 24-hour DEX volume (source).\nSee Vitalik’s recent blogs:\nThe near and mid-term future of improving Ethereum\nPossible futures of Ethereum, Part 2\nThis scope matters.\nDefi hatched\nEnabling bold scaling ideas and ecosystem coordination requires such scale. It’s the bedrock on which Vitalik’s decentralization blueprint stands.\nThese numbers matter not as vanity metrics but as hard evidence: much of what Vitalik Buterin proposes in his blog writings—modular scaling, credible neutrality, permissionless governance—depends on Ethereum sustaining this role as the settlement and security layer for global crypto coordination. When Vitalik speaks about “The Meaning of Decentralization,” he doesn’t just refer to network architecture but to the political and epistemic dimensions of control: who defines protocol values, who updates client software, and who can contest outcomes. That meaning only gains traction when backed by thousands of actively verifying nodes, diverse client implementations, and real capital at stake.\nProjects like Uniswap and other DEXs didn’t just emerge spontaneously—they were incubated in a unique intersection of open research, early crypto capital, and public infrastructure built on Ethereum. Understanding how they were hatched and funded reveals the mechanics of permissionless innovation, the catalytic role of public goods, and the quiet but powerful function of narrative leverage.\nUniswap began as a Reddit comment by Vitalik Buterin in 2016, suggesting the idea of an automated market maker (AMM) based on constant product formulas. Hayden Adams, an unemployed mechanical engineer at the time, decided to implement the idea in 2017, teaching himself Solidity and building the protocol’s early version. He received a $65,000 grant from the Ethereum Foundation in 2018, which helped him finish v1. Notably, Uniswap was one of the first examples of a successful public good funded entirely by EF grants before a token existed.\nThat original grant served a catalytic role, but the real growth came when Uniswap launched its protocol and began attracting liquidity. Paradigm, one of crypto’s leading VC firms, later invested in Uniswap Labs, helping it scale from a pure protocol to a company with developers, designers, and BD capacity. Paradigm reportedly led a $11M Series A round in 2020. Still, even as a venture-backed company, Uniswap the protocol has remained free and open—anyone can fork the contracts and build on top. This public nature was essential to its early network effects.\nThe larger ecosystem of DEXs followed similar but slightly varied paths. Curve Finance, another AMM optimized for stablecoin swaps, was started by Michael Egorov, a physicist and software engineer. Unlike Uniswap, Curve focused early on tokenomics, launching the CRV token with an intricate governance and staking model. It began with relatively low initial funding, but quickly grew via liquidity mining, incentivizing users to provide capital.\nSo, the DeFi ecosystem was not built with the permission of capital allocators—it emerged from a combination of ideas freely shared, public infrastructure like Ethereum and IPFS, and mechanisms like grants, liquidity mining, and retroactive public goods funding. What makes it novel is not just the technology, but how the capital formation and innovation pipeline itself was decentralized.\nPhilosophy and vision\nWhen I look at his posts, I think this is common for geniuses - it’s not so much about being brilliant and solving whatever is at hand as it is a ton of hard work reading, writing (to clarify ideas) and brainstorm with others any thing that pops up in your head that may be relevant or applicable. There’s always something useful that comes out of it and eventually those things accumulate to what you, and others, iterate into working solutions.\n itself is an intentional 50/50 mix of idealism and “describing reality”. It’s intended to show four major constituencies of the ecosystem that can have a supportive and symbiotic relationship with each other. Many crypto institutions in practice are a mix of all four.\nEach of the four parts has something key to offer to the machine as a whole:\nToken holders and defi users contribute greatly to financing the whole thing, which has been key to getting technologies like consensus algorithms and zero-knowledge proofs to production quality.\nIntellectuals provide the ideas to make sure that the space is actually doing something meaningful.\nBuilders bridge the gap and try to build applications that serve users and put the ideas into practice.\nPragmatic users are the people we are ultimately serving.\nRemember that today’s blockchains are by definition developed by people with no prior blockchain experience. We all learned what we know from just digging into it.\nSo if you want to understand where blockchains are going, don’t start with whitepapers or Twitter threads. Look at Vitalik’s posts—but read them through the lens of what the data says. The writing is not a prediction. It’s a description of what’s already unfolding. If you’re building in Web3, the frontier is more than technical. It’s moral, economic, and social. Vitalik’s blog is not just a technical archive; it’s a guide to building a new kind of world. Read it not just as a reference—but as a call to action.\n“The tools we build today encode the freedoms or limitations of tomorrow.” — V.B.\nFurther Reading: vitalik.ca\nThe following are Vitalik’s most influential and discussed blog posts, categorized by topic and with brief analysis, I highly recommend:\n\nThe Meaning of Decentralization (2017)\nKeywords: logical decentralization, political decentralization, architectural decentralization\n\n\n\nExplained that decentralization is not just about node distribution, but also includes “who controls what” and “distribution of failure points”.\n\n\nIt has been cited countless times and is the philosophical starting point of the decentralized narrative of Web3.\n\n\n\nEthereum: Platform for Smart Contracts (2014)\n\n\n\nNot a blog, but equally important. This is the Ethereum white paper he wrote, which proposes the vision of “world computer”.\n\n\nIf you haven’t read it, many core designs of Ethereum are difficult to understand.\n\n\n\nWhy sharding is great: demystifying the technical properties\n\n\n\nA popular explanation of the design ideas of Ethereum sharding.\n\n\nIt is crucial to understand the evolution roadmap of Ethereum scalability.\n\n\n\nDAOs are not corporations: where decentralization in autonomous organizations matters\n\n\n\nIn-depth analysis of the differences between DAO governance models and traditional companies.\n\n\nDAO is more suitable in scenarios of “market externalities”, “high collaboration” and “high divergence”.\n\n\n\nQuadratic Funding and Quadratic Voting\n\n\n\nPromote the concept of the secondary mechanism of Radical Markets and conduct experiments in combination with Gitcoin.\n\n\nEmphasize that the strong preferences of a minority can be expressed fairly to avoid violent voting by large households.\n\n\n\nCredible Neutrality as a Guiding Principle\n\n\nProposed “credible neutrality” (Credible Neutrality), applicable to chains, oracles, and protocol rules.\nOne of the most recognized design philosophies in the crypto field.\n\n\nAn Incomplete Guide to Rollups\n\n\nSystematically summarizes the pros and cons of optimistic rollups and zk rollups.\nPromoted Ethereum’s “modular blockchain” paradigm shift.\n\n\nEndgame (2021)\n\n\nPropose the scalability, security, and endgame of centralized game of future blockchain architecture.\nAssume that there is a super decentralized blockchain, but it must also rely on centralized building blocks (like sequencers).\nDeeply reveal the future direction of L1-L2.\n\n","tags":["blockchain"]},{"title":"进步的幻象：生产力的发展带来了什么","url":"/2025/04/07/capitalism/","content":"进步的幻象\n生产力的进步，通常被视为人类社会发展的核心动力。它意味着用更少的资源和时间创造更多的价值，理应带来更高效的生产、更富裕的生活和更多的闲暇。然而，现实却常常呈现出一种令人困惑甚至绝望的景象：随着技术的飞速发展和生产效率的指数级提高，许多人非但没有因此获得解放，反而感到更加劳碌、压力更大。如果生产力的进步只是让人更加疲惫不堪，那么这种进步的意义究竟何在？我们是否陷入了一种“进步的幻象”，重复着历史的错误，却始终不愿吸取教训？\n这种令人沮丧的悖论并非新鲜事。回顾历史，我们不难发现，生产力进步与劳动者处境之间的关系远非简单的线性正相关。以20世纪30年代的大萧条为例。当时，美国的工厂已经拥有了惊人的生产能力，可以大量制造汽车、收音机等商品。从生产力角度看，这是一个巨大的飞跃。然而，这种进步却伴随着史无前例的经济崩溃。生产过剩与有效需求不足并存，工厂倒闭，数百万人失业，社会陷入贫困和混乱。生产力的高度发达并没有转化为普遍的繁荣和闲暇，反而导致了大规模的苦难。这深刻地揭示了：生产力进步本身并非目的，其价值取决于其成果如何被分配和管理。如果进步的果实主要被少数人攫取，而多数人却因技术进步带来的结构性失业或市场波动而陷入困境，那么这种“进步”对大多数人而言就毫无意义，甚至是一种灾难。\n1. How to turn labor into Relentless Machine?\n随着机器化大工业蓬勃发展，资本主义价值运动的一个必然趋势就是，“对象化劳动在劳动过程本身中与活劳动相对立而成为支配活劳动的力量”；与此同时，活劳动则转变为机器系统中“单纯的活的附件”。如此一来，从价值形成的生产过程看，资本在追逐自我增殖过程中，不可避免地引致价值实体的“空壳化”。形象地讲，这好比资本主义价值运动存在“黑洞”，只吞噬旧价值，却无法吐出新价值。正如美国学者大卫·哈维所指出的，“如果活劳动是价值和利润的源泉，那么用死劳动或机械劳动代替活劳动在政治上和经济上都毫无意义。在马克思看来，这是资本主义的一个核心矛盾”。\n　　回顾资本主义生产史，在福特制下，科学技术的资本主义应用不断推进生产标准化和自动化，实现人机关系的根本翻转，“在工场手工业和手工业中，是工人利用工具，在工厂中，是工人服侍机器”，但劳动作为“有意识的机件”依然不可或缺。随着资本主义的发展，非物质劳动展演于人们的日常沟通、社会合作和情感交流之中，资本一时难以实现对其规训与吸纳，遑论利用“能动的机器体系”来取而代之。进入数字资本主义阶段，资本主义智能化生产步入快车道，极大地拓展了“机器换人”的操作边界，生产过程中直接劳动加速缩减。但数字经济实践表明，在分析式人工智能技术驱动下，智能增强的数字机器不仅囿于特定作业领域，而且日渐曝光的“幽灵工作”时刻昭示：所谓的“无人化”“零劳动”“后工作”依然只是技术乐观派的一种数字乌托邦想象。\n　　其实，在马克思语境下，即使资本获得最完善、最适应的机器体系，即自动的机器体系，也不意味着劳动的彻底退出，而是作为一个环节“分布在机械体系的许多点上”。并且，在马克思看来，“一个在本生产部门内完全不使用可变资本，因而完全不使用工人的资本家”只是一个极端的假定。如上文所述，伴随资本主义生产从机械化到自动化再到智能化，劳动形式也从物质劳动到非物质劳动再到数字劳动，创造价值的活劳动从来就没有真正离场。“蒸汽机、纺织机、电气化和电子化从未持续减少劳动量，相反更加大了劳动量。”但万事因时而变。大模型生产落地后，在通用人工智能的技术加持下，数字机器的决策自主和能力泛化正在以超乎常人想象的节奏飞速提升。撇开“超级智能”“机器意识”等极具争议性话题，从资本主义生产过程看，当今数字机器展现持续迭代的“高阶自动化”能力，正在使越来越多的认知劳动被超高效的智能算法所替代。可以说，“以往的工业自动化系统或智能机械解放的主要是人的体力，取代的是体力劳动者，但ChatGPT之类通用人工智能正在颠覆人的脑力劳动，取代‘脑力劳动者’承担的各种工作”。\n　　面对工业时代机器系统的巨大力量，马克思曾惊呼：“如果机器消灭了整个雇佣工人阶级，那么这对资本来说将是一件十分可怕的事情，因为资本没有雇佣劳动就不成其为资本了！”毋庸置疑，大模型时代数字机器所展示的生产威力，使迄今为止所有的工业机器都相形见绌。随着通用人工智能的技术精进和场景应用，“现在资本正把‘活劳动’的基本能力——认知和感知——重塑为适合资本的机器形式，它致力于加紧对这些基础能力的捕获并加快转化于‘分布式’的智能体系中”。照此趋势发展下去，一个不容回避的理论追问是，就连“站在生产过程的旁边”照料机器的机会，留给普通劳动者的还会有多少？\n2.　大模型生产放大价值运动的“市场悖反”\n在政治经济学语境下，价值反映了商品经济条件下人们相互交换劳动的社会关系。它滥觞于社会分工下私人劳动和社会劳动的对立，成形于商品生产和商品流通的统一。换言之，作为一种社会确证，价值以交换价值为中介实现了具体劳动向抽象劳动、私人劳动向社会劳动的双重转换。进而，“劳动作为一般劳动的对象化和作为满足一般需要的[手段的]性质”通过交换被肯定，自然成为维系资本主义价值体系的生死攸关之事了。\n　　从价值实现看，随着资本主义生产力快速发展，商品供给日趋丰富，市场开拓和需求扩张就变得愈发迫切。但数百年资本主义经济发展表明，虽然资本不择手段来开拓国内外市场，但生产相对过剩始终与资本主义社会化大生产如影随形。究其根源，资本自身的运动一直存在无法克服的“市场悖反”：一方面，为了获取价值，资本竭力制造日益丰富的商品供给；另一方面，为了加速自我增殖，资本又极尽贬低劳动力价值之能事。其结果，资本主义的生产扩张和需求萎缩相向而行，造成价值运动中断，引发周期性危机。\n　　这集中体现在：福特制下生产标准化、流水线化，持续推动劳动“去技能化”，“工人变成了机器的单纯的附属品，要求他做的只是极其简单、极其单调和极容易学会的操作”。于是，“由于这种转移，工人自己的劳动能力就贬值了”。后福特制兴起后，资本力推新自由主义的制度革新，持续推动就业不稳定化，越来越多的普通劳动者只能靠消费信贷勉强度日。进入数字资本主义阶段，数字劳动的兴起给劳动者带来新的就业方式和工作机会，但数字劳动的平台化组织和算法化管理，进一步加剧就业不稳定。\n　　随着通用人工智能技术开始普及，数字资本主义千行百业的生产流程都在经历重组和重塑。麦肯锡估计，在63个应用案例中，生成式AI应用每年能给全球经济带来2.6万亿美元至4.4万亿美元的潜在价值。目前来看，数字科技巨头通过数据、算力和算法的全方位垄断，牢牢掌控通用人工智能的技术经济生态，企图独享大模型生产释放的新一轮数字红利。可以肯定的是，数字资本主义现行平台体制如不改弦更张，必然会有越来越多的劳动者在数字化转型中落入“数字穷人”的生存窘境。\n　　这是因为，首先，在大模型生产下，“机器换人”现象持续向资本主义知识生产领域蔓延，拥有知识专长的“创意阶层”将会步蓝领工人的后尘，被无情的却擅于叙事的数字资本抛弃。其次，通用人工智能技术进一步激活数字泰勒主义，数字资本加速拆解人类认知劳动过程，不断将可计算的工作内容标准化、模块化和自动化，从而将资本主义高阶劳动也“去技能化”。可以料想的是，数字资本主义复杂劳动简单化加速，国民收入分配格局中劳动占比持续走低将是必然趋势。\n　　如此一来，一方面，大模型生产实现知识商品的工业化生产，加上通用人工智能技术赋能传统制造提质增效，数字资本主义商品和服务产能将会呈现指数级增长；另一方面，大模型生产将数量惊人的知识型工作者从资本主义直接生产过程排挤出去，致使长期以来支撑发达资本主义国家大众消费的“中产阶级”加速塌陷。其结果，大模型生产进一步放大资本主义生产扩张与需求收缩的“市场悖反”，数字化的资本生产和流通注定要遭遇更加频繁的生产过剩和更加剧烈的市场动荡。\n3. Global Perspective\n从全球供应链的角度来看，美国与中国之间建立了极为复杂的经济联系。美国对中国长期存在巨额贸易逆差，进口远超出口，许多美企依赖中国进行低成本制造，从而保持消费价格的低廉并提升自身利润。虽然近年来部分企业宣布将生产转移回美国本土，但真正实现回流的比例仍然有限。在美国重新设厂和招募熟练劳动力的成本高昂，成为现实操作中的巨大阻力。\n在此背景下，特朗普政府于2018至2020年间推行了关税战略，对数十亿美元的中国产品加征关税，初期聚焦钢铁和铝，后扩展至各类消费品。其既定目标包括减少贸易逆差、提升美国制造业竞争力，推动制造业回流。然而，成效与预期之间存在不小差距。\n与此同时，中国的全球角色也在发生转变。从曾经的“世界工厂”逐渐向价值链上游移动，中国正将发展重点从基础制造转向更复杂的高附加值产业与技术创新，并在全球经济与地缘政治领域展现出更强的自信心。值得注意的是，美国当前所采取的“去风险”（de-risking）战略，区别于全面“脱钩”（decoupling）。这是一种更为审慎的做法，即在关键产业链减少对中国的依赖，但并不完全切断两国之间的经济联系。\n正是在这种全球结构的变化之中，中国国内的劳工现状更深刻地展现出这一增长逻辑的矛盾和非人道。\n中国改革开放以来，依托廉价劳动力和强大的组织动员能力，其生产力取得爆炸式增长，一跃成为全球制造中心，甚至在高科技产业上亦迅速崛起。然而，与之伴随的，却是令人堪忧的劳工现实：普遍盛行的“996”工作制、激烈的内卷文化、制造业与服务业劳动者超长工时与高强度劳动成为常态。大量年轻人乃至中年人身心俱疲，缺乏自主生活的空间与时间，个体生活的质量并未因国家的经济增长而同步提升。 在改革开放后的几十年里，中国的生产力实现了爆炸式增长，成为“世界工厂”，高科技产业也迅速崛起。然而，与此相伴的却是普遍的“996”工作制、激烈的内卷竞争、以及许多制造业和服务业劳动者面临的超长工时和高强度劳动。许多年轻人和中年人感到身心俱疲，缺乏生活的自主权和闲暇时间。尽管整体社会财富大幅增加，但许多劳动者并没有因此获得应有的回报，反而感到被卷入了无休止的竞争和加班之中。生产力的进步似乎只是让资本和技术所有者获得了更多利润，而广大劳动者却依然处于“更加劳碌”的困境。\n更深层的隐忧在于，这种现象实际上加剧了贫富差距的扩大。技术与生产力的进步确实释放了巨大的经济潜能，但这些红利的分配极为不均。掌握资本、技术与信息资源的少数群体能够轻松积累财富，而普通劳动者则在面对自动化、AI等新技术的冲击下议价能力不断下降，工资增长滞缓。最终，一部分人享受着自由与财富，而更多人却在加班与焦虑中苦苦挣扎。\n“History happens over and over again, people dont learn any things!” 这句看似宿命论的感叹，却揭示了一个令人沮丧的现实。从大萧条的教训，到当下中国以及全球范围内的劳工困境和贫富差距，历史似乎在不断重演。人们似乎总是容易被“效率”和“增长”的口号所迷惑，而忽略了进步的最终目的应该是为了人类的福祉。我们拥有了前所未有的生产力，但却没有建立起一套能够公平分配这种生产力的社会机制和伦理准则。我们仍然在以一种原始的、弱肉强食的方式争夺生产力进步带来的利益，而没有认识到真正的进步应该意味着更少的劳动、更多的自由、更平等的社会。\n因此，如果生产力的进步只是导致更严重的剥削、更剧烈的竞争、更巨大的贫富差距，并最终让多数人感到更加劳碌，那么这种进步就失去了其应有的意义。它只是一种技术上的成功，而不是人类文明的进步。衡量进步的标准不应仅仅是GDP的增长或生产效率的提高，更应该是社会整体福祉的提升、劳动者的解放、以及人与人之间更公平的关系。\n生产力的进步本身是中性的，其价值取决于人类如何运用它。我们是否能从大萧条、从当下的劳工困境中吸取教训，不再仅仅追求“多”和“快”，而是追求“好”和“公平”？这需要我们重新审视进步的定义，需要更健全的社会保障体系、更合理的财富分配机制、更以人为本的政策制定。否则，我们可能永远无法走出“进步的幻象”，在无休止的劳碌中，徒劳地追逐着一个看似光鲜却毫无意义的未来。真正的进步，应该让每一个人都能分享其成果，获得更多闲暇、更多尊严，而不是被它驱使着永不停歇。历史的重复是因为人类并没有从过去的错误中吸取教训。我们仍然在重复着同样的错误，仍然在追求利润而不是改善人类的生活。我们需要改变这种思维方式，需要将生产力的进步转化为人类生活的改善。我们需要建立一个更加公平和公正的社会，在这个社会中，工人的劳动成果被尊重和奖励，而不是被剥削。\n如果生产力的进步只是让人更加劳碌，那么这种进步还有什么意义？我们需要反思我们的价值观和社会体系，需要将生产力的进步转化为人类生活的改善。只有这样，我们才能真正地享受生产力的进步带来的好处，而不是重复着历史上的错误。\n","tags":["marxism"]},{"title":"First Post","url":"/2025/12/01/hello-world/","content":"欢迎来到我的博客空间！在这里记录软件开发，学习经验，生活体会，我将与你分享我在特定领域的专业知识、宝贵经验以及个人成长的点滴。写作对我而言，不仅是传递见解的工具，更是记录自己成长旅程的纪念碑。通过这个平台，我希望能够与你建立起一种分享与学习的连接，共同探讨生活中的种种挑战和机遇。\n我深信每个人都有独特的经历和故事，而通过分享和倾听，我们可以在彼此的经验中汲取智慧，更好地理解和应对生活中的各种情境。这个博客旨在成为一个开放、互动的空间，不仅展示我的专业知识，也记录我在生活中的成长历程。无论你是寻找专业建议、学习心得，还是对他人成长故事感兴趣，我都希望这里能够成为你的一个有益资源!\n"},{"title":"JAVA学习路线","url":"/2023/12/07/Programming%20language/","content":"JAVA\nJava 是一种广泛使用的编程语言和计算平台，最初由Sun Microsystems公司（现属于Oracle公司）于1995年发布。Java 以其稳定性、跨平台性和丰富的库和框架而著称，广泛应用于企业级应用、移动应用、Web应用和嵌入式系统等多个领域。Java EE（Java Platform, Enterprise Edition）提供了一套完整的技术栈，用于构建和运行大型的Web应用和服务。\n其跨平台性、面向对象编程特性、丰富的库和框架、高安全性和高性能，使得Java在过去的几十年中一直保持着重要的地位。对于开发者来说，掌握Java可以带来广阔的职业发展机会和技术成长空间。\n但并不是所有的工程和环境需要企业等级的复杂性，比如一个简单的个人网站或者独自编程的程序师所写的程序。这些程序师会发现Java的复杂管理对于自己要做的程序来说过于强大了。一些人觉得Java在面向对象上面做的没有Ruby和Smalltalk纯粹。但是最新出现的用Java实现的语言Groovy解决了这些问题。\nJava应用通常需要大量的配置文件和复杂的构建工具（如Maven和Gradle）。对于简单项目或个人开发者来说，这种复杂性可能会显得不必要。\n基本概念\nJava 程序员之所以容易搞混值传递和引用传递，主要是因为 Java 有两种数据类型，一种是基本类型，比如说 int，另外一种是引用类型，比如说 String。\n基本类型的变量存储的都是实际的值，而引用类型的变量存储的是对象的引用——指向了对象在内存中的地址。值和引用存储在 stack（栈）中，而对象存储在 heap（堆）中。\n之所以有这个区别，是因为：\n\n栈的优势是，存取速度比堆要快，仅次于直接位于 CPU 中的寄存器但缺点是，栈中的数据大小与生存周期必须是确定的。\n堆的优势是可以动态地分配内存大小，生存周期也不必事先告诉编译器，Java 的垃圾回收器会自动收走那些不再使用的数据。但由于要在运行时动态分配内存，存取速度较慢。\n\nJava 有 8 种基本数据类型，分别是 int、long、byte、short,float,double 、char 和 boolean。它们的值直接存储在栈中，每当作为参数传递时，都会将原始值（实参）复制一份新的出来，给形参用。形参将会在被调用方法结束时从栈中清除。\nJava only uses PASS-BY-VALUE:把引用变量的复制传给数组对象\njdk(develop kit)包含jre包含jvm；JRE是java运行时的环境，包含jvm和核心类库，JDK是开发工具包\nRace Condition（也叫做资源竞争），是多线程编程中比较头疼的问题。特别是Java多线程模型当中，经常会因为多个线程同时访问相同的共享数据，而造成数据的不一致性。为了解决这个问题，通常来说需要加上同步标志“synchronized”，来保证数据的串行访问。但是“synchronized”是个性能杀手，过多的使用会导致性能下降，特别是扩展性下降，使得你的系统不能使用多个CPU资源。without proper synchronization and their operation interleaves on each other.\nClass类\nObject类是一切java类的父类，对于普通的java类，即便不声明，也是默认继承了Object类。典型的，可以使用Object类中的toString()方法。\nClass类是用于java反射机制的，一切java类，都有一个对应的Class对象，他是一个final类。Class 类的实例表示，正在运行的 Java 应用程序中的类和接口。\nClass 类的实例表示正在运行的 Java 应用程序中的类和接口。枚举是一种类，注释是一种接口。每个数组属于被映射为 Class 对象的一个类，所有具有相同元素类型和维数的数组都共享该 Class 对象。基本的 Java 类型（boolean、byte、char、short、int、long、float 和 double）和关键字 void 也表示为 Class 对象。\nClass 没有公共构造方法。Class 对象是在加载类时由 Java 虚拟机以及通过调用类加载器中的 defineClass 方法自动构造的。\n注解\n注解是元数据的一种形式，提供有关于程序但不属于程序本身的数据。注解对它们注解的代码的操作没有直接影响。\n注解的作用\n\n**减少配置：**运行时动态处理，得到注解信息，实现代替配置文件的功能；\n**减少重复工作：**比如第三方框架xUtils，通过注解@ViewInject减少对findViewById的调用，类似的还有（JUnit、ActiveAndroid等）；\n\n通过上面的描述可以发现，其实注解干的很多事情，通过配置文件也可以干，比如为类设置配置属性；但注解和配置文件是有很多区别的，在实际编程过程中，注解和配置文件配合使用在工作效率、低耦合、可拓展性方面才会达到权衡。\n三类：自定义注解、JDK内置注解、还有第三方框架提供的注解。\n\n自定义注解就是我们自己写的注解，比如@UserLog\nJDK内置注解，比如@Override检验方法重写，@Deprecated标识方法过期等\n第三方框架定义的注解比如SpringMVC的@Controller等\n\npublic @interface 注解名称{    属性列表;}\nJava 定义了一套注解，共有 7 个，3 个在 java.lang 中，剩下 4 个在 java.lang.annotation 中。\n作用在代码的注解是\n\n@Override - 检查该方法是否是重写方法。如果发现其父类，或者是引用的接口中并没有该方法时，会报编译错误。\n@Deprecated - 标记过时方法。如果使用该方法，会报编译警告。\n@SuppressWarnings - 指示编译器去忽略注解中声明的警告。\n\n作用在其他注解的注解(或者说 元注解)是:\n\n@Retention - 标识这个注解怎么保存，是只在代码中，还是编入class文件中，或者是在运行时可以通过反射访问。\n@Documented - 标记这些注解是否包含在用户文档中。\n@Target - 标记这个注解应该是哪种 Java 成员。\n@Inherited - 标记这个注解是继承于哪个注解类(默认 注解并没有继承于任何子类)\n\n（class文件。）AOP（面向切面）思想，将程序中所有功能点划分为：需要登录与无需登录两种类型，即两个切面。对于切面的区分即可采用注解。\n反射\n通过java语言中的反射机制可以操作字节码文件（可以读和修改字节码文件。）\n通过反射机制可以操作代码片段。（class文件。）Reflection（反射）是Java被视为动态语言的关键，反射机制允许程序在执行期借助于Reflection API取得任何类的内部信息，并能直接操作任意对象的内部属性及方法\n(3)、RUNTIME\n反射是一开始并不知道我要初始化的类是什么，自然也无法使用new 关键字来创建对象了。这时候，我使用JDK 提供的反射API 进行反射调用。**反射就是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；并且能改变它的属性。**是Java被视为动态语言的关键。\n通过java语言中的反射机制可以操作字节码文件（.class）注解保留至运行期，意味着我们能够在运行期间结合反射技术获取注解中的所有信息\n实例化\nDemo demo = new Demo();\n1）右边的“new Demo”，是以Demo类为模板，在堆空间里创建一个Demo类对象（也简称为Demo对象）。\n2）末尾的()意味着，在对象创建后，立即调用Demo类的构造函数，对刚生成的对象进行初始化。构造函数是肯定有的。如果你没写，Java会给你补上一个默认的构造函数。\n3）左边的“Demo demo”创建了一个Demo 类引用变量。所谓Demo类引用，就是以后可以用来指向Demo对象的对象引用。\n4）“=”操作符使对象引用指向刚创建的那个Demo对象。demo 此时应该叫对象引用，它存储在栈中，保存了对象在堆中的地址\n3.实例化对象的语法：\n类名 引用变量名 = new 构造器名() ;\n4.访问成员属性或成员方法一般语法是：\n引用成员变量名.成员名\nJVM\n是运行不同bytecode的虚拟机。JVM 有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。可移植性高。字节码是JVM理解的代码（.class）。而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。\nnative方法是通过java中的JNI实现的。JNI是Java Native Interface的 缩写。从Java 1.1开始，Java Native Interface (JNI)标准成为java平台的一部分，它允许Java代码和其他语言写的代码进行交互。\nJNI一开始是为了本地已编译语言，尤其是C和C++而设计 的，但是它并不妨碍你使用其他语言，只要调用约定受支持就可以了。\n分为程序计数器、虚拟机栈、本地方法栈3个区域\n\n为什么要有程序计数器？\n为了保证程序(在操作系统中理解为进程)能够连续地执行下去，CPU必须具有某些手段来确定下一条指令的地址。而程序计数器正是起到这种作用，所以通常又称为指令计数器。\n\n线程私有。为了记录线程字节码的指令地址\n执行java方法时，程序计数器是有值的，执行native本地方法时，程序计数器的值为空。\n程序计数器，是唯一一个在java虚拟机规范中没有规定任何OutOfMemoryError的区域。\n程序计数器占用内存很小，在进行JVM内存计算时，可以忽略不计。\n\nJVM内存(Data area)\n运行步骤：\n1.方法区（Method Area）与Java堆一样，是各个线程共享的内存区域。\n2.方法区在JVM启动的时候被创建，并且它的实际的物理内存空间中和Java堆区一样都可以是不连续的。\n3.方法区的大小，跟堆空间一样，可以选择固定大小或者可扩展。\n4.方法区的大小决定了系统可以保存多少个类，如果系统定义了太多的类（加载大量第三方jar包、tomcat部署的工程过多、大量动态生成反射类），导致方法区溢出，虚拟机同样会抛出内存溢出错误：java.lang.OutOfMemoryError: PermGen space（jdk7以前）或者java.lang.OutOfMemoryError: Metaspace（jdk8以后）\n5.关闭JVM就会释放这个区域的内存。\nGarbage collection\n在JVM五种内存模型中，有三个是不需要进行垃圾回收的：程序计数器、JVM栈、本地方法栈。因为它们的生命周期是和线程同步的，随着线程的销毁，它们占用的内存会自动释放，所以只有方法区和堆需要进行GC。\n在Java中，没有任何引用的对象就是一个垃圾。2方法判断：引用计数法，和可达性分析算法。\n1。在当前的jvm中并没有采用，原因是他并不能解决对象之间循环引用的问题。\n假设有A和B两个对象之间互相引用，也就是说A对象中的一个属性是B，B中的一个属性时A,这种情况下由于他们的相互引用，从而是垃圾回收机制无法识别。\n因为引用计数法的缺点有引入了可达性分析算法：\n通过判断对象的引用链是否可达来决定对象是否可以被回收。\n按代的垃圾回收机制\n新生代（Young generation）：绝大多数最新被创建的对象都会被分配到这里，由于大部分在创建后很快变得不可达，很多对象被创建在新生代，然后“消失”。对象从这个区域“消失”的过程我们称之为：Minor GC 。\n老年代（Old generation）：对象没有变得不可达，并且从新生代周期中存活了下来，会被拷贝到这里。其区域分配的空间要比新生代多。也正由于其相对大的空间，发生在老年代的GC次数要比新生代少得多。对象从老年代中消失的过程，称之为：Major GC 或者 Full GC。\n**持久代（Permanent generation）**也称之为 方法区（Method area）：用于保存类常量以及字符串常量。注意，这个区域不是用于存储那些从老年代存活下来的对象，这个区域也可能发生GC。发生在这个区域的GC事件也被算为 Major GC 。只不过在这个区域发生GC的条件非常严苛，必须符合以下三种条件才会被回收：\n1、所有实例被回收\n2、加载该类的ClassLoader 被回收\n3、Class 对象无法通过任何途径访问（包括反射）\n什么时候进行回收\n1.会在cpu空闲的时候自动进行回收\n2.在堆内存存储满了之后\n3.主动调用System.gc()后尝试进行回收\n如何回收\n标记-清除”（Mark-Sweep）算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象\n“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存\nCMS是一个老年代收集器，全称 Concurrent Low Pause Collector，是JDK1.4后期开始引用的新GC收集器，在JDK1.5、1.6中得到了进一步的改进。它是对于响应时间的重要性需求大于吞吐量要求的收集器。对于要求服务器响应速度高的情况下，使用CMS非常合适。\njVM 变量储存\nStack(local variable and methods)***          方法内变量\nHeap(objects and array)***                            实例变量在对象中，可被垃圾收集\n**在Java虚拟机中，对象是在Java堆中分配内存的，这是一个普遍的常识。**但是，有一种特殊情况，那就是如果经过逃逸分析后发现，一个对象并没有逃逸出方法的话，那么就可能被优化成栈上分配。这样就无需在堆上分配内存，也无须进行垃圾回收了。\n（1）程序内存布局场景下，堆与栈表示两种内存管理方式；\n实际内存地址空间中的堆和栈与数据结构中的有区别\n内存中的栈区处于相对较高的地址，以地址的增长方向为上的话，栈地址是向下增长的。\n栈中分配局部变量空间，堆区是向上增长的用于分配程序员申请的内存空间。\n另外还有静态区是分配静态变量，全局变量空间的。\n只读区是分配常量和程序代码空间的；以及其他一些分区。\nprivate` `static` `int` `ii = ``2``; ``// 方法区(jdk7之前) -&gt; 堆(jdk7及以后)  ``private` `static` `String ss``/*引用：方法区(jdk7之前) -&gt; 堆(jdk7及以后)*/`  ``// 常量  ``private` `final` `int` `iii = ``3``;``// 方法区(jdk7之前) -&gt; 堆(jdk7及以后)  ``private` `final` `String sss``/*引用：方法区(jdk7之前) -&gt; 堆(jdk7及以后)*/` `= ``new` `String(``\"c\"``)``/*对象实例：堆*/``;  ``// 静态常量  ``private` `final` `static` `String ssss``/*引用：方法区(jdk7之前) -&gt; 堆(jdk7及以后)*/` `= ``new` `String(``\"d\"``)``/*对象实例：堆*/``;\nDynamic binding\nc当apparent type（reference variable) is called in a method, JVM编译时首先检查 多态中的superclass，然后actual class of object 被检查。就是决定调用哪个方法 at runtime\nJVM 使用 invokevirtual 指令来调用与 C++ 虚拟方法等效的 Java。在 C++ 中，如果我们想要覆盖另一个类中的一个方法，我们需要将其声明为虚拟，但在 Java 中，所有方法默认都是虚拟的（除了 final 和静态方法），因为我们可以覆盖子类中的每个方法。在 Java 中，所有非最终非私有、非静态方法都是虚拟的，这意味着可以在子类中覆盖的任何方法都是虚拟的\n2、堆栈缓存方式区别\n栈使用的是一级缓存， 它们通常都是被调用时处于存储空间中，调用完毕立即释放。\n堆则是存放在二级缓存中，生命周期由虚拟机的垃圾回收算法来决定（并不是一旦成为孤儿对象就能被回收）。所以调用这些对象的速度要相对来得低一些。\n3、堆栈数据结构区别\n堆（数据结构）：堆可以被看成是一棵树，如：堆排序。\n栈（数据结构）：一种先进后出的数据结构。\nobject的life取决于references\nClass variable ,methods use:STATIC\nstatic{   } //set static final 常量\nstatic final变量在静态存储空间，被所有线程共享。static final变量可以在定义时初始化。static final变量可以在静态代码块中初始化。\nfinal成员变量存储在对象中，不同对象有不同的值。 final变量可以在定义时初始化。  final变量可以在构造函数中初始化。\n类加载过程 及 双亲委派\n所有类都由类装载器载入，载入内存中的类对应一个 java.lang.Class 实例。\n类从被加载到内存中开始，到卸载出内存，经历了加载、连接、初始化、使用5个阶段，其中连接又包含了验证、准备、解析三个步骤。这些步骤总体上是按照图中顺序进行的\n[https://img-blog.csdnimg.cn/20200713095842419.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hvcml6b25fanVubW93ZW4=,size_16,color_FFFFFF,t_70]\n4、解析\n虚拟机将用于标识引用的符号替换为实际指向的引用的地址。符号或符号引用只不过是个标识（描述符），而实际地址才是真正的目的内存位置。\n解析动作主要针对：类或接口、字段（类成员变量）、类方法、接口方法等引用进行。\n类或接口的解析：判断所要转化成的直接引用是对数组类型，还是对普通的对象类型的引用，从而进行不同的解析。\n字段解析：对字段进行解析时，会先在本类中查找是否包含有简单名称和字段描述符都与目标相匹配的字段，如果有，则查找结束；如果没有，则会按照继承关系从上往下递归搜索该类所实现的各个接口和它们的父接口，还没有，则按照继承关系从上往下递归搜索其父类，直至查找结束，查找流程如下图所示：\n最后需要注意：理论上是按照上述顺序进行搜索解析，但在实际应用中，虚拟机的编译器实现可能要比上述规范要求的更严格一些。如果有一个同名字段同时出现在该类的接口和父类中，或同时在自己或父类的接口中出现，编译器可能会拒绝编译。\n类方法解析：对类方法的解析与对字段解析的搜索步骤差不多，只是多了判断该方法所处的是类还是接口的步骤，而且对类方法的匹配搜索，是先搜索父类，再搜索接口。\n接口方法解析：与类方法解析步骤类似，由于接口不会有父类，因此，只递归向上搜索父接口就行了。\n\n初始化\n\n给类的静态变量初始化值，不同于准备阶段，此处是使用用于自定义的值进行赋值。\n为什么说java解释和编译并存？\n从.class到机器码 Java通过解释器逐行解释执行，后面引入的JIT编译器运行效率高。当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。\n解释性语言在运行程序的时候才翻译（java,Python），比如解释性basic语言，专门有一个解释器能够直接执行basic程序，每个语句都是执行的时候才翻译。这样解释性语言每执行一次就要翻译一次，效率比较低。编译型语言（C）写的程序执行之前，需要一个专门的编译过程，把程序编译成为机器语言的文件，比如exe文件，以后要运行的话就不用重新翻译了，直接使用编译的结果就行了（exe文件），因为翻译只做了一次，运行时不需要翻译，所以编译型语言的程序执行效率高。\n8个二进制位为一个字节单位。一个英文字母（不分大小写）占一个字节的空间，一个中文汉字占两个字节的空间。英文标点占一个字节，中文标点占两个字节\n泛型（generics）\nJava 泛型（generics）是 JDK 5 中引入的一个新特性, 泛型提供了编译时类型安全检测机制，该机制允许程序员在编译时检测到非法的类型。泛型的本质是参数化类型，也就是说所操作的数据类型被指定为一个参数。\n\n强化类型安全，由于泛型在编译期进行类型检查，从而保证类型安全，减少运行期的类型转换异常。\n消除强制转换\n类型依赖关系更加明确，接口定义更加优好，增强了代码和文档的易读性。\n\n//表示类型参数可以是任何类型\npublic class Apple&lt;?&gt;{}\n//表示类型参数必须是A或者是A的子类\npublic class Apple{}\n//表示类型参数必须是A或者是A的超类型\npublic class Apple{}\n泛型一般有三种使用方式:泛型类、泛型接口、泛型方法。\nArrayList&lt;E&gt;中的E称为类型变量或者类型形参整个ArrayList&lt;Integer&gt;称为参数化的类型\nLambda\n1.8后特性， 目的 简化代码            形式：( parameter-list ) -&gt; { expression-or-statements }\n原来我们创建一个线程并启动它是这样的：\npublic class LamadaTest {    public static void main(String[] args) {        new Thread(new Runnable() {            @Override            public void run() {                System.out.println(\"沉默王二\");            }        }).start();    }}\n通过 Lambda 表达式呢？只需要下面这样：\npublic class LamadaTest {    public static void main(String[] args) {        new Thread(() -&gt; System.out.println(\"沉默王二\")).start();    }}\n使用场景\n一、列表循环\n1.增强for循环写法：\nList&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5);for (Integer integer : list) {    System.out.print(integer+\" \");}\n2.lambda表示写法：\nlist.forEach(x-&gt; System.out.print(x+\" \"));//如果只需要调用单个函数对列表元素进行处理，那么可以使用更加简洁的 方法引用 代替 Lambda 表达式：list.forEach(System.out::print);\n二、事件监听\n1.匿名函数写法：\nnew JButton().addActionListener(new ActionListener() {    @Override    public void actionPerformed(ActionEvent e) {        // handle the events    }});\n2.lambda表达式写法：\n// 这里主要体现为 比写匿名函数更简洁new JButton().addActionListener(e -&gt; {    // handle the events});\n三、代替 Runnable\n1.通常写法：\nnew Runnable() {    @Override    public void run() {        System.out.println(\"hello world\");    }};\n2.lambda表达式写法：\nRunnable runnable = () -&gt; System.out.println(“hello world”);\n四、Map 映射\nList&lt;Integer&gt; list1 = Arrays.asList(1, 2, 3, 4, 5);List&lt;Integer&gt; collect = list1.stream().map(x -&gt; x * 2).collect(Collectors.toList());collect.forEach(System.out::println);五、Reduce 聚合1.增强for循环写法：int result = 0;for (Integer number : numbers) {    result+=number;}\n2.lambda表达式写法：\nList&lt;Integer&gt; numbers = Arrays.asList(1, 2, 3, 4, 5);int sum = numbers.stream().reduce((x, y) -&gt; x + y).get();\n数据类型\nJDK1.6及以前，常量池在方法区，这时的方法区也叫做永久代；\nJDK1.7的时候，方法区合并到了堆内存中，这时的常量池也可以说是在堆内存中；\nJDK1.8及以后，方法区又从堆内存中剥离出来了，但实现方式与之前的永久代不同，这时的方法区被叫做元空间，常量池就存储在元空间。\nA servlet is deployed in a container. life cycle: init() service() destroy()\nhttps://img-blog.csdn.net/20170720145846983?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzMwOTg3MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\nJava 中使用 final 关键字来修饰常量\n自动类型转换：\n低  ------------------------------------&gt;  高计算时低的类型转换为高的，然后运算byte,short,char—&gt; int —&gt; long—&gt; float —&gt; double\n强制类型转换\n强制转换的格式是在需要转型的数据前加上“( )”，然后在括号内加入需要转化的数据类型。有的数据经过转型运算后，精度会丢失，而有的会更加精确，\n\nx = (int)34.56 + (int)11.2; // 丢失精度\ny = (double)x + (double)10 + 1; // 提高精度\n\nOO面向对象\n对象与类\nclass only exists at compile time; an object only exists at runtime\n对象有attributes(instance variable) and operations(methods)\nlocal variable versus. instance variable\n\n局部变量未被初始化 parameter is virtually the same as local variable\n实例变量在类中valid被声明，且被初始化。多线程中，实例变量是多个线程共享资源，要注意同步访问时可能出现的问题\n类变量用static声明，一个对象修改了变量，则所以对象中这个变量的值都会发生改变\n\n构造器\n构造器的继承\n子类构造器会默认调用父类无参构造器，如果父类没有无参构造器，则必须在子类构造器的第一行通过 super关键字指定调用父类的哪个构造器，具体看下文例子。final类是不允许被继承的，编译器会报错。很好理解，由于final修饰符指的是不允许被修改，而继承中，子类是可以修改父类的，这里就产生冲突了，所以final类是不允许被继承的。\n\n只有一对花括号包裹起来的代码我们称之为构造代码块。\n构造代码块与构造方法一样都是在类被实例化的过程中被调用的。\n\n父类静态代码块 &gt; 子类静态代码块 &gt; 父类构造代码块 &gt; 父类构造器 &gt; 子类构造代码块 &gt; 子类构造器\n首先是类的初始化，先要初始化main方法所在的类，即Son类，因为Son继承了Father类，所以要先初始化Father类Father类的初始化是从静态变量和静态代码块顺序执行，也就是先打印5，再打印1Son类的初始化逻辑相同，先打印10，再打印6然后是实例的初始化，同样Son继承了Father，那么需要先初始化Father的实例father实例的初始化顺序应该是9，3（Son类重写了test方法）、2，变量赋值和非静态代码块顺序执行，构造方法最后执行。son实例的初始化顺序为9、8、7第二个son实例的初始化同第二步所以最后的答案为：5、1、10、6、9、3、2、9、8、7\n特性：继承、多态、封装\n多态\n（Polymorphism）按字面的意思就是“多种状态”。在Java中多态性是允许你将父对象设置成为和一个或更多的他的子对象相等的技术，赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。 父类引用指向子类对象\n多态，指允许不同类的对象对同一消息做出响应。即同一消息可以根据发送对象的不同而采用多种不同的行为方式。（发送消息就是函数调用）简单的说，就是一句话：允许将子类类型的指针赋值给父类类型的指针（申明：Java中并没有指针这一说法，但是有引用的地方就会有指针）。     **解说：**在一般的继承关系中，总是父类给予子类更多的财富，让子类拥有更多的功能，而多态讲的是子类对父类的孝敬。只要将子类类型的指针赋值给父类类型的指针，那么我们就可以把父类当做子类来看待，也就是说，我们可以通过父类来使用子类的方法。这就是所说的父子情深。\n（2）作用\n通过继承和多态，我们可以实例化一个父类对象，把各个子类当做父类的一个状态，当我们需要某个子类的时候，我们就借用指针，来得到相应子类的方法。经过这样处理，我们可以利用父类多态，方便的使用其所有子类的方法，从而使我们面对一个统一的对象（父类）来处理多种状态下的情况。\n（3）体现\n必要条件        要有继承；        要有重写；        父类引用指向子类对象。\n多态的方面，分为向上转型和向下转型。\n假定父类为 动物，子类为狗，父类有一个方法发声（），狗继承并覆盖了一个发声方法。在子类重写该方法\n则（以下过程c#实现）：动物 a=new 狗（）；//这就为向上转型a.发声（）；\n在调用 a.发声（）；时调用的是狗的发声（）也可调动物类其他方法 但不能调用狗类方法\n若修改为动物 a=new 狗（）；\n狗b=（狗）a；//这里向下转型\nDynamic Binding 动态绑定是指在执行期间（非编译期）判断所引用对象的实际类型，根据其实际的类型调用其相应的方法。程序运行过程中，把函数（或过程）调用与响应调用所需要的代码相结合的过程称为动态绑定。程序在JVM运行过程中，会把类的类型信息、static属性和方法、final常量等元数据加载到方法区，这些在类被加载时就已经知道，不需对象的创建就能访问的，就是静态绑定的内容；需要等对象创建出来，使用时根据堆中的实例对象的类型才进行取用的就是动态绑定的内容。\n继承与类\n类和类之间的关系有三种：is-a、has-a和use-a关系。\nis-a关系也叫继承inheritance或泛化，比如学生和人的关系、手机和电子产品的关系都属于继承关系。\npublic class Penguin extends Animal {    public Penguin(String myName, int myid) {        super(myName, myid);    }}\nhas-a关系通常称之为聚类（关联）aggregation,比如部门和员工的关系，汽车和引擎的关系都属于关联关系；关联关系如果是整体和部分的关联，那么我们称之为聚合关系；如果整体进一步负责了部分的生命周期（整体和部分是不可分割的，同时同在也同时消亡），那么这种就是最强的关联关系，我们称之为合成关系。\nuse-a关系通常称之为依赖，比如司机有一个驾驶的行为（方法），其中（的参数）使用到了汽车，那么司机和汽车的关系就是依赖关系。\nDelegation（委托）\n委托依赖于动态绑定，因为它要求特定的方法可以在运行时调用不同的代码段。不使用继承，而是利用委托结合A，达到复用A类中代码\nsubclass只有一个parent\nPrivate方法不能继承且不能被子类seen\nUML类之间的关系\n5种关系从弱到强\n\n依赖（dependency）use-a , has-a\n关联（association）\n聚合（aggregation）\n组合（composition）\n继承（inheritance）可以更加细得分为实现（realization）和泛化（generalization）实现和泛化`，实现就是类实现接口，泛化就是类继承类、类继承抽象类、接口继承接口。 如是实现的话，那么子类就不能够在扩充方法，如果是泛化的话，可以在父类基础上再次扩充自己的方法。\n\nis-a关系也叫继承inheritance或泛化，比如学生和人的关系、手机和电子产品的关系都属于继承关系。\nhas-a关系通常称之为聚类（关联）aggregation、dependency,比如部门和员工的关系，汽车和引擎的关系都属于关联关系；关联关系如果是整体和部分的关联，那么我们称之为聚合关系；如果整体进一步负责了部分的生命周期（整体和部分是不可分割的，同时同在也同时消亡），那么这种就是最强的关联关系，我们称之为合成关系。\nuse-a关系通常称之为依赖，比如司机有一个驾驶的行为（方法），其中（的参数）使用到了汽车，那么司机和汽车的关系就是依赖关系。\n聚合关系是“has-a”关系，组合关系是“contains-a”关系；聚合关系表示整体与部分的关系比较弱，而组合比较强；聚合关系中代表部分事物的对象与代表聚合事物的对象的生存期无关，一旦删除了聚合对象不一定就删除了代表部分事物的对象。组合中一旦删除了组合对象，同时也就删除了代表部分事物的对象\n封装Encapsulation\n封装是指把一个对象的状态信息（也就是属性）隐藏在对象内部，不允许外部对象直接访问对象的内部信息。但是可以提供一些可以被外界访问的方法来操作属性。就好像我们看不到挂在墙上的空调的内部的零件信息（也就是属性），但是可以通过遥控器（方法）来控制空调。如果属性不想被外界访问，我们大可不必提供方法给外界访问。但是如果一个类没有提供给外界访问的方法，那么这个类也没有什么意义了。就好像如果没有空调遥控器，那么我们就无法操控空凋制冷，空调本身就没有意义了（当然现在还有很多其他方法 ，这里只是为了举例子）。\nAbstract class\n抽象是相对于具体而言的，一般而言，具体类有直接对应的对象，而抽象类没有，它表达的是抽象概念，一般是具体类的比较上层的父类。比如说，狗是具体对象，而动物则是抽象概念，樱桃是具体对象，而水果则是抽象概念，正方形是具体对象，而图形则是抽象概念。下面我们通过一些例子来说明Java中的抽象类。\n图形类Shape，它有一个方法draw()，Shape其实是一个抽象概念，它的draw方法其实并不知道如何实现，只有子类才知道。这种只有子类才知道如何实现的方法，一般被定义为抽象方法。\n\nThe compiler will not let you instantiate an abstract class ///abstractin terms of classesthat class must be extended, in order to be instantiated•abstractfor methodsthe method must be overridden in the child class\n子类必须implement superclass的所有抽象方法，但父类可以没有抽象方法，有抽象方法的一定是抽象类。\n抽象方法也不能用static修饰，试想一下，如果用static修饰了，那么我们可以直接通过类名调\n用，抽象类不能被实例化.\n抽象类中的抽象方法只是声明，不包含方法体，就是不给出方法的具体实现也就是方法的具体功能。\n\n抽象方法不能用private修饰，因为抽象方法必须被子类实现（覆写），而private权限对于子类来\n说是不能访问的，所以就会产生矛盾\nADvantage:1、由于抽象类不能被实例化，最大的好处就是通过方法的覆盖来实现多态的属性。也就是运行期绑定\n2、抽象类将事物的共性的东西提取出来，由子类继承去实现，代码易扩展、易维护。\n子类继承父类执行顺序\nSonTest.java\npublic class SonTest extends FatherTest {// 然后再加入一个main函数public static void main(String[] args) {System.out.println(\"--子类主程序--\");FatherTest father = new FatherTest(\"父亲的名字\");father.speak();SonTest son = new SonTest(\"儿子的名字\");son.speak();}}\n1.先父再子，执行子类的重写方法\n[https://img2018.cnblogs.com/blog/1080293/201809/1080293-20180927140643561-1197079835.jpg]\n2.静态代码块—主程序—非静态代码块—构造函数—一般方法\n(Override)与(Overload)\n\n重写是子类对父类的允许访问的方法的实现过程进行重新编写, 返回值和形参都不能改变。即外壳不变，核心重写！\n重写的好处在于子类可以根据需要，定义特定于自己的行为。 也就是说子类能够根据需要实现父类的方法。\n重写方法不能抛出新的检查异常或者比被重写方法申明更加宽泛的异常。例如： 父类的一个方法申明了一个检查异常 IOException，但是在重写这个方法的时候不能抛出 Exception 异常，因为 Exception 是 IOException 的父类，只能抛出 IOException 的子类异常。\n在面向对象原则里，重写意味着可以重写任何现有方法。如下  方法1.super\npublic class A {    private String nameA=\"A\";    public void getName() {        System.out.println(\"父类\"+nameA);    }    public static void main(String[] args) {    }}public class B extends A{    private String nameB=\"B\";    @Override   //注解重写方法    public void getName() {        System.out.println(\"子类\"+nameB);        super.getName();    }    public static void main(String[] args) {        B b=new B();        b.getName();    }}Output：子类B父类A\nkeyword : super. 调用父类方法、变量(非private修饰)                        super()调用父类构造器\nWhat is the relation between class and object?\n变量初始化\n三类：1.类的属性\n对于第一种变量，Java虚拟机会自动进行初始化。如果给出了初始值，则初始化为该初始值。如果没有给出，则把它初始化为该类型变量的默认初始值。\nint类型变量默认初始值为0\nfloat类型变量默认初始值为0.0f\ndouble类型变量默认初始值为0.0boolean类型变量默认初始值为false\nchar类型变量默认初始值为0(ASCII码)\nlong类型变量默认初始值为0\n所有对象引用类型变量（String,Array）默认初始值为null，即不指向任何对象。\n注意数组本身也是对象，所以没有初始化的数组引用在自动初始化后其值也是null。\n语法\n高级for()\nint[] nums = {10,20,30,40,50};//方法一:通过数组下标进行遍历for(int i=0;i&lt;nums.length;i++){int n = nums[i];System.out.println(n);}//方法二:使用for-each进行遍历 for(datatype x: nums)    for(int i : index)的意思就是说，遍历index数组，每次遍历的对象用i 这个对象去接收。    相当于：    int i=0; //用于接收index数组中的某一个对象    for(int j = 0;j&lt;index.length;j++){    i = index[j];    }for(int n:nums){System.out.println(n);}\n编译运行\n类——对象——属性、方法\n一个文件中只能有一个public class，表示公开接口。虚拟机会找public类中的main方法\n\n第2、 public 类的名字必须和这个编译单元的文件名彻底相同，包括大小写。\n对于一个public类，它是能够被项目中任何一个类所引用的，只需在使用它前import一下它所对应的class文件便可。将类名与文件名一一对应就能够方便虚拟机在相应的路径（包名）中找到相应的类的信息。若是不这么作的话，就很难去找，并且开销也会很大。\n\njava中的修饰符分为类修饰符，字段修饰符，方法修饰符。根据功能的不同，主要分为以下五种。\n权限访问修饰符\npublic,protected,default,private,这四种级别的修饰符都可以用来修饰类、方法和字段。\npublic–publicinstance variables and methods are inherited•\nprotected–protectedinstance variables and methodsare inherited•\nprivate– any privateinstance variables and methodsare not inherited and cannot be seen by the subclass\n(一般实例变量用private修饰)\nhttps://iknow-pic.cdn.bcebos.com/29381f30e924b899942c9e7463061d950a7bf66e?x-bce-process=image/resize,m_lfit,w_600,h_800,limit_1/quality,q_85\nprivate int[] age; //private 封装了class中的age属性\n非访问修饰符\n抽象方法控制符abstract 、静态方法控制符static 、最终方法控制符final 、本地方法控制符native 、同步方法控制符synchronized\n（1）抽象方法控制符 abstract ： 抽象类不能用来实例化对象，声明抽象类的唯一目的是为了将来对该类进行扩充。\n一个类不能同时被 abstract 和 final 修饰。如果一个类包含抽象方法，那么该类一定要声明为抽象类，否则将出现编译错误。\n抽象类可以包含抽象方法和非抽象方法。\n（2）静态方法控制符 static ：用修饰符 static 修饰的方法称为静态方法。静态方法是属于整个类的类方法；而不使用static 修饰、限定的方法是属于某个具体类对象的方法。 由于 static方法是属于整个类的，所以它不能操纵和处理属于某个对象的成员变量，而只能处理属于整个类的成员变量，即 static 方法只能处理 static的域。\n（3）最终方法控制符 final ：final类不能被继承；final方法不能重写，可被继承；final成员变量可以继承。\n用修饰符 final修饰的方法称为最终方法。最终方法是功能和内部语句不能更改的方法，即。final固定了方法所具有的功能和操作，防止当前类的子类对父类关键方法的错误定义，保证了程序的安全性和正确性。所有被 private 修饰符限定为私有的方法，以及所有包含在 final 类 ( 最终类)不能被extends\n当变量是final时,通过将final局部变量\"复制\"一份,复制品直接作为局部内部中的数据成员.这样:当局部内部类访问局部变量 时,其实真正访问的是这个局部变量的\"复制品\"(即:这个复制品就代表了那个局部变量).因此:类内其他方法访问局部变量需要设置其为final\n（4）本地方法控制符 native ：用修饰符 native 修饰的方法称为本地方法。为了提高程序的运行速度，需要用其它的高级语言书写程序的方法体，那么该方法可定义为本地方法用修饰符 native 来修饰。\n（5）同步方法控制符 synchronized ：该修饰符主要用于多线程程序中的协调和同步\nStatic keyword\n\n!https://images2018.cnblogs.com/blog/1295451/201808/1295451-20180815143157281-699250705.png\nstatic静态成员变量：被所有对象共享，无需初始化变量，方便调用\n静态成员变量虽然独立于对象，但是不代表不可以通过对象去访问，所有的静态方法和静态变量都可以通过对象访问（只要访问权限足够）。\t\tthis访问\nthis.方法名称\n用来访问本类的成员方法\nthis();                    //访问本类的构造方法\nstatic final:定义常量\nstatic方法：可重写不能重载，隐藏，其中不能使用非静态方法(main中不能直接调用非静态方法)，不能使用非静态变量\ninstance method can invoke static method\n想在不创建对象的情况下调用某个方法，就可以将这个方法设置为static。我们最常见的static方法就是main方法，只能访问static变量。因为程序在执行main方法的时候没有创建任何对象，因此只有通过类名来访问。\nimport\n\n\n单类型导入(single-type-import)\n（例:import java.util.ArrayList; ）\n\n\n按需类型导入(type-import-on-demand)\n（例:import java.util.*;）\njava以这样两种方式导入包中的任何一个public的类和接口\n\n\nString\nString的底层是一个用private和final修饰的char数组。final可以保证数组的的引用地址不会被改变，private不允许外部访问可以保证数组的值不会被修改，这样就能保证String类的不可变性。是对象不是基本数据类型\n\n因为String类的不可变性，才能使得JVM可以实现字符串常量池；字符串常量池可以在程序运行时节约很多内存空间，因为不同的字符串变量指向相同的字面量时，都是指向字符串常量池中的同一个对象。这样一方面能够节约内存，另一方面也提升了性能。\n因为String类的不可变性，从而保证了字符串对象在多线程环境下是线程安全的。\n\n常量池(constant pool)指的是在编译期被确定，并被保存在已编译的.class文件中的一些数据。它包括了关于类、方法、接口等中的常量，也包括字符串常量。\n对字符串常量池进行总结:\n当用new关键字创建字符串对象时, 不会查询字符串常量池; 当用双引号直接声明字符串对象时, 虚拟机将会查询字符串常量池.\n说白了就是: 字符串常量池提供了字符串的\n复用\n功能,\n除非我们要显式创建新的字符串对象\n, 否则对同一个字符串虚拟机只会维护一份拷贝。\n!https://img-blog.csdnimg.cn/img_convert/9b21555ad4b131f7b86d599a6d465f51.png\n private String s/*引用：堆*/ = new String(\"a\")/*对象实例：另一块堆空间中*/;private String s1/*引用：堆*/ = \"aa\"/*字面量aa：字符串常量池*/;// 字符串常量池是运行时常量池的一部分，在jdk7之前位于方法区，jdk7开始移至堆中String a = \"hello2\";String b = \"hello\";String c = b + 2;System.out.println((a == c));输出结果为:false。由于有符号引用的存在，所以  String c = b + 2;不会在编译期间被优化，不会把b+2当做字面常量来处理的String a = \"hello2\";final String b = \"hello\";String c = b + 2;System.out.println((a == c));输出结果为：true。对于被final修饰的变量，会在class文件常量池中保存一个副本，也就是说不会通过连接而进行访问\nStringBuffer and StringBuilder\n\n操作少量的数据: 适用String\n单线程操作字符串缓冲区下操作大量数据: 适用StringBuilder are mutable\n多线程操作字符串缓冲区下操作大量数据: 适用StringBuffer are mutable\n\nString &lt;–&gt; 包装类\nString --&gt; Integer: Integer i1 = new Integer(“12”); 或者 Integer i1 = Integer.valueOf(“12”)\n包装类 --&gt; String: String str = 任何对象.toString();\nStringbuffer(synchronisation) vs.   new StringBuilder(“cat”); 5.0后新方法\nScanner class\n1.把word当成delimiter myScanner.next()\n2.读取primitive type values\n3.Reading console input : new  Scanner(System.in)\nequals ==区别\n== 对于基本数据类型来说，比较的是值。对于引用数据类型来说，比较的是对象的内存地址。\n\n因为 Java 只有值传递，所以，对于 == 来说，不管是比较基本数据类型，还是引用数据类型的变量，其本质比较的都是值，只是引用类型变量存的值是对象的地址。\n\nequals() 作用不能用于判断基本数据类型的变量，只能用来判断两个对象是否相等。equals()方法存在于Object类中，而Object类是所有类的直接或间接父类。\n``equals()` 方法存在两种使用情况：\n\n类没有覆盖 equals()方法 ：通过equals()比较该类的两个对象时，等价于通过“==”比较这两个对象，使用的默认是 Object类equals()方法。\n类覆盖了 equals()方法 ：一般我们都覆盖 equals()方法来比较两个对象中的属性（值）是否相等；若它们的属性（值）相等，则返回 true(即，认为这两个对象相等)。\n\n举个例子：\npublic class test1 {    public static void main(String[] args) {        String a = new String(\"ab\"); // a 为一个引用  堆内存“ab”        String b = new String(\"ab\"); // b为另一个引用,对象的内容一样        String aa = \"ab\"; // 放在常量池中        String bb = \"ab\"; // JVM从常量池中查找 bb指向同一地址        if (aa == bb) // true            System.out.println(\"aa==bb\");        if (a == b) // false，非同一对象            System.out.println(\"a==b\");        if (a.equals(b)) // true            System.out.println(\"aEQb\");        if (42 == 42.0) { // true            System.out.println(\"true\");        }    }}Copy to clipboardErrorCopied\n说明：\n\n若当前对象和比较的对象是同一个对象，即return true。也就是Object中的equals方法。 若当前传入的对象是String类型，则比较两个字符串的长度，即value.length的长度。 若长度不相同，则return false 若长度相同，则按照数组value中的每一位进行比较，不同，则返回false。若每一位都相同，则返回true。 若当前传入的对象不是String类型，则直接返回false\n当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。\n\n一、equeals相等，那么hashCode一定相等\n二、hashCode相等，那么equals不一定返回true\n三、hashCode不相等，那么equals一定不相等\n为什么有了equals这样有效的方法还需要hashCode呢？\n因为：\n在HashSet(特性：不含有重复的内容)，有新对象加入时，会先计算对象的HashCode值来判断对象加入的位置，同时于已有的对象hashCode作比较，如果有相同的，再用equals去比较，若相同那么则不加入hashSet，若不相同，会使其加入到其他的位置。如果没有其他对象hashCode与之相同的，那么这个对象必定是一个新对象，这样就减少了equals的执行次数、先用hashCode去比较，从而大大提高了速度\n将String转换成int\nint guess = Integer.parseInt(StringGuess)\nArrayListandArray\nArray数组和ArrayList区别?\n首先，ArrayList是基于数组实现的，他存储的是引用类型\nArrayList是接口List的实现类，而且java为其提供了丰富的增删改查等方法，使用起来较为较为方便。\n数组，只能存储单一的数据类型，一旦数组的长度给定无法改变\n扩展：ArrayList不是线程安全的，只能用在单线程环境下，\n多线程环境下可以考虑用Collections.synchronizedList(List l)方法返回一个线程安全的ArrayList类，也可以使用concurrent并发包下的CopyOnWriteArrayList类。\n什么时候应该使用 Array 而不是 ArrayList？\n对于基本类型数据，集合使用自动装箱来减少编码工作量。但是，当处理固定大小的基本数据类型的时\n候，这种方式相对比较慢\n在数组声明中包含数组长度永远是不合法的！如：int[l5] arr; 。因为，声明的时候并没有实例化任何对象，只有在实例化数组对象时，JVM才分配空间，这时才与长度有关。在数组构造的时候必须指定长度，因为JVM要知道需要在堆上分配多少空间。数组不是集合，它只能保存同种类型的多个原始类型或者对象的引用\narraylist:建立在heap上 ArrayList \n是一个特殊的数组–动态数组。来自于System.Collections命名空间；通过添加和删除元素，就可以动态改变数组的长度。\nArrayList arr = new ArrayList(3);//初始容量为3ArrayList&lt;String&gt; arr = new ArrayList&lt;E&gt;();ArrayList arr1 = new ArrayList(); //不初始化刚开始的数组容量，当数组容量满时数组会自动一当前数组容量的2倍扩容arr.add()arr.remove()arr.size() arr.get()boolean inIt = arr.constains(f)arr.isEmpty()\nJAVA library API\nAPI（Application Programming Interface,应用程序编程接口）是一些预先定义的函数或类，目的是提供应用程序与开发人员基于某软件或硬件的以访问一组的能力，而又无需访问源码，或理解内部工作机制的细节。\n超类：java.lang.Objectis the ultimate parent of EVERY class in java\nimport java.util.ArrayList; //We need to know what package the class is in\njava.lang provides fundamental自动部署\n包括Math.random()是令系统随机选取大于等于 0.0 且小于 1.0 [0,1)的伪随机 double 值\n另一种方法\nMath.abs();                   Math.round();参数上加0.5，然后进行下取整\nMath.max(a,b);\n//生成97-122的int型的整型int` `intValue=(``int``)(Math.random()*``26``+``97``);\njava.io: input output\njava.awt: 画画painting images, users interfaces\nJDK Class Library\nLots of classes in the API are abstract\njava.lang-- Provides classes that are fundamental to the design of the Java programming language.\njava.io-- Provides for system input and output through data streams, serialization and the file system.\njava.awt-- Contains all of the classes for creating user interfaces and for painting graphics and images.\nMain函数 psvm\n什么是public因为要在类外面调用main()所以是public为什么是static因为系统开始执行一个程序前,并没有创建main()方法所在类的实例对象,它只能通过类名类调用主方法main()作为程序入口,所以该方法是static为什么是void因为主方法没有返回值为什么main主方法名为什么是String args[]或者String[] args？这表示给主方法传一个字符串数组。String[] args是专门用来接收命令行参数的。并默认从args[0]开始接收，然后依次是args[1]、args[2]、args[3]…并且在命令行的输入的字符串中默认以“ ”为分隔符。        public static void main(String args[]){        String s1=args[0];        String s2=args[1];        String s3=args[2];        System.out.println(s3);        }        打开cmd，运行指令 java E i love “China”  输出：China        当输入 ebu java program //ebu==args[0]  java==args[1]        当输入 \"ebu java program\"  // args[0] == ebu java program\ninterface 接口\ninterface 接口名称 [extends 其他的接口名] {        // 声明变量        // 抽象方法，无内容 JDK1.8后可以定义方法by default static}public interface List&lt;E&gt; {    public boolean add(E e);    public E remove(int index);    public void clear();    …}public class LoggingList&lt;E&gt; implements List&lt;E&gt; {    private final List&lt;E&gt; list;    public LoggingList&lt;E&gt;(List&lt;E&gt; list) {        this.list = list;    }    public boolean add(E e) {        System.out.println(\"Adding \" + e);        return list.add(e);    } // 委托    public E remove(int index) {        System.out.println(\"Removing at \" + index);        return list.remove(index);    }    …}\n有时必须从几个类中派生出一个子类，继承它们所有的属性和方法。但是，Java不支持多重继承。有了接口，就可以得到多重继承的效果。接口定义的是多个类都要实现的操作，即“what to do”。类可以实现接口，从而覆盖接口中的方法，实现“how to do”。\nvs. abstract class\n两者的设计思想不同，接口是为了面向抽象特征自顶向下，抽象类是自底向上泛化共有部分. 开发者继承抽象类是为了使用抽象类的属性和行为; 开发者实现接口只是为了使用接口的行为.抽象类是从子类中发现公共部分，然后泛化成抽象类，子类继承该父类即可，但是接口不同。实现它的子类可以不存在任何关系，共同之处。例如猫、狗可以抽象成一个动物类抽象类，具备叫的方法。鸟、飞机可以实现飞Fly接口，具备飞的行为，这里我们总不能将鸟、飞机共用一个父类吧！所以说抽象类所体现的是一种继承关系，要想使得继承关系合理，父类和派生类之间必须存在\"is-a\" 关系，在概念本质上应该是相同的。对于接口则不然，并不要求接口的实现者和接口定义在概念本质上是一致的， 仅仅是实现了接口定义的规则而已。\n接口(interface)是抽象方法和常量值的定义的集合。抽象类和接口是配合而非替代关系，它们经常一起使用，接口声明能力，抽象类提供默认实现，实现全部或部分方法，一个接口经常有一个对应的抽象类。\n比如说，在Java类库中，有：\n\nCollection接口和对应的AbstractCollection抽象类\nList接口和对应的AbstractList抽象类\nMap接口和对应的AbstractMap抽象类\n\n对于需要实现接口的具体类而言，有两个选择，一个是实现接口，自己实现全部方法，另一个则是继承抽象类，然后根据需要重写方法。\n从本质上讲，接口是一种特殊的抽象类，这种抽象类中只包含常量和方法的定义，而没有变量和方法的实现。\n/*在JAVA中，一个类无法继承自多个类，但是可以实现多个接口，使用关键字implements\n多个接口之间使用“,”隔开  多个接口之间，没有先后顺序\n这个类叫做实现类，这个类必须实现所有接口的所有方法\n*/接口与抽象类的区别：\n接口里面不可以实现方法体，抽象类可以实现方法体注：JDK 1.8 以后，接口里可以有静态方法和方法体了。\n注：JDK 1.8 以后，接口允许包含具体实现的方法，该方法称为\"默认方法\"，默认方法使用 default 关键字修饰。更多内容可参考 Java 8 默认方法。\n注：JDK 1.9 以后，允许将方法定义为 private，使得某些复用的代码不会把方法暴露出去。更多内容可参考 Java 9 私有接口方法。\n接口可以多继承接口，抽象类不可                       接口需要被子类实现，抽象类是被子类继承（单一继承）\n接口中只能有公有的方法和属性而且****必须赋初始值，抽象类中可以有私有方法和属性\n接口中不能存在静态方法，但是属性可以是final，抽象类中方法中可以有静态方法，属性也可以;   接口的所有的成员必须要public\n4)接口中所有的属性是隐式的static和final的\npublic class Bat\nimplements\nFlyable,Bitable;\ninterface与抽象类\nQustion:\n1.为什么不直接在类里面写对应的方法, 而要多写1个接口(或抽象类)?\n2.既然接口跟抽象类差不多, 什么情况下要用接口而不是抽象类.\n3.为什么interface叫做接口呢? 跟一般范畴的接口例如usb接口, 显卡接口有什么联系呢?\n1.为了使用多态，方便调用时重写1个对象方法 而不是重载多个\n包装类wrapper class\n当需要存入集合时，将基础数据类型实例封装为Java对象。常用的包装类可以分为三类：Character、Number、Boolean\n8种对应 （Integer String Byte Short Long Boolean Double Float）\n!https://pic3.zhimg.com/80/v2-7b1127ae4c80b9cae683e32c296da81e_1440w.jpg\n自动装箱（Autoboxing）：把一个基本类型的变量，直接赋值给对应的包装类变量，或者赋值给Object变量（Object是所有类的父类，子类对象可以直接赋给父类变量----Java的向上自动转型特性）如：\nInteger i=3;或Object j=4;\n    Integer a = 127;当Integer对象封装的数据在1byte的取值范围内，多个对象共享同一个对象空间，即就是同一个对象•    Integer a1 = 128;•    Integer b = 127;•    Integer b1 = 128;•    System.out.println(a==b);//true•    System.out.println(a1==b1);//false1 System.out.println(Integer.valueOf(\"1000\")==Integer.valueOf(\"1000\"));   --false2 System.out.println(Integer.valueOf(\"128\")==Integer.valueOf(\"128\"));    --false3 System.out.println(Integer.valueOf(\"127\")==Integer.valueOf(\"127\"));   --true4 System.out.println(Integer.valueOf(\"-128\")==Integer.valueOf(\"-128\"));   --true5 System.out.println(Integer.valueOf(\"-1000\")==Integer.valueOf(\"-1000\"));    --false\n自动拆箱（AutoUnboxing）：把一个包装类变量，直接赋值给一个基本类型的变量，如：\nint a=i;\n注意：int b=j;编译会报错，原因是：Java有向上自动转型特性，但不能自动向下转型，想要转要加强制转换，如：\nint b = (Integer)j;\nRecursion vs. Iteration\n递归(recursion)实际上不断地深层调用函数，直到函数有返回才会逐层的返回，因此，递归涉及到运行时的堆栈开销（参数必须压入堆栈保存，直到该层函数调用返回为止），所以有可能导致堆栈溢出的错误；但是递归编程所体现的思想正是人们追求简洁、将问题交给计算机，以及将大问题分解为相同小问题从而解决大问题的动机。\nGUI\nSwing组件\n如图所示：swing组件主要可分为三个部分，后面会详细介绍\n（1）顶层容器:：常用有JFrame(默认size（0）and invisible)，JDialog\n（2）中间容器：JPanel，JOptionPane，JScrollPane，JLayeredPane 等，主要以panel结尾。\n（3）基本组件：JLabel，JButton，JTextField，JPasswordField，JRadioButton 等。\njava.awt（更慢） vs.  javax.swing（more efficient, consistensy across system written by pure JAVA）\nFrame是Window的子类，一个Frame对象就是一个有标题有边界的顶层窗口。(默认BorderLayout)\nPanel是最简单的容器类，是Container的子类。（默认FlowLayout）一个Panel对象就是要给应用程序提供空间，用来添加组件，包括其它的Panel对象。\nComponent（Container是Component的子类），Component是基类\nSwing 中使用 JTextField 类实现一个单行文本框，它允许用户输入单行的文本信息\nLayout\nflowlayout          gridlayout         borderLayout\nmyframe.add(button)\n5.0后允许我们可以省去调用getContentPane()而直接在容器内应用add(),setLayout()和remove()。然而，我们还是不能忽略了ContentPane，即使我们可能将不会再使用ContentPane来添加部件到容器。\nException\nUsing exception让run-time error 在compile time被捕获，更有效\n\n!https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/2020-12/Java异常类层次结构图2.png\n在 Java 中，所有的异常都有一个共同的祖先 java.lang 包中的 Throwable 类。Throwable 类有两个重要的子类 Exception（异常）和 Error（错误）。Exception 能被程序本身处理(try-catch)， Error 是无法处理的(只能尽量避免)。\nException 和 Error 二者都是 Java 异常处理的重要子类，各自都包含大量子类。\n当java识别到runtime error ,an exception is thrown\n\n\nException :程序本身可以处理的异常，可以通过 catch 来进行捕获。Exception 又可以分为 受检查异常(必须处理) 和 不受检查异常(可以不处理)。\n\n\nError ：Error 属于程序无法处理的错误,大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题 ，我们没办法通过 catch 来进行捕获 。例如，Java 虚拟机运行错误（Virtual MachineError）、虚拟机内存不够错误(OutOfMemoryError)、类定义错误（NoClassDefFoundError）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止。\neg:   Exception in thread “main” java.lang.AssertionError:\n\n\ntry catch：自己处理异常\n\n\ntry {\n*可能出现异常的代码\n*} catch（Exception e）{\ne.printStackTrace();\n\n如果出现了异常类A类型的异常，那么执行该代码\n*} …（catch可以有多个）\n\n\n\nfinally {\n*最终肯定必须要执行的代码（例如释放资源的代码）\n*}\n\n\nthrow\nthrow new Exception(“What are u doing”);\n（1）throws用于方法头，表示的只是异常的申明，而throw用于方法内部，抛出的是异常对象。\n（2）throws可以一次性抛出多个异常，而throw只能一个\n（3）throws抛出异常时，它的上级（调用者）也要申明抛出异常或者捕获，不然编译报错。而throw的话，可以不申明或不捕获（这是非常不负责任的方式）但编译器不会报错。\nAssertion\n判断语句是否为真，假——》\nComparable sort\n若一个类要实现Comparator接口：它一定要实现compareTo(T o1,T o2) 函数，但可以不实现 equals(Object obj) 函数。\n为什么可以不实现 equals(Object obj) 函数呢？ 因为任何类，默认都是已经实现了equals(Object obj)的。 Java中的一切类都是继承于java.lang.Object，在Object.java中实现了equals(Object obj)函数；所以，其它所有的类也相当于都实现了该函数。\n(2) int compare(T o1, T o2) 是“比较o1和o2的大小”。返回“负数”，意味着“o1比o2小”；返回“零”，意味着“o1等于o2”；返回“正数”，意味着“o1大于o2”。\n(3) 这个排序算法是“经过调优的合并排序”算法\n我们不难发现：Comparable相当于“内部比较器”，而Comparator相当于“外部比较器”。 Comparator中文译为比较器，它可以作为一个参数传递到Collections.sort和Arrays.sort方法来指定某个类对象的排序方式。同时它也能为sorted set和sorted map指定排序方式。    同Comparable类似，指定比较器的时候一般也要保证比较的结果与equals结果一致，不一致的话，对应的sorted set和sorted map的行为同样会变得怪异。推荐实现的比较器类同时实现java.io.Serializable接口，以拥有序列化能力，因为它可能会被用作序列化的数据结构（TreeSet、TreeMap）的排序方法。\n总结：\n一个类实现了 Comparable 接口，意味着该类的对象可以直接进行比较（排序），但比较（排序）的方式只有一种，很单一。\n一个类如果想要保持原样(不implements comparable，又需要进行不同方式的比较（排序），就可以定制比较器（实现 Comparator 接口）。\nComparable 接口在 java.lang 包下，而 Comparator 接口在 java.util 包下，算不上是亲兄弟，但可以称得上是表（堂）兄弟。\nCollection\n容器\nJava中容器主要分为两大类：Collection和Map\n容器可以管理对象的生命周期、对象与对象之间的依赖关系，您可以使用一个配置文件（通常是XML），在上面定义好对象的名称、如何产生（Prototype 方式或Singleton 方式）、哪个对象产生之后必须设定成为某个对象的属性等，在启动容器之后，所有的对象都可以直接取用，不用编写任何一行程序代码来产生对象，或是建立对象与对象之间的依赖关系。\n!https://img-blog.csdnimg.cn/20190717224652123.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2phdmFlZV9nYW8=,size_16,color_FFFFFF,t_70\nhttps://img-blog.csdn.net/20180807200307368?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5MzczMjg1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\n包括set，list，queue\nCollection中的集合，元素是孤立存在的（理解为单身），向集合中存储元素采用一个个元素的方式存储。\nMap中的集合，元素是成对存在的(理解为夫妻)。每个元素由键与值两部分组成，通过键可以找对所对应的值。\nCollection中的集合称为单列集合，Map中的集合称为双列集合。\n需要注意的是，Map中的集合不能包含重复的键，值可以重复；每个键只能对应一个值。\nMap中常用的集合为HashMap集合、LinkedHashMap集合。\nList\nList必须按照插入的顺序保存元素，Set不能有重复元素（通过比较hashcode和equals方法）但是也没有顺序，Queue按照排队规则来确定对象产生的顺序（通常与它们被插入的顺序相同）。\nArrayList是实现了基于动态数组的数据结构，LinkedList是基于链表结构。\n对于随机访问的get和set方法，ArrayList要优于LinkedList，因为LinkedList要移动指针。\n对于新增和删除操作add和remove，LinkedList比较占优势，因为ArrayList要移动数据。\nLinkedList头尾操作的性能非常优良\nTreemap\ntreemap由红黑树实现，自动对key进行排序\n1、HashMap无序，TreeMap有序。\n2、HashMap覆盖了equals（）方法和hashcode（）方法，这使得HashMap中两个相等的映射返回相同的哈希值；\nTreeMap则是实现了SortedMap接口，使其有序。\n3、HashMap的工作效率更高，而TreeMap则是基于树的增删查改。更推荐使用HashMap。\n4、HashMap基于数组+链表+红黑树（jdk1.8之后）实现，TreeMap是基于红黑树实现。\n5、两者都不是线性安全的。\nHashmap\nAn object that maps keys to values. A map cannot contain duplicate keys; each key can map to at most one value.\nHashSet底层是HashMap，HashMap的底层（数组+链表+红黑树)来实现的，它之所以有相当快的查询速度主要是因为它是通过计算散列码来决定存储的位置。HashMap 的底层是数组加链表   HashMap 是用哈希表来存储数据的  。哈希表的底层是数组，数组里面是entry 对象  。默认长度是16 。 当像 哈希表里面添加一个对象的时候，会先调用 对象的 hashcode 算法，算出哈希码值 。根据哈希算法算出对应的数组的索引值，再根据索引值查找数组 ，数组中是否存在对象，如果不存在对象直接存进去  。如果数组中存在该对象，会调用对象的equals 方法 ，比较key值是否相等 。如果相等  ，value 值 直接覆盖  。如果不相等  ，则形成链表结构\njdk1.7  先添加的往后移，后添加的排前面  。这种情况就叫哈希碰撞  。这种情况应该尽量避免 ，否则一个索引中 链表的数据量大时  ，该索引中再添加一个对象时 equals 比较会影响效率 。因此  hashmap 提出了加载因子  用来避免碰撞  。加载因子的默认值是0.75 。当元素达到现有表的 75%的时候 进行自动扩容  ，一旦扩容 就会重新排序，减少哈希碰撞 。就是说大小为16的HashMap，到了第13个元素，就会扩容成32。\nJDK1.8中，HashMap在存储一个元素时，当对应链表长度大于8时，链表就转换为红黑树，这样又大大提高了查找的效率。\nHashSet 允许有 null 值。HashSet 是无序的，即不会记录插入的顺序。\nHashSet 不是线程安全的， 如果多个线程尝试同时修改 HashSet，则最终结果是不确定的。 您必须在多线程访问时显式同步对 HashSet 的并发访问。\nHashMap 中，键值对的存储是通过1. put(key,vlaue)\nget方法，传入key，就可以查询到value。 HashMap的底层数组长度总是2的n次方，\nHashtable 是早期Java类库提供的一个哈希表实现，本身是同步的，不支持 null 键和值，由于同步导致的性能开销，所以已经很少被推荐使用。\n解决HashMap的哈希碰撞？ 闭散列也叫开放定址法 or 开散列 链地址法(开链法)\nJava中HashMap是利用“拉链法”处理HashCode的碰撞问题。在调用HashMap的put方法或get方法时，都会首先调用hashcode方法，去查找相关的key，当有冲突时，再调用equals方法。hashMap基于hasing原理，我们通过put和get方法存取对象。当我们将键值对传递给put方法时，他调用键对象的hashCode()方法来计算hashCode，然后找到bucket（哈希桶）位置来存储对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当碰撞发生了，对象将会存储在链表的头节点。hashMap在每个链表节点存储键值对对象。当两个不同的键却有相同的hashCode时，他们会存储在同一个bucket位置的链表中。\n如果两个对象根据 equals() 方法相等，则它们的哈希码必须相同。\nHashCode是使用Key通过Hash函数计算出来的，由于不同的Key，通过此Hash函数可能会算的同样的HashCode，所以此时用了拉链法解决冲突，把HashCode相同的Value连成链表. 但是get的时候根据Key又去桶里找，如果是链表说明是冲突的，此时还需要检测Key是否相同\nHashmap扩容：扩大数组长度，对原数组进行rehash操作，把原数组copy到新数组中 resize()\nMap.Entry是为了更方便的输出map键值对。一般情况下，要输出Map中的key 和 value  是先得到key的集合keySet()，然后再迭代（循环）由每个key得到每个value。values()方法是获取集合中的所有值，不包含键，没有对应关系。而Entry可以一次性获得这两个值。\ngetOrDefault, computeIfAbsent, putIfAbsent\n这三个方法都很像，都是对map中不存在key时的处理。\n这三个函数在执行基于map的分组时会很常用，比如分组求和或者分组生成list。\n其中，get的是只读处理，不会影响map的结构。语义是如果不存在返回指定的默认值，否则返回key对应的value。\n三者语义上的区别：\ngetOrDefault：仅仅是返回值，如果不存在返回指定的默认值，不修改map的结构\nputIfAbsent：key不存在时，塞一个值，不应该关心返回值\ncomputeIfAbsent：获取key对应的value，不存在时塞一个并返回\nprivate final Map&lt;Integer, Node&gt; keyToNode = new HashMap&lt;&gt;();\n在Java中，**final关键字用于声明不可变的变量。当final关键字用于声明类成员变量时，表示这个成员变量的引用不能被修改，即不能再指向新的对象或实例。在示例中， 表示 keyToNode 成员变量在初始化后不能被重新赋值。虽然它所引用的HashMap对象本身可以修改（即向其中添加、删除键值对等操作），但是keyToNode所指向的实例不能再指向另一个HashMap**实例。\n在给**Map&lt;Integer, Node&gt; keyToNode添加final关键字后，这个成员变量在被声明后就不能再指向其他的Map对象。一旦被初始化，keyToNode引用的HashMap对象不能被更改为另一个HashMap**对象。\n主要的原因在于：\n\n线程安全性：在多线程环境下，**final**可以提供一定程度的线程安全，因为它保证了在初始化之后，引用不会发生变化，避免了引用指向其他对象带来的竞态条件和不确定性。\n代码安全性：**final关键字可以确保其他部分的代码不会无意间修改keyToNode**的引用，导致意外的行为。\n\n手写get和put方法：public class MyHashMap{    private int getIndex(int key) {            int hash = Integer.hashCode(key);            hash ^= (hash &gt;&gt;&gt; 16);//高16位不动；低16位与高16位做异或运算 为了增加了结果的随机性            return hash % CAPACITY;        }    public void put(int key, int value) {        int idx = getIndex(key);        Node now = nodes[idx], tmp = now;        if (tmp != null) {            Node prev = null;            while (tmp != null) {                if (tmp.key == key) {                    tmp.value = value;                    return;                }                prev = tmp;                tmp = tmp.next;            }            tmp = prev;        }        Node node = new Node(key, value);        if (tmp != null) {            tmp.next = node;        } else {            nodes[idx] = node;        }    }    public int get(int key) {        int idx = getIndex(key);        Node now = nodes[idx];        if (now != null) {            if (now.key == key) {                return now.value;            } else {                while (now != null) {                    if (now.key == key) {                        return now.value;                    }                    now = now.next;                }            }        }        return -1;    }\n\nQueue\ndeque（double-ended queue，双端队列）是一种具有队列和栈的性质的数据结构。双端队列中的元素可以从两端弹出)。**虽然没有严格要求 Deque 实现禁止插入 null 元素，但强烈鼓励他们这样做。**强烈建议允许使用 null 元素的任何 Deque 实现的用户不要利用插入 null 的能力。这是因为 null 被各种方法用作特殊返回值来指示双端队列为空。\nQueue 中 remove() 和 poll()都是用来从队列头部删除一个元素。\n在队列元素为空的情况下，remove() 方法会抛出NoSuchElementException异常，poll() 方法只会返回 null 。\nhttps://imgconvert.csdnimg.cn/aHR0cHM6Ly9jYXJ0b29uLWJsb2cub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tLzEzOTg1OTQ5NjktNWNjZTVlOWFkYTczZF9hcnRpY2xleC5qcGc?x-oss-process=image/format,png\n使用java的同学请注意，如果你使用Stack的方式来做这道题，会造成速度较慢； 原因的话是Stack继承了Vector接口，而Vector底层是一个Object[]数组，那么就要考虑空间扩容和移位的问题了。 可以使用LinkedList来做Stack的容器，因为LinkedList实现了Deque接口，所以Stack能做的事LinkedList都能做，其本身结构是个双向链表，扩容消耗少。 但是我的意思不是像100%代码那样直接使用一个LinkedList当做队列，那确实是快，但是不符题意。 贴上代码，这样的优化之后，效率提高了40%，超过97%。\nVector 类实现了一个动态数组。和 ArrayList 很相似，但是两者是不同的：\n\nVector 是同步访问的。\nVector 包含了许多传统的方法，这些方法不属于集合框架。\n\n使用场景\n1.需要线程同步\n使用Collections工具类中synchronizedXxx()将线程不同步的ArrayDeque以及LinkedList转换成线程同步。\n2.频繁的插入、删除操作：LinkedList\n3.频繁的随机访问操作：ArrayDeque\n4.未知的初始数据量：LinkedList\nlist.add()      list.remove()\nPriorityQueue\nPriorityQueue和Queue的区别在于，它的出队顺序与元素的优先级有关，对PriorityQueue调用remove()或poll()方法，返回的总是优先级最高的元素。e.g. 我们放入的顺序是\"apple\"、“pear”、“banana”，但是取出的顺序却是\"apple\"、“banana”、“pear”，这是因为从字符串的排序看，\"apple\"排在最前面，\"pear\"排在最后面。\n因此，放入PriorityQueue的元素，必须实现Comparable接口，PriorityQueue会根据元素的排序顺序决定出队的优先级。\n优先队列默认结构为二叉小顶堆（层序遍历） 弹出最小元素\n小顶堆：每个结点的值都小于或等于其左右孩子结点的值\n大顶堆：每个结点的值都大于或等于其左右孩子结点的值\n（升序–》大根堆，降序–》小根堆）\n不清楚优先队列的同学建议先看一下这位dalao的博客\n**升序下的小顶堆（默认情况）**queue.offer(1); queue.offer(9); queue.offer(5); queue.offer(6); queue.offer(8);\nreturn [1,6,5,9,8]\n（1）add：插入一个元素，不成功会抛出异常\npublic boolean add(E e) { return offer(e);}\n我们看到add方法其实是通过调用offer方法实现的。我们直接看offer方法\n（2）offer：插入一个元素，不能被立即执行的情况下会返回一个特殊的值（true 或者 false）\nStream\nJava 8 是一个非常成功的版本，这个版本新增的Stream，配合同版本出现的Lambda闭包 ，给我们操作集合（Collection）提供了极大的便利。Stream流是JDK8新增的成员，允许以声明性方式处理数据集合，可以把Stream流看作是遍历数据集合的一个高级迭代器。Stream 是 Java8 中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找/筛选/过滤、排序、聚合和映射数据等操作。使用Stream API 对集合数据进行操作，就类似于使用 SQL 执行的数据库查询。也可以使用 Stream API 来并行执行操作。简而言之，Stream API 提供了一种高效且易于使用的处理数据的方式。\n————————————————\n流是什么?\n从支持数据处理操作的源生成元素序列.数据源可以是集合,数组或IO资源。\n从操作角度来看,流与集合是不同的. 流不存储数据值; 流的目的是处理数据,它是关于算法与计算的。\n如果把集合作为流的数据源,创建流时不会导致数据流动; 如果流的终止操作需要值时,流会从集合中获取值; 流只使用一次。\n流中心思想是延迟计算,流直到需要时才计算值。\n1、Streams\n新增的 Stream API（java.util.stream）将函数式编程引入了 Java 库中。这是目前为止最大的一次对 Java 库的完善，以便开发者能够写出更加有效、更加简洁和紧凑的代码。Steam API 极大的简化了集合操作（后面我们会看到不止是集合）。\nStream概述\n\n非主流式定义: 像写SQL一样来处理集合；\n理解类定义: 流式处理；\n流式处理学习路线: 创建,操作(中间操作,终止操作)；\n\nStream创建\n\nCollection等集合接口实现的stream()方法和parallelStream()方法；\nArrays提供的数组流；\nStream类提供的静态创建方法of()；\n\n创建Stream流的三种方式\nStream中间操作\n\n过滤:：filter；\n切片 &amp; 分页：sklip , limit；\n去重：distinct(去重的对象需要实现hashCode和equals)；\n映射：map, flatMap；\n排序：sort；\n\nStream终止操作\n\n匹配\n\nallMatch：检查是否匹配所有元素；\nanyMatch：检查是否匹配至少一个元素；\nnoneMatch：检查是否没有匹配所有元素\n\n\n查找\n\nfindFirst：查找找到的第一个元素；\nfindAny：查找找到的任意一个元素\n\n\n计算流中元素个数：count()；\n计算流中元素的最大/最小值：max() , min()；\n内部迭代：forEach；\n收集：collect；\n\nFile Stream\n1 处理的数据单位不同，可分为：字符流，字节流\n2.数据流方向不同，可分为：输入流，输出流\n3.功能不同，可分为：节点流，处理流。\n过滤流就是对节点流进行一系列的包装。例如BufferedInputStream和BufferedOutputStream，使用已经存在的节点流来构造，提供带缓冲的读写，提高了读写的效率，以及DataInputStream和DataOutputStream，使用已经存在的节点流来构造，提供了读写Java中的基本数据类型的功能。他们都属于过滤流。bytestream        characterstream\nout就是System里面的一个静态数据成员，而且这个成员是java.io.PrintStream类的引用。如下图，被关键字static修饰的成员可以直接通过\"类名.成员名\"来引用，而无需创建类的实例。所以System.out是调用了System类的静态数据成员out。\nOutputStreamWriter是从字符流到字节流的桥接怎么理解？\n1、字符的输出需要通过字符流来操作，但是本质最后还是通过字节流输出到计算机上进行存储的2、 因此OutputStreamWriter流的作用就是利用字节流作为底层输出流然后构建字符输出流，字符输出流输出字符到流中，然后通过指定的字符集把流中的字符编码成字节输出到字节流中，其作用就是一个桥梁，使得双方链接起来\nOutputStream需要封装在PrintWriter中以使用 println.\n在try and catch中写read and write\nFileReader fr = new FileReader()\nBufferedReader   in = new BufferedReader(new InputStreamReader(socket.getInputStream()));\nPrintWriter out = new PrintWriter(new BufferedWriter(new OutputStreamWriter(socket.getOutputStream())), true);\nJava web生态\nRPC 的全称是 Remote Procedure Call 是一种进程间通信方式。它允许程序调用另一个地址空间(通常是共享网络的另一台机器上)的过程或函数，而不用程序员显式编码这个远程调用的细节。即无论是调用本地接口/服务的还是远程的接口/服务，本质上编写的调用代码基本相同。\nRPC 会隐藏底层的通讯细节(不需要直接处理Socket通讯或Http通讯)\nRPC 是一个请求响应模型。客户端发起请求，服务器返回响应(类似于Http的工作方式)\nRPC 在使用形式上像调用本地函数(或方法)一样去调用远程的函数(或方法)。\nSpring\nSpring是一个轻量级的IoC和AOP容器框架。是为Java应用程序提供基础性服务的一套框架，目的是用于简化企业应用程序的开发，它使得开发者只需要关心业务需求。\nIOC，Inversion of Control，控制反转，指将对象的控制权转移给Spring框架，由 Spring 来负责控制对象的生命周期（比如创建、销毁）和对象间的依赖关系。\n最直观的表达就是，以前创建对象的时机和主动权都是由自己把控的，如果在一个对象中使用另外的对象，就必须主动通过new指令去创建依赖对象，使用完后还需要销毁（比如Connection等），对象始终会和其他接口或类耦合起来。而 IOC 则是由专门的容器来帮忙创建对象，将所有的类都在 Spring 容器中登记，当需要某个对象时，不再需要自己主动去 new 了，只需告诉 Spring 容器，然后 Spring 就会在系统运行到适当的时机，把你想要的对象主动给你。也就是说，对于某个具体的对象而言，以前是由自己控制它所引用对象的生命周期，而在IOC中，所有的对象都被 Spring 控制，控制对象生命周期的不再是引用它的对象，而是Spring容器，由 Spring 容器帮我们创建、查找及注入依赖对象，而引用对象只是被动的接受依赖对象，所以这叫控制反转。\n（2）什么是DI：    IoC 的一个重点就是在程序运行时，动态的向某个对象提供它所需要的其他对象，这一点是通过DI（Dependency Injection，依赖注入）来实现的，即应用程序在运行时依赖 IoC 容器来动态注入对象所需要的外部依赖。而 Spring 的 DI 具体就是通过反射实现注入的，反射允许程序在运行的时候动态的生成对象、执行对象的方法、改变对象的属性\nAOP\n面向切面编程(AOP)常用于在一个访问过程中纵向切开，插入控制访问。但是AOP编程不会影响正常的执行流程，仅仅是在正常执行流程关键点进行切入编程。举个例子：公司有个人力资源管理系统已经上线，但是系统运行不稳定，有时运行很慢。为了检测系统哪里出了问题，同时又不能破坏原有系统，开发人员想要监控每个方法的执行时间，再根据这些执行时间判断问题所在。当问题解决后，再把这些监控移除。由于系统目前已经运行，如果手动修改系统中成千上万个方法，工作量非常大并且对原系统侵入太深，而且后续移除又很麻烦。\n这时，在系统运行过程中动态添加代码，以相同代码检测每一个方法的执行，这就是采用面向切面编程。Spring框架对于AOP提供了很好的支持。在AOP中，有一些常见的概念首先介绍给大家：\n①Joinpoint（连接点）：类中可以被增强的方法被称为链接点。例如，想修改哪个方法的功能，那么该方法就是一个连接点。\n②Pointcut（切入点）：对Joinpoint进行拦截的定义即为切入点。例如，拦截所有以insert开始的方法，这个定义即为切入点。\n③Advice（通知）：拦截到Joinpoint后要做的事就是通知。例如，打印日志监控，通知分为前置通知、后置通知、异常通知、最终通知和环绕通知。\n④Aspect（切面）：Pointcut和Advice结合就是一个切面。\n⑤Target（对象）：要增强的类被称为Target。\n工厂设计模式 : Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。\n代理设计模式 : Spring AOP 功能的实现。\n单例设计模式 : Spring 中的 Bean 默认都是单例的。\n模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。\n包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。\n观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。\n适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配 Controller。\nhttps://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg-blog.csdnimg.cn%2F20201108145728249.png%3Fx-oss-process%26%2361%3Bimage%2Fwatermark%2Ctype_ZmFuZ3poZW5naGVpdGk%2Cshadow_10%2Ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dobDg2MTRqb2hu%2Csize_16%2Ccolor_FFFFFF%2Ct_70%23pic_center&amp;refer=http%3A%2F%2Fimg-blog.csdnimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1653288524&amp;t=633df1c1d9d18223b8b1c9f20ef7713e\n\n发起请求到中央调度器 DispatcherServlet。\n中央调度器接收到请求后，调用 HandlerMapping 映射器查找 Handler。\nHandlerMapping 映射器根据xml配置、注解进行查找具体的处理器 Handler，生成处理器对象及连接器一并向中央调度器返回 Handler。\n中央调度器调用 HandlerAdapter 设配器执行Handler。\nHandlerAdapter 适配器经过适配调用具体的处理器（Controller 也称：控制器）进行业务逻辑操作。\nHandler执行完成给适配器返回ModelAndView。\nHandlerAdapter 将 Handler 执行结果 ModelAndView 返回给中央调度器（ModelAndView是springmvc框架的一个底层对象，包括 Model和view）。\n中央调度器将 ModelAndView 传给 ViewResolver 视图解析器进行视图解析，根据逻辑视图名解析成真正的视图(jsp)。\nViewResolver 视图解析器向中央调度器返回View。\n中央调度器进行视图渲染，视图渲染将模型数据(在ModelAndView对象中)填充到request域。\n前端控制器向用户响应结果。\n\nvs. springboot\nspringboot可以创建独立的Spring应用、嵌入式Tomact、Jetty、Undertow容器、提供starters简化构建配置、尽可能自动配置spring应用、提供生产指标、完全没有代码生成和XML配置要求\n区别\n1、Maven依赖\nSpringBoot只需要一个依赖（spring-boot-starter-web）就能启动和运行Web应用程序，但是Spring却最少需要两个依赖。\n2、测试库\n我们测试库常用到的SpringTest、JUnit、Hamcres和Mockito库，在SpringBoot中只需要添加spring-boot-starter-test依赖就可以自动包含上面这些库了，但是在Spring中就需要将上面所有的库都要添加到依赖上。\n3、MVC配置\n当我们创建JSPWeb应用程序时，Spring需要定义调度程序servlet，映射和其他支持配置，对此我们可以使用web.xml文件或Inirializer类来做这件事。但是SpringBoot只需要在application配置文件中配置几个属性就可以代替Spring的那些操作\n4、ThyMeleaf模板引擎配置\n在SpringBoot1X只需要 spring-boot-starter-thymeleaf的依赖项来启用 Web应用程序中的 Thymeleaf支持。但是由于 Thymeleaf3.0中的新功能，我们必须将 thymeleaf-layout-dialect 添加为SpringBoot2XWeb应用程序中的依赖项。配置好依赖，我们就可以将模板添加到 src/main/resources/templates文件夹中， SpringBoot将自动显示它们。\n5、引导配置\n我们用SpringBoot它程序的入口是SpringAppliaction注释的类，但是Spring就比较麻烦了，它支持传统的web.xml引导方式和最新的Servlet3+方法。\nJQuery和BootStrap是前端开发的两个重要框架，其中JQuery主要提供JavaScript框架，而BootStrap提供CSS排版框架。在SpringBoot开发中，引入JQuery和BootStrap有两种方法。AJAX——核心XMLHttpRequest对象，而JQuery也对Ajax异步操作进行了封装，这里看一下几种常用的方式。 ，.post， ，.getJSON。\nAjax（Asynchronous JavaScript And XML）是一种异步交互模式，实现异步页面刷新。JQuery自带Ajax功能，可以基于JQuery实现Ajax。XMLHttpRequest(XHR) 用于在后台与服务器交换数据。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。\n\n\n\nopen(method,url,async)\n规定请求的类型、URL 以及是否异步处理请求。method：请求的类型；GET 或 POSTurl：文件在服务器上的位置async：true（异步）或 false（同步）\n\n\n\n\nsend(string)\n\n\n\n\n每当 readyState 改变时，就会触发 onreadystatechange 事件。\nreadyState 属性存有 XMLHttpRequest 的状态信息\nxmlhttp.onreadystatechange=function() {\nif (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200)    {        document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText;    } }\nSerialization\n序列化 (Serialization)是将对象的状态信息转换为可以存储或传输的形式的过程。在序列化期间，对象将其当前状态写入到临时或持久性存储区。以后，可以通过从存储区中读取或反序列化对象的状态，重新创建该对象。\n序列化使其他代码可以查看或修改，那些不序列化便无法访问的对象实例数据。确切地说，代码执行序列化需要特殊的权限：即指定了 SerializationFormatter 标志的 SecurityPermission。在默认策略下，通过 Internet 下载的代码或 Internet 代码不会授予该权限；只有本地计算机上的代码才被授予该权限。\n**Serializable接口：**一个对象序列化的接口，一个类只有实现了Serializable接口，它的对象才能被序列化。一般用在实体类中\nSerializable接口其实是给jvm看的，通知jvm，我不对这个类做序列化了，你(jvm)帮我序列化就好了。Serializable接口就是Java提供用来进行高效率的异地共享实例对象的机制，实现这个接口即可。\n实现了Serializable接口的类可以被ObjectOutputStream转换为字节流，同时也可以通过ObjectInputStream再将其解析为对象。\n底层IO操作都是以字节流的方式进行的，所以写操作都涉及将编程语言数据类型转换为字节流，而读操作则又涉及将字节流转化为编程语言类型的特定数据类型。而Java作为一门面向对象的编程语言，对象作为其主要数据的类型载体，为了完成对象数据的读写操作，也就需要一种方式来让JVM知道在进行IO操作时如何将对象数据转换为字节流，以及如何将字节流数据转换为特定的对象，而Serializable接口就承担了这样一个角色。\n目的\n1、以某种存储形式使自定义对象持久化；    2、将对象从一个地方传递到另一个地方。           3、使程序更具维护性。\nThread并发\nJava5开始，在Java.util.concurrent包下提供了大量支持高效并发访问的集合接口和实现类\nN threads over M cores: 1. N&lt;=M 真并行处理 2. N&gt;M pseudo parallel    只有runnable到running时才会占用cpu时间片\n3特性：安全性，可见性，有序性。\n1 线程安全是程式设计中的术语，指某个函数、函数库在多线程环境中被调用时，能够正确地处理多个线程之间的共享变量，使程序功能正确完成。\n2 可见性，JVM提供了Volatile和Synchronized。通过volatile、synchronized、Lock手段达到有序性。\n线程是并行的最小单位，一个线程实现一个功能\n守护线程\n在Java中有两类线程：User Thread(用户线程)、Daemon Thread(守护线程)\nAtomicity\ncannot be interrupted, and once it starts is always completes.\nJLS定义了线程对主存的操作指令：lock，unlock，read，load，use，assign，store，write。这些行为是不可分解的原子操作，在使用上相互依赖，read-load从主内存复制变量到当前工作内存，use-assign执行代码改变共享变量值，store-write用工作内存数据刷新主存相关内容。\n\nread（读取）：作用于主内存变量，把一个变量值从主内存传到线程的工作内存中，以便随后的load动作使用；\nload（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。\nuse（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。\nassign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。\nstore（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。\nwrite（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。\n\n采用继承Thread类方式：（1）优点：编写简单，直接使用this便可访问当前线程，无需使用Thread.currentThread()方法。（2）缺点：JAVA中不支持多重继承，所以线程类继承Thread类后便不能再继承其他父类。\n采用实现Runnable接口方式：\n（1）优点：可实现多个接口，还可以继承其他的类。可多个线程共享同一个目标对象，适合用于多个相同线程处理同一份资源，从而将CPU代码和数据分开，形成清晰的模型，较好地体现面向对象的思想。\n（2）缺点：编程稍微复杂，访问当前线程，必须使用**Thread.currentThread()**方法。\n1.**start（）方法来启动线程，真正实现了多线程运行。**这时无需等待run方法体代码执行完毕，可以直接继续执行下面的代码；通过调用Thread类的start()方法来启动一个线程， 这时此线程是处于就绪状态， 并没有运行。 然后通过此Thread类调用方法run()来完成其运行操作的， 这里方法run()称为线程体，它包含了要执行的这个线程的内容， Run方法运行结束， 此线程终止。然后CPU再调度其它线程。在main线程中 create a new thread\n2.**run（）方法当作普通方法的方式调用。**程序还是要顺序执行，要等待run方法体执行完毕后，才可继续执行下面的代码； 程序中只有主线程——这一个线程， 其程序执行路径还是只有一条， 这样就没有达到写线程的目的。\n每当使用 Java 命令执行一个类时，实际上都会启动一个 JVM，\n每一个JVM实际上就是在操作系统中启动一个线程\n，Java 本身具备了垃圾的收集机制。所以在 Java 运行时至少会启动两个线程，一个是 main 线程，另外一个是垃圾收集线程。\n!https://img2018.cnblogs.com/blog/1655301/201909/1655301-20190928151648653-69670221.png\nOS: 假如线程正在运行中，程序向硬盘发送访问请求然后等待，这时CPU空转，所以线程进入阻塞状态，CPU执行其他线程。\nsleep相当于让线程睡眠，交出CPU，让CPU去执行其他的任务。但是有一点要非常注意，sleep方法不会释放锁，\n直接调用interrupt方法不能中断正在运行中的线程。但是如果配合isInterrupted()能够中断正在运行的线程，因为调用interrupt方法相当于将中断标志位置为true\n!https://images2015.cnblogs.com/blog/682616/201611/682616-20161115184256092-1439402886.jpg\nsleep() yield() join()\n① sleep()方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会； 因此，Thread.Sleep(0)的作用，就是“触发操作系统立刻重新进行一次CPU竞争”。睡眠1毫秒。这也是我们在大循环里面经常会写一句Thread.Sleep(0) ，因为这样就给了其他线程比如Paint线程获得CPU控制权的权力，这样界面就不会假死在那里。\n② 线程执行sleep()方法后转入TIMED_WAITING状态，而执行yield()方法后转入就绪（ready）状态；\n③ sleep()方法声明抛出InterruptedException，而yield()方法没有声明任何异常；\nyield()方法并不会让线程进入阻塞状态，而是让线程重回就绪状态，它只需要等待重新获取CPU执行时间，所以执行yield()的线程有可能在进入到可执行状态后马上又被执行。还有一点和 sleep 不同的是 yield 方法只能使同优先级或更高优先级的线程有执行的机会\njoin()方法阻塞正在运行的线程，在线程B中调用了线程A的Join()方法**(B 暂停wait()，直到线程A执行完毕后(B notify()，才会继续执行线程B。如果调用此方法的线程没有执行完毕，程序不会往下继续执行）**，也就是等到该方法的线程执行完毕后程序再往下继续执行。注意该方法也需要捕捉异常。\nwait()、notify()\nwait 方法是属于 Object 类中的，wait 过程中线程会释放Condition对象锁，只有当其他线程调用 notify 才能唤醒此线程。\n只有当 notify/notifyAll() 被执行时候，才会唤醒一个或多个正处于等待状态的线程，然后继续往下执行，直到执行完synchronized 代码块的代码或是中途遇到wait() ，再次释放锁。\n也就是说，notify/notifyAll() 的执行只是唤醒沉睡的线程，而不会立即释放锁，锁的释放要看代码块的具体执行情况。所以在编程中，尽量在使用了notify/notifyAll() 后立即退出临界区，以唤醒其他线程让其获得锁\n4、wait() 需要被try catch包围，以便发生异常中断也可以使wait等待的线程唤醒。\nsynchronized (obj) {                System.out.println(\"thread1 start\");                try {                    obj.wait();                } catch (InterruptedException e) {                    e.printStackTrace();\nInterrupt\n对一个线程，调用 interrupt() 时，\n① 如果线程处于被阻塞状态（例如处于sleep, wait, join 等状态），那么线程将立即退出被阻塞状态，并抛出一个InterruptedException异常。仅此而已。\n② 如果线程处于正常活动状态，那么会将该线程的中断标志设置为 true，仅此而已。被设置中断标志的线程将继续正常运行，不受影响。\ninterrupt() 并不能真正的中断线程，需要被调用的线程自己进行配合才行。\n也就是说，一个线程如果有被中断的需求，那么就可以这样做。\n① 在正常运行任务时，经常检查本线程的中断标志位，如果被设置了中断标志就自行停止线程。\n每一个线程都有一个boolean类型的标志，此标志意思是当前的请求是否请求中断，默认为false。当一个线程A调用了线程B的interrupt方法时，那么线程B的是否请求的中断标志变为true。而线程B可以调用方法检测到此标志的变化。\n\n阻塞方法：如果线程B调用了阻塞方法（wait,sleep,join），如果是否请求中断标志变为了true，那么它会抛出InterruptedException异常。抛出异常的同时它会将线程B的是否请求中断标志置为false\n**非阻塞方法：**可以通过线程B的isInterrupted方法进行检测是否请求中断标志为true还是false，另外还有一个静态的方法interrupted方法也可以检测标志。但是静态方法它检测完以后会自动的将是否请求中断标志位置为false。例如线程A调用了线程B的interrupt的方法，那么如果此时线程B中用静态interrupted方法进行检测标志位的变化的话，那么第一次为true，第二次就为false。\n\nsleep、wait和join这些方法的内部会不断的检查中断状态的值，从而自己抛出InterruptEdException。\npublic void run() {    try {        // 1. isInterrupted()保证，只要中断标记为true就终止线程。        while (!isInterrupted()) {            // 执行任务...        }    } catch (InterruptedException e) {             // Restore the interrupted status             Thread.currentThread().interrupt();         }    //，因为你的程序是实现了Runnable接口，然后在重写Runnable接口的run方法的时候，那么子类抛出的异常要小于等于父类的异常。而在Runnable中run方法是没有抛异常的。所以此时是不能抛出InterruptedException异常。如果此时你只是记录日志的话，那么就是一个不负责任的做法，因为在捕获InterruptedException异常的时候自动的将是否请求中断标志置为了false。至少在捕获了InterruptedException异常之后，如果你什么也不想做，那么就将标志重新置为true，以便栈中更高层的代码能知道中断，并且对中断作出响应。}\nstatic boolean interrupted()\n测试当前线程（正在执行这一命令的线程）是否被中断。这一调用会将当前线程的中断状态重置为 false\nboolean isInterrupted()   更好\n测试线程是否被终止。不像静态的中断方法，这一调用不改变线程的中断状态\n一个while循环中使用Thread.interrupted()作为条件,则在循环中断之后,您不知道是否终止,因为Thread.interrupted()返回true或其他一些条件已更改或一个休息声明跑了所以在这种情况下,使用Thread.currentThread().isInterrupted()真的是你唯一的选择.\nsynchronized和Lock\nJava中存在两种锁机制：synchronized和Lock，Lock接口及其实现类是JDK5增加的内容\nsynchronized:  Java的关键字，在jvm层面上                 Lock: 是一个接口\n用法\nsynchronized: 在需要同步的对象中加入此控制，synchronized可以加在方法上，也可以加在特定代码块中，括号中表示需要锁的对象。\nLock: 一般使用ReentrantLock类做为锁。在加锁和解锁处需要通过lock()和unlock()显示指出。所以一定要在finally块中写unlock()以防死锁。\n锁的释放\nsynchronized: 1、以获取锁的线程执行完同步代码，释放锁 2、线程执行发生异常，jvm会让线程释放锁。在发生异常时候会自动释放占有的锁，因此不会出现死锁\nLock: 在finally中必须释放锁，不然容易造成线程死锁。必须手动unlock来释放锁，可能引起死锁的发生\n锁的获取\nsynchronized: 假设A线程获得锁，B线程等待。如果A线程阻塞，B线程会一直等待\nLock: 分情况而定，Lock有多个锁获取的方式，大致就是可以尝试获得锁，线程可以不用一直等待(可以通过tryLock判断有没有锁)\n锁的状态         synchronized: 无法判断                           Lock: 可以判断\n性能\nsynchronized: 少量同步\nLock: 大量同步\n\nLock可以提高多个线程进行读操作的效率。（可以通过readwritelock实现读写分离）\n在资源竞争不是很激烈的情况下，Synchronized的性能要优于ReetrantLock，但是在资源竞争很激烈的情况下，Synchronized的性能会下降几十倍，但是ReetrantLock的性能能维持常态；\nReentrantLock提供了多样化的同步，比如有时间限制的同步，可以被Interrupt的同步（synchronized的同步是不能Interrupt的）等。在资源竞争不激烈的情形下，性能稍微比synchronized差点点。但是当同步非常激烈的时候，synchronized的性能一下子能下降好几十倍。而ReentrantLock确还能维持常态。公平锁机制。什么叫公平锁呢？也就是在锁上等待时间最长的线程将获得锁的使用权。通俗的理解就是谁排队时间最长谁先执行获取锁。\n\n\n② 是否可手动释放\nsynchronized 不需要用户去手动释放锁，synchronized 代码执行完后系统会自动让线程释放对锁的占用； ReentrantLock则需要用户去手动释放锁，如果没有手动释放锁，就可能导致死锁现象。一般通过lock()和unlock()方法配合try/finally语句块来完成，使用释放更加灵活。\n③ 是否可中断\nsynchronized是不可中断类型的锁，除非加锁的代码中出现异常或正常执行完成； ReentrantLock则可以中断，可通过trylock(long timeout,TimeUnit unit)设置超时方法或者将lockInterruptibly()放到代码块中，调用interrupt方法进行中断。\n④ 是否公平锁\nsynchronized为非公平锁 ReentrantLock则即可以选公平锁也可以选非公平锁，通过构造方法new ReentrantLock时传入boolean值进行选择，为空默认false非公平锁，true为公平锁。\n可重入锁是指同一个线程如果首次获得了这把锁，那么因为它是这把锁的拥有者，因此 有权利再次获取这把锁\n公平锁是等待锁的线程不会饿死。缺点是整体吞吐效率相对非公平锁要低，等待队列中除第一个线程以外的所有线程都会阻塞，CPU唤醒阻塞线程的开销比非公平锁大。\n非公平锁的优点是可以减少唤起线程的开销，整体的吞吐效率高，因为线程有几率不阻塞直接获得锁，CPU不必唤醒所有线程。缺点是处于等待队列中的线程可能会饿死，或者等很久才会获得锁。\n调度\nsynchronized: 使用Object对象本身的wait 、notify、notifyAll调度机制\nLock: 可以使用Condition进行线程之间的调度\n底层实现\nsynchronized: 底层使用指令码方式来控制锁的，映射成字节码指令就是增加来两个指令：monitorenter和monitorexit。当线程执行遇到monitorenter指令时会尝试获取内置锁，如果获取锁则锁计数器+1，如果没有获取锁则阻塞；当遇到monitorexit指令时锁计数器-1，如果计数器为0则释放锁。\nLock: 底层是CAS乐观锁，依赖AbstractQueuedSynchronizer类，把所有的请求线程构成一个CLH队列。而对该队列的操作均通过Lock-Free（CAS）操作。每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。\n独占锁是一种悲观锁每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。，synchronized就是一种独占锁；它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起直到持有锁的线程释放锁。\n所谓乐观锁就是每次不加锁,假设没有冲突而去完成某项操作;如果发生冲突了那就去重试，直到成功为止。\nCAS(Compare And Swap)是一种有名的无锁算法CPU层面。CAS算法是乐观锁的一种实现。CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B并返回true，否则返回false。\n注:synchronized和ReentrantLock都是悲观锁。ReentrantLock 是悲观锁。ReentrantLock 一上来调用 lock 方法，就直接锁住了，其他获取不到锁的线程都会被加入到 AQS 的同步等待队列中，这还能叫乐观锁？CAS 是乐观的，不代表基于 CAS 实现的 ReentrantLock 是乐观的。\npublic void test () throw Exception {    // 1.初始化选择公平锁、非公平锁    ReentrantLock lock = new ReentrantLock(true);    // 2.可用于代码块    lock.lock();    try {        try {            // 3.支持多种加锁方式，比较灵活; 具有可重入特性            if(lock.tryLock(100, TimeUnit.MILLISECONDS)){ }        } finally {            // 4.手动释放锁            lock.unlock()        }    } finally {        lock.unlock();   }\n注:什么时候使用悲观锁效率更高、什么使用使用乐观锁效率更高？\n实际生产环境里边，如果并发量不大，完全可以使用悲观锁定的方法，这种方法使用起来非常方便和简单。\n但是如果系统的并发非常大的话，悲观锁定会带来非常大的性能问题，所以就要选择乐观锁定的方法。\n\n\nsynchronized修饰一个代码块，被修饰的代码块称为同步语句块，其作用的范围是大括号{}括起来的代码，作用的对象是调用这个代码块的对象；\tthis.notifyAll()会自动执行 解锁对象\n\n\n修饰一个方法，被修饰的方法称为同步方法，其作用的范围是整个方法，作用的对象是调用这个方法的对象；\n\n\n修改一个静态的方法，其作用的范围是整个静态方法，作用的对象是这个类的所有对象；\n\n\n修改一个类，其作用的范围是synchronized后面括号括起来的部分，作用主的对象是这个类的所有对象。*\n同步锁\n\n\n一个线程访问一个对象中的synchronized(this)同步代码块时，其他试图访问该对象的线程将被阻塞。当两个并发线程(thread1和thread2)访问同一个对象(syncThread)中的synchronized代码块时，在同一时刻只能有一个线程得到执行，另一个线程受阻塞，必须等待当前线程执行完这个代码块以后才能执行该代码块。    synchronized是对类的当前实例进行加锁，防止其他线程同时访问该类的该实例的所有synchronized块，注意这里是“ 类的当前实例 ”，类的两个不同实例就没有这种约束了。那么static synchronized恰好就是要控制类的所有实例的访问了.synchronized相当于this.synchronized，而staticsynchronized相当于类名.synchronized.\n当一个线程访问对象的一个synchronized(this)同步代码块时，另一个线程仍然可以访问该对象中的非synchronized(this)同步代码块。\n线程池\n线程池顾名思义就是事先创建若干个可执行的线程放入一个池中（容器），需要的时候从池中获取线程不用自行创建，使用完毕不需要销毁线程而是放回池中，从而减少创建和销毁线程对象的开销。任何池化技术都是减低资源消耗、解决资源分配，例如我们常用的 数据库连接池。https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html\n\n内存池(Memory Pooling)：预先申请内存，提升申请内存速度，减少内存碎片。\n连接池(Connection Pooling)：预先申请数据库连接，提升申请连接的速度，降低系统的开销。\n实例池(Object Pooling)：循环使用对象，减少资源在初始化和释放时的昂贵损耗。\n\n第2个问题： 为什么要用线程池？\n1降低资源消耗          2提高响应速度        3提高线程的可管理性\n（减少分配和释放现成的开销和控制线程数量），向下进一步解为什么要用多线程（可以并发工作），以及什么是线程（OS对任务执行的抽象），OS大概是如何调度的等（抢占式/协作式等）。\n而向上就能可以理解如何去管理一组资源（用列表/集合/树），如何去“borrow”和“return”一个资源（锁/CAS），线程的创建释放策略（比如固定数量，定义最小最大数量，根据请求负载和某种算法做自适应等），还有一些设计模式上的考虑（池的实现可以切换而不改变接口；核心代码可以抽象为管理“池”而不仅仅是线程池；能够模拟为异步不阻塞的操作等），以及一些性能上的考量。\n如何运行\nThreadPoolExecutor线程池在内部实际上构建了一个生产者消费者模型，将线程和任务两者解耦，并不直接关联，从而良好的缓冲任务，复用线程。线程池的运行主要分成两部分：任务管理、线程管理。\n任务管理部分充当生产者的角色，当任务提交后，线程池会判断该任务后续的流转：（1）直接申请线程执行该任务；（2）缓冲到队列中等待线程执行；（3）拒绝该任务。线程管理部分是消费者，它们被统一维护在线程池内，根据任务请求进行线程的分配，当线程执行完任务后则会继续获取新的任务去执行，最终当线程获取不到任务的时候，线程就会被回收。\n参数：\n1.corePoolSize：核心池的大小；创建了线程池之后，默认情况下，线程池中没有线程，而是等待任务来了之后，创建线程执行任务，当线程的个数达到核心池大小后，后来的任务就会放在缓存队列中。\n2.maximumPoolSize：线程池最大线程数，线程池所能容纳的线程。\n3.keepAliveTime：多余的线程没有任务执行之后，线程在线程池中最多待多久的时间才销毁，直到只剩下corePoolSize个线程为止。\n4.TimeUnit：参数keepAliveTime的时间单位，一共7种取值\n线程的同步与互斥：\n互斥：当一个公共资源同一时刻只能被一个进程或线程使用，多个进程或线程不能同时使用公共资源。如：当线程A在使用打印机时，其他线程都需要等待。\n同步：两个或两个以上的进程或线程在运行过程中协同步调，按预定的先后次序运行。如：A任务的运行依赖B任务产生的数据。\n同步保证了线程运行的顺序性，互斥保证了线程运行的安全性。\n互斥具有唯一性和排它性，访问是无序的。\n由于线程共享进程的资源和地址空间，所以在访问到他们的公共资源的时候，一定会出现线程的同步和互斥现象，多线程访问一个数据的次序一定是杂乱无章的，所以我们引入互斥锁保证在某一端时间内只有一个线程在执行某些代码，条件变量完成同步过程。\n实现线程同步互斥的四种方式：\n1.临界区：适合一个进程内的多线程访问公共区域或代码段使用\n2.互斥量：适合不同进程内多线程访问公共区域或代码段使用，与临界区相似\n3.事件：通过线程间触发事件实现同步互斥\n4.信号量：与临界区和互斥量不同，可以实现多个线程同时访问公共区域数据，原理与操作系统中的PV操作类似，先设置一个访问公共区域的线程最大连接数，每有一个线程访问共享区资源数就减1，直到资源数小于等于0。\nDeadlock,starvation,race conditon\nA deadlock is a state in which each member of a group of actions, is waiting for some other member to release a lock. A livelock on the other hand is almost similar to a deadlock, except that the states of the processes involved in a livelock constantly keep on changing with regard to one another, none progressing. Thus Livelock is a special case of resource starvation, as stated from the general definition, the process is not progressing.\n只有具备全部的4个条件时，才会出现死锁\n互斥：共享资源 X 和 Y 只能被一个线程占用；\n占有且等待：线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X；\n不可抢占：其他线程不能强行抢占线程 T1 占有的资源；\n循环等待：线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待。\n如何去避免死锁：思路就是打破产生死锁的四个条件，因为锁本身就是通过互斥来解决线程安全问题的，所以不可打破，所以要打破其余3个条件，（1）打破占有且等待，一次性申请所有资源；（2）打破不可抢占，占有部分资源的线程，进一步申请其他资源时，如果申请不到，主动释放占有的资源；（3）打破循环等待，按序申请资源，指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的。\nLinux的锁机制：\n互斥锁：mutex，用于保证在任何时刻，都只能有一个线程访问该对象。当获取锁操作失败时，线程会进入睡眠，等待锁释放时被唤醒\n读写锁：rwlock，分为读锁和写锁。处于读操作时，可以允许多个线程同时获得读操作。但是同一时刻只能有一个线程可以获得写锁。其它获取写锁失败的线程都会进入睡眠状态，直到写锁释放时被唤醒。 注意：写锁会阻塞其它读写锁。当有一个线程获得写锁在写时，读锁也不能被其它线程获取；写者优先于读者（一旦有写者，则后续读者必须等待，唤醒时优先考虑写者）。适用于读取数据的频率远远大于写数据的频率的场合。\nStarvation:\n饥饿是一个与活锁和死锁密切相关的问题。在动态系统中，对资源的请求不断发生。因此，需要一些策略来决定谁何时获得资源。这个过程是合理的，可能会导致一些进程永远不会得到服务，即使它们没有死锁。 导致队列增长两次然后它  可以消费，从而导致计算不足 }\n当“贪婪”线程使共享资源长时间不可用时，就会发生饥饿。例如，假设一个对象提供了一个通常需要很长时间才能返回的同步方法。如果一个线程频繁调用这个方法，其他同样需要频繁同步访问同一对象的线程也会经常被阻塞。\n避免race condition\n1.避免使用共享变量 在多线程环境中，race condtion的出现往往伴随着共享变量，如上面例子中的账户余额和count变量，都可以被多个线程读取操作，那么避免问题发生最简单直接的办法就是避免使用共享变量。可以使用不可变对象或者线程本地对象（例如java中的threadlocal）。 2.使用同步或者原子操作 使用同步可以避免race condition的问题，但是线程同步往往伴随很多同步的性能开销，还可能导致死锁。两个办法是使用原子操作，例如java中的提供的原子类。\nmonitor\nfeatrures: 1. mutual exclusion(supported by synchronized)\n2.cooperation(supported by wait() and notify())    线程协调工作\n在不同的锁状态下，Mark word会存储不同的信息，这也是为了节约内存常用的设计。当锁状态为重量级锁（锁标识位=10）时，Mark word中会记录指向\nMonitor对象\n的指针，这个Monitor对象也称为\n管程或监视器锁。\nsynchronzied 需要关联一个对象，而这个对象就是 monitor object。\nmonitor 的机制中，\nmonitor\nobject 充当着维护 mutex以及定义 wait/signal API 来管理线程的阻塞和唤醒的角色。\nJava 语言中的 java.lang.Object 类，便是满足这个要求的对象，任何一个 Java 对象都可以作为 monitor 机制的 monitor object。\nJava 对象存储在内存中，分别分为三个部分，即对象头、实例数据和对齐填充，而在其对象头中，保存了锁标识；同时，java.lang.Object 类定义了\nwait()，notify()，notifyAll() 方法，这些方法的具体实现，依赖于一个叫 ObjectMonitor\n模式的实现，这是 JVM 内部基于 C++ 实现的一套机制\n（1）当多个线程同时访问一段同步代码时，首先会进入 _EntryList 队列中。\n（2）当某个线程获取到对象的Monitor后进入临界区域，并把Monitor中的 _owner 变量设置为当前线程，同时Monitor中的计数器 _count 加1。即获得对象锁。\n（3）若持有Monitor的线程调用 wait() 方法，将释放当前持有的Monitor，*owner变量恢复为null，*count自减1，同时该线程进入 _WaitSet 集合中等待被唤醒。\n（4）在WaitSet 集合中的线程会被再次放到EntryList 队列中，重新竞争获取锁。\n（5）若当前线程执行完毕也将释放Monitor并复位变量的值，以便其他线程进入获取锁。\n线程争抢锁的过程要比上面展示得更加复杂。除了_EntryList 这个双向链表用来保存竞争的线程，ObjectMonitor中还有另外一个单向链表 _cxq，由两个队列来共同管理并发的线程。\nisAlive()&lt;join()常用            3ways to terminate a thread:\nvolatile\nvolatile总是与优化有关，被 volatile 修饰的变量可以禁止指令重排序优化。volatile的本意是“易变的” 因为访问寄存器要比访问内存单元快的多,所以编译器一般都会作减少存取内存的优化，但有可能会读脏数据。当要求使用volatile声明变量值的时候，系统总是重新从它所在的内存读取数据，即使它前面的指令刚刚从该处读取过数据。精确地说就是，遇到这个关键字声明的变量，编译器对访问该变量的代码就不再进行优化，从而可以提供对特殊地址的稳定访问\nJava提供了volatile关键字来保证可见性。当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去**内存中读取新值。关闭缓存（从Load到Store）**是不安全的，中间如果其他的CPU修改了值将会丢失。下面的测试代码可以实际测试voaltile的自增没有原子性（原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。\n内存屏障（memory barrier）和volatile什么关系？上面的虚拟机指令里面有提到，如果你的字段是volatile，Java内存模型将在写操作后插入一个写屏障指令，在读操作前插入一个读屏障指令。这意味着如果你对一个volatile字段进行写操作，你必须知道：1、一旦你完成写入，任何访问这个字段的线程将会得到最新的值。2、在你写入前，会保证所有之前发生的事已经发生，并且任何更新过的数据值也是可见的，因为内存屏障会把之前的写入值都刷新到缓存。\nDCL（双端检锁）机制不一定线程安全，原因是有指令重排序的存在，加入volatile可以禁止指令重排\nvolatile 在并发情况下是线程不安全的，意味着其他线程拿到的值可能不是最新的。because\nvolatile 变量在各个线程中的确是一致的，但由于 Java 程序中的一条语句映射到物理机器上可能（或者说是基本上）会被转化成若干条本地机器码，也就是说它并不具有原子性，因此线程中变量的一致性并不能得出在并发运行时是线程安全的这一结论（即变量运算的过程中，内存中的变量可能就已经被其它线程修改了）\neg. a++     b= a+1\n\nfinal fields cannot use volatile\naccessed by 1thread cannot use volatile\ncomplex operations cannot use volatile\n\n而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。　　另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。\n保证按线程顺序执行的两种方法：\n\nThread类的join方法：使宿主线程阻塞指定时间或者直到寄生线程执行完毕\nCountDownLatch类：指定计数器，当计数器清零即取消阻塞\n\n指令重排序\n我们所编写的代码在执行的时候，并不一定是按照我们所编写的顺序一行行的执行的，它们的执行顺序可能是经过重排序的。\n3.1 流水线机制\n也许有人会纳闷，好好的代码为什么要对它进行重排序呢？这就涉及到了CPU的流水线机制，现代CPU几乎都采用流水线机制加快指令的处理速度，那么什么是流水线机制呢？这里用一个经典的洗衣机与干衣机的例子进行说明：\n首先假设洗衣机洗衣服需要花费1个小时，干衣机烘干衣物需要半小时；\n正常情况下，一个人先洗衣在干衣需要1.5个小时，但我们注意到，在洗衣机运转的时候干衣机是空闲的，反之亦然，这种利用效率无疑是很低的；\n现在我们这么做，一个人洗好衣服后，进行干衣的时候，就让下一个人去洗衣服，这样一轮洗衣+干衣的时间就被压缩到了1小时（由耗时最长的操作决定）；\n为进一步提升效率，可以再购置台洗衣机，并让它们有序的运行（两台洗衣机洗衣机结束的时间相差半小时，并和干衣机正确的衔接），这样一轮洗衣+干衣的时间就被压缩到了半小时。\n这就是流水线的思想，当然上述例子只涉及两个操作，当有多个操作的时候利用多级的流水线，也可以达到上面类似的效果。\n通过指令重排序，可以根据流水线的需求，合理的修改语句的执行顺序，在保证执行结果正确的前提下（当然这里指的是串行的情况下），尽可能的减少流水线中的空操作，大大的提升了执行效率。\n3.2 指令重排序带来的问题\n虽然指令重排序看上去很美好，但在并行的环境中就有可能会出现问题。\n假设在线程A中有一段初始化方法，方法在初始化结束之后，会修改变量A的值（用于判断是否初始化结束）；\n由于在串行语义中，只是在方法中对变量A进行了赋值，并没有其它与其相关联的操作，因此赋值操作很可能会被重排序（初始化还未完成时就被置位）；\n这时候另一个线程需要判断初始化是否结束，就去查询了变量A的值，由于指令重排序，此线程可能会在初始化还未结束的时候，就得到一个标识着初始化结束含义的变量的值，那么线程后续的操作必然会受到影响。\n而利用关键字 volatile 就可以避免这种情况的发生，被 volatile 修饰的变量可以禁止指令重排序优化。\nSocket\nservlet 不建立连接，仅仅是处理http请求的内容。 所有的输入输入输出电文都由applicationserver 进行处理。到servlet时，已经转换成对象了。属于应用层的东西。\nsocket(套接字)是网络通信的接口，通过接口的TCP/Ip/UDP通信， 需要自己建立连接，自己分析输入电文构造输出电文，属于transport层。 Socket套接字是通信的基石，是支持TCP/IP协议的网络通信的基本操作单元。它是网络通信过程中端点的抽象表示，包含进行网络通信必须的五种信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。Socket本质是编程接口(API)，对TCP/IP的封装，TCP/IP也要提供可供程序员做网络开发所用的接口，这就是Socket编程接口；HTTP是轿车，提供了封装或者显示数据的具体形式；Socket是发动机，提供了网络通信的能力。\nServerSocke.accept()用于等待socket连接， socket是server与客户端通信\njava Socket\nclient socket(需要IP 和port  number)                vs.              server socket(只需要port number返回service)\nAccept() listen\naccept()函数 　creates a new Socket object，系统调用  accept()  会有点古怪的地方的！你可以想象发生  这样的事情：有人从很远的地方通过一个你在侦听  (listen())  的端口连接  (connect())  到你的机器。它的连接将加入到等待接受  (accept())  的队列  中。你调用  accept()  告诉它你有空闲的连接。它将返回一个新的套接字文  件描述符！这样你就有两个套接字了，原来的一个还在侦听你的那个端口，  新的在准备发送  (send())  和接收  (  recv())  数据。这就是这个过程！\nsession\nJavaweb开发中的监听器，是用于监听web常见对象 HttpServletRequest HttpSession ServletContext\n为了接受连接，先用socket()创建一个套接口的描述字，然后用listen()创建套接口并为申请进入的连接建立一个后备日志，然后便可用accept()接受连接了。listen()仅适用于支持连接的套接口\nAPI = 一堆classes associated with\nHttpSession session = request.getSession();\nServlet\nA servlet is a Java Programming language class that is used to extend the capabilities of servers that host applications accessed by means of a request-response programming model. Although servlets can respond to any type of request, they are commonly used to extend the applications hosted by web servers. It is also a web component that is deployed on the server to create a dynamic web page.       动态地扩展Server的能力，并采用请求－响应模式提供Web服务\nServlet没有main方法，不能独立运行，它必须被部署到Servlet容器中，由容器来实例化和调用 Servlet的方法（如doGet()和doPost()），Servlet容器在Servlet的生命周期内包容和管理Servlet。在JSP技术 推出后，管理和运行Servlet/JSP的容器也称为Web容器。\n五个方法，最难的地方在于形参，\n然而Tomcat会事先把形参对象封装好传给我…除此以外，既不需要我写TCP连接数据库，也不需要我解析HTTP请求，更不需要我把结果转成HTTP响应，request对象和response对象帮我搞定了。\n那么如何是Servlet线程安全呢？答案是不要使用实例变量或类变量。当然你也能够使用synchronized同步方法或使用单线程模型，但这样效率不高。暂时变量是不会影响线程安全的,由于他们是在栈上分配空间,并且每一个线程都有自己私有的栈空间.\nJSP同步也一样。由于jsp会被编译成servlet。在jsp中&lt;%! String unsafeVar; %&gt;声明的变量事实上是servlet的实例变量，而&lt;% String safevar %&gt;变量声明是局部变量。\nhttps://img-blog.csdn.net/20180430163324407?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p0MTU3MzI2MjU4Nzg=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\ntomcat\nTomcat运行在JVM之上，是servlet JSp的web容器，它和HTTP服务器一样，绑定IP地址并监听TCP端口，同时还包含以下职责：\n\n管理Servlet程序的生命周期\n将URL映射到指定的Servlet进行处理\n与Servlet程序合作处理HTTP请求——根据HTTP请求生成HttpServletRequest对象并传递给Servlet进行处理，将Servlet中的HttpServletResponse对象生成的内容返回给浏览器.\n\n虽然Tomcat也可以认为是HTTP服务器，但通常它仍然会和Nginx配合在一起使用：\n\n\n动静态资源分离——运用Nginx的反向代理功能分发请求：所有动态资源的请求交给Tomcat，而静态资源的请求（例如图片、视频、CSS、JavaScript文件等）则直接由Nginx返回到浏览器，这样能大大减轻Tomcat的压力。\n\n\n负载均衡，当业务压力增大时，可能一个Tomcat的实例不足以处理，那么这时可以启动多个Tomcat实例进行水平扩展，而Nginx的负载均衡功能可以把请求通过算法分发到各个不同的实例进行处理\nhttps://img-blog.csdn.net/20180622221844320?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA3MjU5Ng==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\n\n\nPrintWriter out=response.getWriter()\n1，从HttpServletResponse中get一个PrintWriter；,\n2，打个通俗的比方就是通过HttpServletResponse对象得到一支笔，然后out是输出字符流，即servlet接受到request请求后，servlet使用out来返回结果，不管客户端是什么（浏览器或者httpclient 或者别的serlvet等等），它都和客户端建立一个流输出管道，然后把字符流输出给请求端。\nout.print(“”);\nout.print(“任何内容”);\nout.print(“”);\n3，通过PrintWrite，以流方式输出html，返回给客户端，显示在IE上。\n4，取一个响应客户端的流对象\n5，获取PrintWriter流，用来在客户端输出\nHTTP REQUEST HEADER\nHTTP2.0 focus on reduce payload time. Using SPDY multiplex,universal encryption,server push\nJSP\nJSP全称Java Server Pages，是一种动态网页开发技术。它使用JSP标签在static HTML网页中插入dynamic Java servlet代码。标签通常以&lt;%开头以%&gt;结束。\nJSP是一种Java servlet，主要用于实现Java web应用程序的用户界面部分。网页开发者们通过结合HTML代码、XHTML代码、XML元素以及嵌入JSP操作和命令来编写JSP。JSP通过网页表单获取用户输入数据、访问数据库及其他数据源，然后动态地创建网页. JSP标签有多种功能，比如访问数据库、记录用户选择信息、访问JavaBeans组件等，还可以在不同的网页中传递控制信息和共享信息。\nScriptlet\n&lt;%       %&gt;称作jsp表达式，用于将已经声明的变量或者表达式输出到网页上面。\n&lt;%!        %&gt;   JSP 声明中定义的变量、方法和类是全局性的，在 JSP 页面中的任何地方都能够使用。\n注意2：JSP 声明中不能使用out.print()系列方法做 输出操作。\n&lt;%-- --%&gt; 服务器端注释，解析时将跳过这些代码，从而在客户端是查看源代码是看不到这些注释的。\n客户端注释，是查看源代码是可以看到这些注释的。\ndirective\njsp常用的编译指令有3个：include指令、page指令、taglib指令。\npage指令不能作用于动态的包含文件，例如对使用jsp:include包含的文件，page指令的设置是无效的。一般情况下，page编译指令位于页面最上方，一个页面可以有多个编译配置指令。\n2、语法格式：&lt;%@page attribute1=“value1” attribute2=“value2”… %&gt;\n&lt;% out.println(“” + “d=new Date(); document.write(d.toGMTString());” + “”); %&gt;\n下表列出与Page指令相关的属性：\nUntitled Database\npredefined variable\nAction\nUntitled Database\nJavaBeans\nSimple java classes for storing and accessing information. 面向对象A bean encapsulates many objects into one object so that we can access this object from multiple places. Moreover, it provides easy maintenance.JavaBean\n主要用来传递数据，即把一组数据组合成一个JavaBean便于传输。此外，JavaBean可以方便地被IDE工具分析，生成读写属性的代码，主要用在图形界面的可视化设计中。 很多java class 都符合javabean需求\n1、所有属性为private               must be packed\n2、提供默认构造方法           no main()\n3、提供getter和setter\n4、实现serializable接口\n没有javabeans的话jsp页面直接和数据层进行交互，这样会使得代码的可维护性变很差，而且在jsp中出现大量的业务逻辑代码是很不好的。\njsp:useBean 标签可以在 JSP 中声明一个 JavaBean，然后使用。声明后，JavaBean 对象就成了脚本变量，可以通过脚本元素或其他自定义标签来访问。jsp:useBean 标签的语法格式如下：\n&lt;jsp:useBean id=\"bean 的名字\" scope=\"bean 的作用域\" typeSpec/&gt;\n其中，根据具体情况，scope 的值可以是 page，request，session 或 application。id值可任意只要不和同一 JSP 文件中其它 jsp:useBean 中 id 值一样就行了。\n3.Session是用户全局变量，在整个会话期间都有效。只要页面不关闭就一直有效（或者直到用户一直未活动导致会话过期，默认session过期时间为30分钟，或调用HttpSession的invalidate()方法）。存放在HttpSession对象中\n4.application是程序全局变量，对每个用户每个页面都有效。存放在ServletContext对象中。它的存活时间是最长的，如果不进行手工删除，它们就一直可以使用\n总结：当数据只需要在下一个forward有用时，用request就够了；\n若数据不只是在下一个forward有用时，就用session。\n上下文，环境信息之类的，用application。\nMVC\nhttps://bkimg.cdn.bcebos.com/pic/ac6eddc451da81cb26660e7e5066d01608243184?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5/format,f_auto\n渲染页面方面，大概有三种方式：\n\n后端渲染 + 后端路由\n前后端分离 + 前端渲染\n前后端分离 + 前端渲染 + 前端路由\n\n前后端分离\n\n前后端分离（Frontend-Backend Separation）:\n\n在前后端分离的方式中，前端（浏览器端）和后端（服务器端）是独立的系统。\n前端通常是一个单页应用（SPA），由JavaScript等技术在浏览器中运行。\n前端通过接口（API）与后端通信，获取纯数据，然后使用这些数据在浏览器中动态渲染页面。\n传递给浏览器的是数据，而非完整的HTML页面。\n\n\n服务器端渲染（Server-Side Rendering，SSR）:\n\n在服务器端渲染的过程中，服务器负责将数据转换为完整的HTML页面，然后发送给浏览器。\n浏览器收到的是已经渲染好的HTML，而不仅仅是数据。这有助于更快地展示内容，因为不需要等待浏览器执行JavaScript来渲染页面。\n这种方式通常被认为对搜索引擎友好，因为搜索引擎能够更容易地理解和索引HTML内容。\n\n\n\n1.数据量：前后端分离中传递数据，所以传输量会小。\n服务器端渲染，会传输更大的数据，而且，会有很多内容是重复的。\n2 体验：前后端多了一个渲染数据的过程，服务器端省去了这个过程。这也是一直被提到的首屏渲染的问题。\n3 解耦：前后端分离中，传输的是数据（只能由前端发送ajax请求，后端接受到ajax请求后，通过response返回数据，数据可以是规定数据结构，比如文本，json，xml，html 等，由前端ajax接受，但是如果html 文件不在项目中，需要jsonp跨域请求)）全部交给前端来处理，后端只负责提供数据。\n服务器端渲染中，传输的是Html，后端传给前端的Model，通常是通过Hidden的Input来处理，或者是直接用模板技术生成（JSP，Velocity，freemak）等。\n数据和展现并未分离，在过去，这被称之为套页面。\n4 控制：网页之间有各种跳转交互，在前后端分离中，跳转的页面控制，全部是由前端来决定。跟后端完全没有关系。在服务器端渲染的方式中，大部分是由后端来决定，少部分是由前端来决定\n5 SEO：前后端分离的方式，通常的载体是SPA，所以拿到的是没有数据的空壳子，很多搜索引擎，不支持SPA方式的SEO。\n\n客户端渲染不利于 SEO 搜索引擎优化\n服务端渲染是可以被爬虫抓取到的，客户端异步渲染是很难被爬虫抓取到的\n\n","categories":["计算机科学"],"tags":["计算机基础","编程"]},{"title":"Personal Thoughts","url":"/2024/03/11/journal/","content":"“那一天我二十一岁，在我一生的黄金时代。我有好多奢望。我想爱，想吃，还想在一瞬间变成天上半明半暗的云。后来我才知道，生活就是个缓慢受锤的过程，人一天天老下去，奢望也一天天消失，最后变得像挨了锤的牛一样。可是我过二十一岁生日时没有预见到这一点。我觉得自己会永远生猛下去，什么也锤不了我。” ———— 王小波\n大学毕业后已经在家“无所事事”地待了1.5个学期，亲朋好友们纷纷探询我的近况，他们的言语中充满了一种隐约的焦虑：“赶紧找份工作吧。”在这社会的凝视之下，未投身职场的我仿佛成了众人眼中的异类。普遍而言，社会往往对年轻人有一套普遍接受的生活轨迹预期：完成学业、找到工作、稳定下来。但事实上，每个人的生活节奏和价值追求都是独一无二的，不应该仅仅因为周围的声音就匆忙做出决定，我是这样想的。我觉得赚钱不是目的，是手段，完整地发展自己，听从内心的声音有时比被外界所影响的改变更为重要与可贵。\n23岁对你们而言都是哪年？对我而言就是现在，我又应该以什么样的状态面对我的生活呢？现在是2024年3月，去年的这时候我刚刚拿到UofT和TAMU的offer，结束了磨人的申请季。我内心不至于激动澎湃却也有一丝淡淡的欣慰，可能是因为未来的不确定性消除了一些。然而事情并非一帆风顺，预定的计划倒在了IRCC无限期拖延的背景调查处，就差临门一脚，中道崩殂。如果没有10043，国家之间的关系好上那么一点，我会不会有机会出国留学，遵循着我原先的计划与期望，在此时此刻面临新的问题和挑战？可惜没有如果，只有现实。\n在这片刻的沉默和反思中，我逐渐领悟到，每一步走得慢一些，未必不是一种恩赐。这段时间让我有机会深入探索自我，思考究竟什么是对我而言真正重要的。或许，这段被外界视为“无所事事”的时光，正是我为未来打下坚实基础的宝贵机遇。毕竟，人生的旅程充满了未知和变数，学会在不确定中寻找自我，可能就是我现阶段最宝贵的收获。\n共勉。\n","tags":["personal growth"]},{"title":"JS和Tailwind","url":"/2024/03/10/index/","content":"欢迎来到我的前端小天地，这里不仅仅是代码的海洋，更是我对技术的热爱与追求的体现。我是一位独立的软件开发者，专注于为每个项目打造独特而精致的前端体验。\n在这个数字化的时代，网站已经不再只是展示信息的窗口，更是用户与技术交互的平台。而我，对每一行代码都有着执着的追求，因为我相信，卓越的细节决定了卓越的用户体验。\n为何选择我？\n定制而非模板： 我的每一份工作都是独一无二的，我从不借助廉价的主题，而是为你精心打磨每一个细节，为你呈现出真正属于你的网站。\n从大项目到细节设计： 作为一名全职开发者，我在与大型客户合作的同时，同样专注于每个细节的完美呈现。无论是响应式设计还是移动优先策略，我总是以用户为中心。\n轻量化而高效： 我坚信没有必要通过过多的插件和主题来使网站臃肿。我的设计理念是“简而不减”，不仅摆脱了繁琐的编辑器，更拥抱了原生的JavaScript，让你的网站保持高效。\n技术SEO的精通： 我不仅仅关注网站的外观，更深入关注技术SEO。你的网站不仅会令用户眼前一亮，更会在搜索引擎中占据有利位置。\n在这个浮躁的网络世界，我力求通过博客分享我的经验和见解，教育人们为何他们应该关心技术细节，因为当他们了解时，他们就会关心。我的经验告诉我，用户不仅看到了网站的外观，更感受到了技术的品质。\n一起探索前端的未知领域吧！感谢你阅读我的博客，期待我们的技术之旅。\nMarketing\nhttps://www.reddit.com/r/webdev/comments/19ewsgp/is_wordpress_the_cheapest_fastest_way_of_making/\nhttps://www.smashingmagazine.com/2022/10/roadmap-building-business-chatbot/\nAirtasker fiverr which best for solo software developer\nWell, I know exactly what makes me a better developer than others.\n\nAll my work is custom made, I will not sell you a cheap theme\nI’m a full-time developer at an agency with large clients\nEverything responsive, even better: I always work mobile first\nNo unnecessary plugins, they’ll bloat the website in the long run\nNo bloated editors like Elementor as well, but clean custom fields instead\nNo Jquery but vanilla Javascript, which is faster\nVery experienced with technical SEO\n\nI know that clients do not care about technical aspects, but still. It drives me nuts that wannabe “website makers” are in my way of getting clients by reselling themes and changing some fonts and colours. Anyway, thank you for your tips.\n如果您能教育他们为什么他们应该关心，他们就会关心。当我和这些人打交道时，他们会问，既然他们可以在 wix 或 Wordpress 上以低廉的价格免费完成工作，为什么还要付钱给我呢？我告诉他们这是一个完全不同的产品。他们的工作很可能是模板泵送和转储，没有真正关心或思考内容策略，并且具有无聊但对客户有吸引力的通用文本。最重要的是，他们的页面速度得分很糟糕。Wix 和 WordPress 网站得分为 17-30/100。我的作品得分为 98-100。\n开发\n网络通信basic\nJSX\nconst element = &lt;h1&gt;Hello, world!&lt;/h1&gt;;\n这个有趣的标签语法既不是字符串也不是 HTML。\n它被称为 JSX，它是 JavaScript 的语法扩展。我们建议将它与 React 一起使用来描述 UI 应该是什么样子。JSX 可能会让您想起模板语言，但它具有 JavaScript 的全部功能。\n\n\n.js: 表示这是一个 JavaScript 文件。\n\n\n.jsx: 表示这是一个 JavaScript XML 文件，即包含 React 组件内部构建标签的类 XML 语法。\nJavaScript XML 是React组件内部构建标签的类XML语法。可以理解为React提供的语法糖，可以让编译器更方便快速的选择编译方式。JavaScript 是能够被浏览器直接识别的，JavaScript XML需要经过编译器（webpack等）转换成 JavaScript\n在实际使用中，.js 和 .jsx 的语法和后缀可以互换，语法上也完全兼容。建议使用统一的 .js 后缀，无需特意区分。\n\n\n.ts: 表示这是一个 TypeScript 文件。文件内容不支持类似 &lt;div&gt; 这种 HTML 语法，因为 TypeScript 是一种静态类型语言，需要遵循类型定义。\n\n\n.tsx: 表示这是一个 TypeScript 文件，同时也包含了 JavaScript XML（JSX）语法。.tsx 文件在 TypeScript 的基础上支持 JSX 语法，可以包含类似 &lt;div&gt; 的标签。\n\n\n总的来说，.js 和 .jsx 在 React 中通常是互换使用的，而 TypeScript 用户更倾向于使用 .ts 和 .tsx，其中 .tsx 用于包含 JSX 语法。\nJSX 产生 React “元素”。我们将在下一节探索将它们渲染到 DOM 。下面，您可以找到入门所需的 JSX 基础知识。在React中，JSX（JavaScript XML）通常要求在返回的顶级元素中包裹所有内容。这是因为当返回的内容包含多个元素时，React需要一个根元素来渲染。\n在 React 项目中，index.tsx 和 App.tsx 在应用程序结构中具有不同的用途：\nindex.tsx：该文件充当 React 应用程序的入口点。它通常是启动或构建应用程序时执行的第一个文件。在大多数 React 应用程序中，index.tsx 或 index.js 是 ReactDOM 将应用程序的根组件渲染到 DOM 中的位置。它负责将主要组件（通常是 App.tsx）渲染到 HTML 文件中。\nApp.tsx：此文件包含 React 应用程序的根组件。这是应用程序的主要结构开始的地方。该组件通常代表应用程序的整体布局，包括路由结构以及跨应用程序的多个页面或部分保留的任何全局组件或布局。\n使用 index.tsx 作为入口点的原因是使用 webpack 或 Create React App (CRA) 等工具捆绑和构建 React 应用程序的方式。 index.tsx 文件被指定为配置中的入口点，因此当构建或启动应用程序时，它会查找该文件作为应用程序的起点。同时，App.tsx 是开发人员定义 React 应用程序主要结构的传统位置。该文件通常包含路由逻辑、全局上下文/提供程序，并设置将呈现给用户的布局或结构。\nNext.js\nNext.js 是一个流行的开源 React 框架，用于构建 Web 应用程序。前后端集成方案但复杂场景下还是需要分离的后端It’s kind of a backend for the frontend.\nWhen to use?\n\nHow Complex Is Your Application?\n\nFor simple applications, a monolithic approach might be sufficient. For complex applications with distinct frontend and backend requirements, separation could be beneficial.\n\n\nTeam Structure:\n\nDo you have separate teams or developers specializing in frontend and backend development? If so, separation might align with your team structure.\n\n\nFuture Plans:\n\nConsider your long-term plans. If you anticipate significant growth, scalability requirements, or the need for microservices, a separate approach might be more future-proof.\n\n\nDevelopment Speed:\n\nIf your priority is rapid development and deployment, a monolithic approach with Next.js API routes can be convenient.\n\n\n\n以下是与 Next.js 相关的一些关键功能和概念：\nNext.js 具有两种形式的预渲染： 静态生成（Static Generation） 和 服务器端渲染（Server-side Rendering）。这两种方式的不同之处在于为 page（页面）生成 HTML 页面的 时机 。\n\n静态生成 （推荐）：HTML 在 构建时 生成，并在每次页面请求（request）时重用。\n服务器端渲染：在 每次页面请求（request）时 重新生成 HTML。\n\n\nServer-side rendering (SSR): Next.js allows you to perform server-side rendering, generating HTML pages on the server at runtime before serving them to the client. This can improve initial loading times and SEO.\nStatic Site Generation (SSG): Next.js provides static site generation, enabling the creation of static websites that can be pre-built during the build phase. This allows for high performance and easy deployment of static content.\nFile-based Routing: Next.js uses a file-based routing system where routes are defined based on the file structure. For example, creating a file in the pages directory automatically creates a route for that file.\nAPI Routes: Next.js allows developers to create API endpoints easily using API routes. These routes are defined in the api directory and can handle server-side logic or connect to databases.\nAutomatic Code Splitting: Next.js performs automatic code splitting, allowing only the necessary JavaScript to be loaded for each page. This helps in optimizing the performance of the application.’\n\nA hydration often associated with SSR, means \"Each generated HTML is associated with minimal JavaScript code necessary for that page. When a page is loaded by the browser, modern JavaScript frameworks making it dynamic by attaching event listeners and state management to it using JavaScript.\nI see “use client” as “enable client side interactivity”.  将需要交互性的最叶子节点变成客户端组件是一个有效的性能优化策略，特别是在构建大型、复杂的 Web 应用时。这种方法允许开发者精细控制渲染行为，优化用户体验，同时保持应用的快速响应和高效加载。\nNext. js has two server runtimes to run your application: the Node. js Runtime (default) and the Edge Runtime. When doing SSR, or serving API routes, the application code will be executed in the Node.\nNext.js 的可配置性真的是一个很强大的特色，它准备了一套默认就很好用的默认配置，但这些配置基本上用户都可以 完全 控制它（完全做一个保留，但大型工程基本上都是可以支撑的）。\n下面我们来分析一下它的可配置功能。\n\n配置文件 next.config.js 中暴露了 webpack 实例，因此你可以完全控制 webpack; 支持配置自定义配置，你可以把一些公用的不变的配置写在 serverRuntimeConfig 或者 publicRuntimeConfig 中，前者只会出现在服务端，后者会暴露到客户端。\n可 自定义 server ，你可以在启动服务的时候做一些自己想要做的处理，比如 node.js 性能监控等等。   不自定义 server ，也可以使用它提供的 middreware 机制来拦截请求或者校验权限等事项。\n自定义 Document，也就是_document.js，用于自定义配置 html 生成内容，比如插入 Google 分析脚本。\n自定义错误界面 也就是 404 或者 500 错误状态的页面。\n自定义页面 head 属性，使用next/head 提供的 Head 组件，用于自定义 html document 头部的 title/meta/base 等标签信息。\n可自定义 [babel](https://link.juejin.cn/?target=https%3A%2F%2Fnextjs.org%2Fdocs%2Fadvanced-features%2Fcustomizing-babel-config) 和 [postcss](https://link.juejin.cn/?target=https%3A%2F%2Fnextjs.org%2Fdocs%2Fadvanced-features%2Fcustomizing-postcss-config) 等工程化规则配置。\n\n在现代JavaScript项目中，特别是使用模块打包器如Webpack或Rollup时，对模块的处理非常关键，因为它直接影响到最终构建的体积和性能。理解如何处理这些不同情况的模块导入是优化项目的重要部分。下面是对你提供的每个资源处理方式的解释：\n代码 import { Button } from \"@shopify/polaris\"; 存在以下可能：\n\n导入它：导入并包含该模块，分析评估它并继续进行依赖分析\n跳过它：不导入它，不分析评估它但会继续进行依赖分析\n排除它：不导入它，不评估且不做依赖分析\n\n为了利用 tree shaking 的优势，必须：\n\n使用 ES2015 模块语法（即 import 和 export）；\n确保没有编译器将 ES2015 模块语法转换为 CommonJS（顺带一提，这是现在常用的 @babel/preset-env 的默认行为，请参阅 文档 以了解更多信息）。\n在项目的 package.json 文件中添加 \"sideEffects\" 属性。\n使用 mode 为 \"production\" 的配置项以启用 更多优化项，包括压缩代码与 tree shaking。\n\n你可以将应用程序想象成一棵树。绿色表示实际用到的源码和库，是树上活的树叶。灰色表示未引用代码，是秋天树上枯萎的树叶。为了除去死去的树叶，你必须摇动（shake）这棵树，使它们落下。\nAPI\nMethods and Technologies for Data Fetching\n\nXMLHttpRequest (XHR): One of the earliest methods to perform asynchronous web requests. While still in use, it has largely been superseded by newer technologies.\nFetch API: A modern, promise-based mechanism native to the browser that simplifies making web requests. It’s more powerful and flexible than XHR.\nAxios: A popular JavaScript library that provides an easy-to-use API for making HTTP requests. It’s based on promises, making it suitable for modern web development.\nGraphQL: A query language for APIs and a runtime for executing those queries by using a type system you define for your data. Unlike the traditional REST approach, GraphQL allows clients to request exactly the data they need, making data fetching more efficient.改善restful的缺点，避免了client获取不足和获取过度\nLibraries/Frameworks Built-in Functions: Many modern JavaScript frameworks and libraries like React, Angular, and Vue offer built-in functions or recommend specific libraries (like react-query, Apollo for GraphQL) to facilitate data fetching and state management.\n\nConsiderations in Data Fetching\n\n\nAsynchronous Nature: Data fetching is inherently asynchronous. JavaScript provides mechanisms like callbacks, promises, and async/await syntax to handle asynchronous operations.\n\n\nError Handling: Proper error handling is crucial to manage network errors, invalid responses, or data parsing errors gracefully.\n\n\nPerformance and Optimization: Techniques like caching, data prefetching, and lazy loading are used to optimize the data fetching process and improve user experience.\n\n\nSecurity: It’s important to secure data requests to protect sensitive information from being exposed or intercepted through practices like using HTTPS, securing API keys, and sanitizing inputs.\n\n\nREST 适用于简单的 CRUD 操作和资源管理。\n\n\nGraphQL 适用于需要灵活性和精确数据的场景。\n\n\nRPC 适用于强调过程调用的场景，尤其是在多语言和多平台之间进行通信时。\n\n\nCSS\n布局的传统解决方案，基于盒状模型，依赖 [display](https://developer.mozilla.org/en-US/docs/Web/CSS/display) 属性 + [position](https://developer.mozilla.org/en-US/docs/Web/CSS/position)属性 + [float](https://developer.mozilla.org/en-US/docs/Web/CSS/float)属性。它对于那些特殊布局非常不方便，比如，垂直居中就不容易实现。\n\nabsolute：元素会相对于最近的已定位的祖先元素进行定位（即非**static**定位），如果没有，则相对于文档的初始包含块定位。\nfixed：元素会相对于浏览器窗口进行定位，即使窗口滚动，元素也会保持在指定的位置。\nsticky：元素是基于用户的滚动位置在**relative和fixed**定位之间切换的。\n\nIn CSS, position is a fundamental property that dictates how an element is positioned in the document layout. The values static and relative are two different settings for this property, and they have distinct behaviors:\n\nPosition: Static\n\nDefault Behavior: static is the default position value for any HTML element. If you don’t explicitly set the position property, it defaults to static.\nNormal Document Flow: Elements with position: static are positioned in the normal document flow. This means the element is positioned according to its place in the HTML structure, following the sequence and layout dictated by the HTML and other CSS properties.\nNo Offsetting: With position: static, you cannot use top, right, bottom, or left properties to move the element. These properties will have no effect on a statically positioned element.\nNo Z-Index: Static elements don’t react to the z-index property, so you can’t use it to change the stacking order of elements with position: static.\n\n\nPosition: Relative\n\nAdjust Position: When you set an element to position: relative, it allows you to position the element relative to its normal position in the document flow.\nOffsets: By using top, right, bottom, or left properties on an element with position: relative, you can move the element away from its normal position. For example, top: 10px; will move the element 10 pixels down from where it would normally be.\nDocument Flow Impact: Unlike absolute or fixed positioning, an element with position: relative still occupies its original space in the document flow. This means it doesn’t overlap other elements or leave a gap in the layout.\nStacking Context: You can use z-index on relatively positioned elements to control their stacking order.\nBase for Absolutely Positioned Children: If an element with position: absolute is inside a container with position: relative, the absolute element will be positioned relative to its nearest positioned ancestor (the relative container in this case), not the entire page.\n\n\n\nIn summary, the primary difference is that static positioning is the default layout behavior with no ability to offset elements or use z-index, while relative positioning allows you to nudge elements from their normal position, use z-index, and serve as a positioning context for absolutely positioned child elements.\n\nTailwind\nTailwind 适合的是新生代的公司，还没有自己的设计体系，要从零开始做一套自己的。这时候 Tailwind 的优势就出来了：你还没有自己的组件对吧？那你要用更小颗粒度的东西先搭出来通用组件吧？这个更小颗粒度的东西到底是什么呢？就是 HTML 元素和 CSS 样式了。\n这时候你可以选择原生 CSS，也可以选择 Tailwind。原生 CSS 被设计出来时，页面还没被拆分为组件，样式可以有丰富的上下文以来，所以支持复杂的 selector。自从有了组件的概念后，大家习惯了在组件和组件之间划清界限，互相不能影响对方的样式。就算你是我的父组件，你也只能划定一个区域给我渲染，你无权干预我的样式。面对这样的需求，Tailwind 的优势就出来了。\n你要用样式组合出组件，然后用组件组合出应用。你不需要也不允许一个组件的样式影响到另一个组件的样式。Tailwind 能够给你很好用的可复用样式，而且它们互不干扰对方。\n思维转变：使用组合\n万物相通的，这些年面向对象语言中，迎来新的架构思想变化：组合优于继承，多用组合少用继承，熟悉react的同学，也清楚hook和class是两种开发思维的不同，原先一个庞大的class组件，变为多个简单的hook组件的组合，代码复用率得到提升。\n而Tailwind CSS也是迎合这一思想的，而且它更彻底，组合到了原子级（一行css一个class名，例如：Tailwind CSS中，flex类表示 display：flex ）。\n按需编译&amp;高复用率\nTailwind CSS是用到多少，编译多少，多余的一点没有，因此极大的降低了css体积。\n针对质疑的三问：\n\nTailwind CSS就是全新的，它对于CSS的意义，犹如react之于一众js框架&amp;操作dom库。Bootstrap虽有utility classes，但它并不是原子化的，且使用上需要先加载整个Bootstrap的css文件，不管你用到的还是没用的。\n他没有增加记忆成本，反而每个class都有其实际CSS属性的含义，是对CSS的简写，且原先需要服用css属性，一样需要记忆，而Tailwind CSS熟悉后，不仅不增加成本，还大大提升了开发效率，因为class属性的记忆也是复用的。\n如果觉得class命名太长，且一些业务涉及动态设置class名，情况就更严重了，这里建议在使用Tailwind CSS时，可以结合着使用classnames 和twin.macro（css-in-js）\n\n\nThe base layer is for things like reset rules or default styles applied to plain HTML elements.\nThe components layer is for class-based styles that you want to be able to override with utilities.\nThe utilities layer is for small, single-purpose classes that should always take precedence over any other styles.\n\nPostCSS 就是 CSS 界的 Babel。\n它们本身只做两件事：\n\n把源代码（或者符合一定条件的扩展语法）解析为一个自带遍历访问、节点操作接口的树；\n把语法树输出为代码字符串。\n\n主要的功能，都由插件提供，在第一步完成以后，这棵树以及上面的节点会由一个个插件依次进行处理。插件可以做很多事，比如：\n\n识别一些浏览器尚未支持的语法，转换为浏览器支持的（Autoprefixer、cssnext）；\n在不破坏解析的前提下在一定程度上扩展语法，提供私有的语法糖；\n识别出源码中不符合编码规范之处，输出结果来给 linter 显示（Stylelint）；\n找到并去除冗余代码、将代码压缩为等价且更短的写法（cssnano）；\n\n和手写 CSS 相比，主要优势很简单：写出更短、更标准、更容易维护的代码。\nBabel 是一个工具链，主要用于将采用 ECMAScript 2015+ 语法编写的代码转换为向后兼容的 JavaScript 语法，以便能够运行在当前和旧版本的浏览器或其他环境中。下面列出的是 Babel 能为你做的事情\nWebSocket 是一种计算机通信协议，允许服务器和客户端之间进行实时双向通信。 WebSocket 使用单个传输控制协议 (TCP) 连接，旨在实现 Web 应用程序中的高效数据传输。\nAn EventSource instance opens a persistent connection to an HTTP server, which sends events in text/event-stream format. The connection remains open until closed by calling [EventSource.close()](https://developer.mozilla.org/en-US/docs/Web/API/EventSource/close).\n\n服务器推送：EventSource专注于服务器向客户端主动推送事件的模型，这对于ChatGPT对话非常适用。ChatGPT通常是作为一个长期运行的服务，当有新的回复可用时，服务器可以主动推送给客户端，而不需要客户端频繁发送请求。\n自动重连和错误处理：EventSource具有内置的自动重连机制，它会自动处理连接断开和重新连接的情况。这对于ChatGPT对话而言很重要，因为对话可能需要持续一段时间，连接的稳定性很重要。\n简单性和易用性：相对于WebSocket，EventSource的API更加简单易用，只需实例化一个EventSource对象，并处理服务器发送的事件即可。这使得开发者可以更快速地实现对话功能，减少了一些复杂性。\n广泛的浏览器支持：EventSource在大多数现代浏览器中得到广泛支持，包括移动端浏览器。相比之下，WebSocket在某些旧版本的浏览器中可能不被完全支持，需要考虑兼容性问题。\n\n设计原则\n在这种情况下，响应性是指用户界面 (UI) 或网页设计平滑地调整其布局、元素和功能以适应不同的屏幕尺寸和分辨率而不损失可用性或可读性的能力。它涉及以一种为用户提供最佳观看和交互体验的方式设计和编码界面，无论他们使用什么设备。\n这通常涉及：\n流体布局：设计可以使用百分比等相对单位或使用 CSS Grid 或 Flexbox 根据屏幕尺寸调整和重新排列内容的布局。\n媒体查询：使用 CSS 媒体查询根据设备的屏幕宽度应用不同的样式，从而为台式机、平板电脑和移动设备进行定制设计。\n触摸友好的元素：确保按钮、表单输入和其他交互元素足够大并且易于在触摸屏上点击。\n优化内容：在较小的屏幕上调整或隐藏某些内容或功能，以优先考虑重要信息并保持可用性。\n视口元标记：在 HTML 标头中包含视口元标记以控制移动浏览器上的布局。\n测试：定期在不同设备和屏幕尺寸上测试帐户创建屏幕，以确保其按预期运行并在所有平台上提供无缝的用户体验。\nService worker API\nService Worker 是一种在浏览器背后运行的脚本，能够拦截和处理网络请求，实现诸如缓存文件、推送通知、离线访问等功能。它为 Web 应用提供了一种途径，使其能够在离线状态下运行，并且改善了性能和用户体验。\nService worker 是一个注册在指定源和路径下的事件驱动 worker。它采用 JavaScript 文件的形式，控制关联的页面或者网站，拦截并修改访问和资源请求，细粒度地缓存资源。你可以完全控制应用在特定情形（最常见的情形是网络不可用）下的表现。\nWeb Worker 是 HTML5 提供的一项技术，允许在浏览器中创建多线程的 JavaScript Runtime,  允许主线程创建 Worker 线程，将一些任务分配给后者运行。在主线程运行的同时，Worker 线程在后台运行，两者互不干扰。等到 Worker 线程完成计算任务，再把结果返回给主线程。这样的好处是，一些计算密集型或高延迟的任务，被 Worker 线程负担了，主线程（通常负责 UI 交互）就会很流畅，不会被阻塞或拖慢。\nWorker 线程一旦新建成功，就会始终运行，不会被主线程上的活动（比如用户点击按钮、提交表单）打断。这样有利于随时响应主线程的通信。但是，这也造成了 Worker 比较耗费资源，不应该过度使用，而且一旦使用完毕，就应该关闭。\nWeb Worker 主要有两种类型：Dedicated Worker 和 Shared Worker。\nService worker 运行在 worker 上下文：因此它无法访问 DOM，相对于驱动应用的主 JavaScript 线程，它运行在其他线程中，所以不会造成阻塞。它被设计为完全异步；因此，同步 XHR 和 Web Storage 不能在 service worker 中使用。 出于安全考量，Service worker 只能由 HTTPS 承载，毕竟修改网络请求的能力暴露给中间人攻击会非常危险，如果允许访问这些强大的 API，此类攻击将会变得很严重。在 Firefox 浏览器的用户隐私模式，Service Worker 不可用。\nevent.waitUntil(    caches.open(CACHE_NAME)      .then(cache =&gt; {        console.log('缓存已打开');        return cache.addAll(urlsToCache);      })  );\n当 Service Worker 拦截到一个网络请求时（例如在 fetch 事件中），使用 caches.match(event.request) 的目的是检查该请求的资源是否已经存在于缓存中。如果存在缓存，则返回缓存中的响应；如果不存在缓存，则继续向网络发起请求获取资源。\n接着，通过 then() 方法处理这个返回的 Promise，继续执行相应的逻辑。例如，如果匹配到缓存，则可以直接返回缓存的响应；如果未找到缓存，则继续使用 fetch(event.request) 从网络获取资源。\n页面解析渲染的过程\n连接建立后 成功接收响应\n1.HTML 被 HTML 解析器解析成 DOM 树；\n当parser发现非阻塞资源，例如一张图片，浏览器会请求这些资源并且继续解析。当遇到一个 CSS 文件时，解析也可以继续进行，但是对于 &lt;script&gt; 标签（特别是没有 [async](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Statements/async_function) 或者 defer 属性的）会阻塞渲染并停止 HTML 的解析。等待获取 CSS 不会阻塞 HTML 的解析或者下载，但是它确实会阻塞 JavaScript，因为 JavaScript 经常用于查询元素的 CSS 属性。尽管浏览器的预加载扫描器加速了这个过程，但过多的脚本仍然是一个重要的瓶颈。\n2.CSS  被 CSS 解析器解析成 CSSOM 树；\n在解析 CSS 和创建 CSSOM 的同时，包括 JavaScript 文件在内的其他资源也在下载（这要归功于预加载扫描器）。JavaScript 会被解析、编译和解释。脚本被解析为抽象语法树。有些浏览器引擎会将抽象语法树输入编译器，输出字节码。这就是所谓的 JavaScript 编译。大部分代码都是在主线程上解释的，但也有例外，例如在 web worker 中运行的代码。\n3.结合 DOM 树和 CSSOM 树，生成一棵渲染树(Render Tree)，这一过程称为 Attachment；\n\n布局（Layout）： 布局是确定每个元素在屏幕上的精确位置和大小的过程。浏览器会遍历渲染树中的每个元素，考虑它们的样式信息、大小、位置等，然后计算出它们在屏幕上的最终位置。这个过程通常也被称为回流（reflow）。\n绘制（Paint）： 一旦浏览器完成布局，它就可以开始将页面上的元素绘制到屏幕上。这包括绘制文本、图像、背景颜色等。浏览器使用计算机的图形处理单元（GPU）来加速绘制操作，以提高性能和流畅度。\n\n第四步和第五步是最耗时的部分，这两步合起来，就是我们通常所说的渲染。\n减少HTML的回流（reflow）是优化网页性能的关键之一，因为回流操作会导致页面重新布局，消耗较多的计算资源。以下是一些减少HTML回流的最佳实践：\n\n以局部布局的形式组织HTML结构：确保你的HTML结构是有序的，并且样式和布局信息应该尽可能靠近彼此。这意味着你应该将样式应用到尽可能低层级的DOM节点上，以减少影响范围。例如，如果你只想修改**&lt;p&gt;元素的样式，那么应该将类（class）或样式规则应用到&lt;p&gt;**元素上，而不是其父元素上。\n避免不必要的DOM树修改：尽量减少DOM树的修改次数，因为每次修改都可能触发回流。如果不需要修改某个DOM元素的样式或内容，就不要修改它。\n使用文档碎片（Document Fragment）：如果需要动态创建一组DOM元素并将其添加到文档中，可以使用文档碎片来一次性添加，而不是一个个添加。这样可以减少回流次数。\n批量操作DOM和CSS：如果必须多次操作DOM元素，尽量将这些操作合并到一个批处理中，以减少回流的发生。例如，在循环中修改多个DOM元素之前，可以先将它们保存到数组中，然后一次性应用修改。\n使用CSS类切换：使用CSS类切换来改变元素的样式，而不是直接操作样式属性。这样可以更容易管理和优化样式的应用。\n使用虚拟DOM（Virtual DOM）：在一些现代JavaScript框架（如React、Vue等）中，使用虚拟DOM可以最小化对实际DOM的修改，从而减少回流的频率。\n使用事件委托：将事件处理程序绑定到祖先元素上，以便处理子元素的事件，而不是为每个子元素都绑定事件处理程序。\n\nJS\nJavaScript的编程思想主要体现在以下几个方面：\n\n事件驱动编程：JavaScript最初设计用于增强网页交互性，因此它在核心上是事件驱动的。这意味着代码的执行通常是由用户行为（如点击、滚动等）或者浏览器事件（如页面加载完成）触发的。\n原型继承：JavaScript使用原型（prototype）来实现对象间的继承。与传统的类继承不同，原型继承允许对象直接从其他对象继承属性和方法，这使得JavaScript的对象系统非常灵活。\n函数式编程：JavaScript支持一流函数（first-class functions），这意味着函数可以像任何其他对象一样被使用，包括作为参数传递、赋值给变量或作为其他函数的返回值。这使得JavaScript非常适合函数式编程风格。\n异步编程：由于JavaScript经常用于处理网络请求和用户界面，异步编程在JavaScript中非常重要。Promise和async/await是JavaScript处理异步操作的现代工具。\n灵活的类型系统：JavaScript是一种弱类型语言，这意味着变量可以在运行时更改类型。虽然这提供了很大的灵活性，但也可能导致意外的类型转换和错误。\n模块化：随着ES6标准的推出，JavaScript开始支持模块化编程，允许开发者将代码分割成可重用的模块。\n客户端和服务器端编程：虽然JavaScript最初是为浏览器设计的，但随着Node.js的出现，它现在也被广泛用于服务器端编程。这意味着现代JavaScript开发者可以使用同一种语言编写客户端和服务器端代码。\n动态脚本语言：作为一种解释型语言，JavaScript代码在运行时被解释和执行，而不是事先编译。这使得JavaScript具有高度的灵活性和动态性。\n\n\n\n继承和原型链\nJavaScript 对象是变量的容器。\n在基于类的面向对象语言中（比如Java和C++）， 是构建在类(class)和实例(instance)上的。其中类定义了所有用于具有某一特征对象的属性。类是抽象的事物， 而不是其所描述的全部对象中的任何特定的个体。另一方面， 一个实例是一个类的实例化，是其中的一个成员。\nJavaScript 中的原型链是一种用于实现继承和共享属性的机制。理解原型链对于深入理解 JavaScript 是非常重要的。这里是一个简单的解释：\n\n什么是原型？\n\n在 JavaScript 中，几乎所有的对象都有一个内部链接指向另一个对象，这个对象被称为它的“原型”。\n每个对象的原型都有自己的原型，以此类推，形成了一个“原型链”。\n\n\n原型链的作用：\n\n当尝试访问一个对象的属性时，如果这个对象本身没有这个属性，JavaScript 会沿着原型链向上查找，直到找到这个属性或达到原型链的顶端（null）。\n这就允许对象继承另一个对象的属性和方法。\n\n\n如何创建原型链？\n\n在 JavaScript 中，原型链通常是通过构造函数和它的 prototype 属性建立的。\n当你创建一个新对象时（使用 new 关键字），这个对象的原型就被设置为其构造函数的 prototype 属性。\n\n\n示例：\n假设有一个构造函数 Animal 和它的原型 Animal.prototype。如果你创建了一个 Animal 的实例，比如 let dog = new Animal()，那么 dog 的原型就是 Animal.prototype。\n原型链的顶端：\n\n原型链的顶端是 Object.prototype。所有的 JavaScript 对象都继承自 Object.prototype，除非它们的原型被显式设置为 null。\nObject.prototype 的原型是 null，这表示原型链的结束。\n\n\n原型链的好处和风险：\n\n好处：允许对象共享方法和属性，节省内存。\n风险：如果不正确使用，可能导致意外的副作用，比如修改原型上的属性会影响所有从该原型继承的对象。\n\n\n\n基于原型的面向对象在基于原型的语言中（如JavaScript）并不存在这种区别：不论是构造函数(constructor)，实例(instance)，原型(prototype)本身都是对象。基于原型的语言具有所谓的原型对象的概念，新对象可以从中获得原始的属性。\n\n\nJavaScript 是一门高级语言。对于写网络应用程序而言，它足够灵活且富有表达力。它有许多优势——它是动态类型的，不需要编译环节以及一个巨大的能够提供强大框架、库和其他工具的生态系统。\n\n\nWebAssembly 是一门低级的类汇编语言。它有一种紧凑的二进制格式，使其能够以接近原生性能的速度运行，并且为诸如 C++和 Rust 等拥有低级的内存模型语言提供了一个编译目标以便它们能够在网络上运行。（注意，WebAssembly 有一个在将来支持使用了垃圾回收内存模型的语言的高级目标。）\n\n\n语法细节\neval() 是一个危险的函数，它使用与调用者相同的权限执行代码。如果你用 eval() 运行的字符串代码被恶意方（不怀好意的人）修改，你最终可能会在你的网页/扩展程序的权限下，在用户计算机上运行恶意代码。更重要的是，第三方代码可以看到某一个 eval() 被调用时的作用域，这也有可能导致一些不同方式的攻击。相似的window.Function [Function](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Function) 就不容易被攻击。\neval() 通常比其他替代方法更慢，因为它必须调用 JS 解释器，而许多其他结构则可被现代 JS 引擎进行优化。此外，现代 JavaScript 解释器将 JavaScript 转换为机器代码, it’s inefficient to use eval()\nNumber.isNaN(), that only returns true if the value is actually NaN. The isNaN function returns unexpected values because it tries to convert the value to number and then check if it is NaN\n一元运算符加号（+）首先把非基本类型通过ToPrimitive抽象操作转换为基本类型，如果加号中的两项有一项是字符串，另一项则进行ToString操作，进行字符串拼接，如果是布尔值加数字，则对布尔进行ToNumber操作再求和\n\nJavaScript 数组是可调整大小的，并且可以包含不同的数据类型。（当不需要这些特征时，可以使用类型化数组。）\nJavaScript 数组不是关联数组  JavaScript 数组的索引从 0 开始：数组的第一个元素在索引 0 处，第二个在索引 1 处，以此类推，最后一个元素是数组的 [length](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Array/length) 属性减去 1 的值。\nJavaScript 数组复制操作创建浅拷贝。（所有 JavaScript 对象的标准内置复制操作都会创建浅拷贝，而不是深拷贝）。\n\nAssociative Array 有很多别名，它和 Map（映射）、Dictionary（字典）本质上是一个东西。 Associative Array 由 Key-Value Pair（键值对）组成，在 Associative Array 里面，每个 key 都是独一无二的。 这种数据类型支持【增查改删】这四种常见操作。 2. 我们为什么需要关联数组？ Array 缺点也很明显，【查改删】特别麻烦（需要找到另一块连续内存）。 而另一种线性数据结构——Linked List，通过增加指针的方式来尝试解决 Array 存在的问题，这种方式的缺点在于，每一个操作都需要遍历所有的元素（因为它是线性的……）。 既然线性数据结构无法满足我们的需求，那我们就用非线性数据结构来实现 Associative Array 吧！ 在选择数据结构的时候，我们要考虑可能的操作，如果只是简单的【查】，那么 Array 是最高效的，但是如果涉及到【增查改删】这些复杂的操作，就应该选择 Associative Array 的相关数据结构（比如 dictionary，map……）。\nAssociative Array 常用 Tree 和 Hash Table 这两种数据结构来实现\n\n\n问号（?）在 JavaScript 中的三种主要用途\n三元运算符 可选链 空值凝聚\n空值凝聚的工作原理与逻辑 OR 运算符完全一样，只是当左边的值为 undefined 或 null 时，你会得到右边的值。短路机制=》 对付0或’ ’时  ||无效 =》 空值凝聚\n换句话说，?? 只允许 undefined 或者 null 值，不允许空字符串（''）或 0。\nJavaScript 是一种单线程语言，这意味着它一次只能执行一段代码。然而，事件循环的概念允许 JavaScript 高效地处理异步操作，例如 I/O 操作、计时器和回调，而不会阻塞主线程。\n事件循环是 JavaScript 处理并发的关键部分，并提供非阻塞方式来管理异步操作。它的工作原理如下：\n调用堆栈：每当 JavaScript 程序开始运行时，它都会首先执行全局范围内的代码。调用堆栈跟踪当前正在执行的函数。当一个函数被调用时，它被添加到调用堆栈的顶部，当它返回时，它被从堆栈中删除。\nWeb API 和异步任务：JavaScript 可以访问浏览器或环境提供的 Web API，例如 setTimeout、fetch 和 XMLHttpRequest。这些 API 允许启动异步任务。当异步任务启动时，它会移出主线程并进入 Web API 环境，其余代码继续执行。\n回调队列：异步任务完成后，会生成一条消息（回调），并将其放入回调队列中。该队列保存等待执行的消息/回调。\n事件循环：事件循环的工作是监视调用堆栈和回调队列，使得IO操作js中永不阻塞。当调用堆栈为空时（即所有同步代码已执行完毕），事件循环检查回调队列中是否有任何消息/回调。如果有消息，则会将其从队列移至调用堆栈，并执行其关联的函数。\n**微任务队列：**除了回调队列之外，还有一个微任务队列。 Promise 和某些 API（例如queueMicrotask）将任务添加到此队列中。微任务比常规回调具有更高的优先级，并且它们在事件循环的下一个周期之前执行。 Microtask-》DOM 渲染-》 Macrotask\nEvent loop\nJavaScript 有一个基于事件循环的并发模型，事件循环负责执行代码、收集和处理事件以及执行队列中的子任务。之所以称之为 事件循环，是因为它经常按照类似如下的方式来被实现：\nJS\nwhile (queue.waitForMessage()) {  queue.processNextMessage();}\nqueue.waitForMessage() 会同步地等待消息到达 (如果当前没有任何消息等待被处理)。\n\n“执行至完成”\n每一个消息完整地执行后，其他消息才会被执行。这为程序的分析提供了一些优秀的特性，包括：当一个函数执行时，它不会被抢占，只有在它运行完毕之后才会去运行任何其他的代码，才能修改这个函数操作的数据。这与 C 语言不同，例如，如果函数在线程中运行，它可能在任何位置被终止，然后在另一个线程中运行其他代码。\n这个模型的一个缺点在于当一个消息需要太长时间才能处理完毕时，Web 应用程序就无法处理与用户的交互，例如点击或滚动。为了缓解这个问题，浏览器一般会弹出一个“这个脚本运行时间过长”的对话框。一个良好的习惯是缩短单个消息处理时间，并在可能的情况下将一个消息裁剪成多个消息。\n闭包closure\n使用闭包主要是为了设计私有的方法和变量。闭包的优点是可以避免全局变量的污染(计数器困境)；缺点是闭包会常驻内存，增加内存使用量，使用不当很容易造成内存泄漏。在JavaScript中，函数即闭包，只有函数才会产生作用域  闭包有3个特性\n（1）函数嵌套函数。\n（2）在函数内部可以引用外部的参数和变量\n（3）参数和变量不会以垃圾回收机制回收\n设想下如果你想统计一些数值，且该计数器在所有函数中都是可用的。\n你可以使用全局变量，函数设置计数器递增：\n如果我在函数内声明计数器，如果没有调用函数将无法修改计数器的值：\nfunction add() {\nvar counter = 0;\nreturn counter += 1;\n}\nadd();\nadd();\nadd();\n// 本意是想输出 3, 但事与愿违，输出的都是 1 !\n是一个函数以及其捆绑的周边环境状态（lexical environment，词法环境）的引用的组合。换而言之，闭包让开发者可以从内部函数访问外部函数的作用域。在 JavaScript 中，闭包会随着函数的创建而被同时创建。\n通常你使用只有一个方法的对象的地方，都可以使用闭包。\nfunction makeAdder(x) {  return function (y) {    return x + y;  };}var add5 = makeAdder(5);var add10 = makeAdder(10);console.log(add5(2)); // 7console.log(add10(2)); // 12\n在 Web 中，你想要这样做的情况特别常见。大部分我们所写的 JavaScript 代码都是基于事件的 — 定义某种行为，然后将其添加到用户触发的事件之上（比如点击或者按键）。我们的代码通常作为回调：为响应事件而执行的函数。\n防抖和节流\n是优化高频率执行代码的一种手段\n如：浏览器的 resize、scroll、keypress、mousemove 等事件在触发时，会不断地调用绑定在事件上的回调函数，极大地浪费资源，降低前端性能\n为了优化体验，需要对这类事件进行调用次数的限制，对此我们就可以采用 防抖（debounce） 和 节流（throttle） 的方式来减少调用频率\n\n节流: n 秒内只运行一次，若在 n 秒内重复触发，只有一次生效\n防抖: n 秒后在执行该事件，若在 n 秒内被重复触发，则重新计时\n\n一个经典的比喻:\n想象每天上班大厦底下的电梯。把电梯完成一次运送，类比为一次函数的执行和响应\n假设电梯有两种运行策略 debounce 和 throttle，超时设定为15秒，不考虑容量限制\n电梯第一个人进来后，15秒后准时运送一次，这是节流\n电梯第一个人进来后，等待15秒。如果过程中又有人进来，15秒等待重新计时，直到15秒后开始运送，这是防抖\n\n防抖：防止抖动，单位时间内事件触发会被重置，避免事件被误伤触发多次。代码实现重在清零 clearTimeout。防抖可以比作等电梯，只要有一个人进来，就需要再等一会儿。业务场景有避免登录按钮多次点击的重复提交。\n节流：控制流量，单位时间内事件只能触发一次，与服务器端的限流 (Rate Limit) 类似。代码实现重在开锁关锁 timer=timeout; timer=null。节流可以比作过红绿灯，每等一个红灯时间就可以过一批。\n\n完成节流可以使用时间戳与定时器的写法  使用时间戳写法，事件会立即执行，停止触发后没有办法再次执行\n可以将时间戳写法的特性与定时器写法的特性相结合，实现一个更加精确的节流。实现如下\n function throttle(func, delay) {  let timeoutId;  let lastExecTime = 0;  return function(...args) {    const currentTime = new Date().getTime();    if (currentTime - lastExecTime &gt;= delay) {      // 如果距离上次执行的时间大于等于指定的延迟时间，执行函数      func.apply(this, args);      lastExecTime = currentTime; // 更新上次执行的时间    } else {      // 如果未达到延迟时间，则设置定时器延迟执行函数      clearTimeout(timeoutId);      timeoutId = setTimeout(() =&gt; {        func.apply(this, args);        lastExecTime = currentTime; // 更新上次执行的时间      }, delay - (currentTime - lastExecTime));    }  };}\n!https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d9552c9b-777c-450b-9626-98509e1bd1eb/Untitled.png\n防抖在连续的事件，只需触发一次回调的场景有：\n\nsearch搜索联想，用户在不断输入值时，用防抖来节约请求资源。\n窗口大小resize。只需窗口调整完成后，计算窗口大小。防止重复渲染。\n\n节流在间隔一段时间执行一次回调的场景有：\n\n滚动加载，加载更多或滚到底部监听\n鼠标连续点击，mousedown\n\nReact\nReact 是一个由 Facebook 开发的用于构建用户界面的JavaScript库。React 的设计思想主要包括以下几个方面：\n\n组件化： React 的核心思想之一是组件化开发。它将用户界面划分为独立的组件，每个组件有自己的状态和属性，可以被独立地开发、测试和维护。组件化使得代码更具可复用性，提高了开发效率。\n虚拟 DOM： React 引入了虚拟 DOM 的概念。虚拟 DOM 是一个虚拟的浏览器内存中的文档对象模型。当组件的状态发生变化时，React 先在虚拟 DOM 上进行更新，然后通过对比新旧虚拟 DOM，找出变化的部分，最终只更新实际发生变化的部分到真实 DOM。这种优化使得页面更新更为高效。\n单向数据流： React 遵循单向数据流的原则，即数据的流动是单向的。父组件可以通过 props 将数据传递给子组件，但子组件不能直接修改父组件的数据。这种数据流的清晰性有助于理解和追踪数据在应用中的变化。有时造成问题当多个组件需要共享状态时，React推荐的解决方案是将状态提升到这些组件的最近公共父组件。但这会使得公共父组件变得庞大和复杂，同时增加了组件间的依赖性，使得组件重用变得更加困难。\n声明式编程： React 使用声明式的编程风格，开发者只需描述目标的状态是什么，React 会自动负责如何达到这个状态。相比于命令式编程，声明式编程更易于理解和维护。\n可组合性： React 组件可以被自由组合，一个组件可以包含其他组件。这种可组合性使得构建复杂界面变得更为灵活，开发者可以通过组合简单的组件来构建复杂的 UI。\n无状态组件： React 推崇无状态组件的概念。无状态组件指的是没有内部状态（state）的组件，它只依赖于传入的 props。这种组件简单、易于测试和理解，有助于提高应用性能。\n生命周期方法： React 组件具有生命周期方法，这些方法可以在组件的不同阶段执行。这使得开发者可以在组件的不同生命周期阶段执行特定的逻辑，例如在组件挂载后获取数据、在组件更新前进行清理等。\n\nReactElement通过createElement创建，调用该方法需要传入三个参数：\n\ntype\nconfig\nchildren\n\ntype指代这个ReactElement的类型\n\n字符串比如div，p代表原生DOM，称为HostComponent\nClass类型是我们继承自Component或者PureComponent的组件，称为ClassComponent\n方法就是functional Component\n原生提供的Fragment、AsyncMode等是Symbol，会被特殊处理\nTODO: 是否有其他的\n\nReact Hooks 是 React 16.8 版本引入的一项功能，它允许函数式组件拥有类组件中的状态（state）、生命周期方法（如 componentDidMount、componentDidUpdate、**componentWillUnmount**等）以及其他 React 特性。\n纯粹原则\n函数式编程的世界中，纯函数 通常具有如下特征：\n\n只负责自己的任务。它不会更改在该函数调用前就已存在的对象或变量。\n输入相同，则输出相同。给定相同的输入，纯函数应总是返回相同的结果。\n\n上述示例的问题出在渲染过程中，组件改变了 预先存在的 变量的值。为了让它听起来更可怕一点，我们将这种现象称为 突变（mutation） 。纯函数不会改变函数作用域外的变量、或在函数调用前创建的对象——这会使函数变得不纯粹！\n但是，你完全可以在渲染时更改你 刚刚 创建的变量和对象。在本示例中，你创建一个 [] 数组，将其分配给一个 cups 变量，然后 push 一打 cup 进去\n传递给事件处理函数的函数应直接传递，而非调用。例如：\n\n\n\n传递一个函数（正确）\n调用一个函数（错误）\n\n\n\n\n\n\n\n\n\n区别很微妙。在第一个示例中，handleClick 函数作为 onClick 事件处理函数传递。这会让 React 记住它，并且只在用户点击按钮时调用你的函数。\n在第二个示例中，handleClick() 中最后的 () 会在 渲染 过程中 立即 触发函数，即使没有任何点击。这是因为在 JSX { 和 } 之间的 JavaScript 会立即执行。\nwhat’s the problem?\nconst cart = [5, 15, 25];let total = 0;const withTax = cart.map((cost) =&gt; {  total += cost; // Side effect here  return cost * 1.2;});console.log(withTax); // [6, 18, 30]console.log(total); // 45\n渲染会及时生成一张快照\n计数器困境 counter\nReact 会对 state 更新进行批处理 这让你可以更新多个 state 变量——甚至来自多个组件的 state 变量——而不会触发太多的 重新渲染。但这也意味着只有在你的事件处理函数及其中任何代码执行完成 之后，UI 才会更新。这种特性也就是 批处理，它会使你的 React 应用运行得更快。它还会帮你避免处理只更新了一部分 state 变量的令人困惑的“半成品”渲染。 React 不会跨 多个 需要刻意触发的事件（如点击）进行批处理——每次点击都是单独处理的。请放心，React 只会在一般来说安全的情况下才进行批处理。这可以确保，例如，如果第一次点击按钮会禁用表单，那么第二次点击就不会再次提交它。\nexport default function Counter() {  const [number, setNumber] = useState(0);  return (    &lt;&gt;      &lt;h1&gt;{number}&lt;/h1&gt;      &lt;button onClick={() =&gt; {        setNumber(number + 1);        setNumber(number + 1);        setNumber(number + 1);      }}&gt;+3&lt;/button&gt;// Can this work? why    &lt;/&gt;  )}//if don't work,try to use functiononclick={()=&gt;{setNumber(n=&gt;n+1);setNumber(n=&gt;n+1);setNumber(n=&gt;n+1);}}Q:&lt;button onClick={() =&gt; {setNumber(number + 5);setNumber(n =&gt; n + 1);setNumber(42);}}&gt;增加数字&lt;/button&gt; A： 返回什么？？\n主要区别：\nstate 在其表现出的特性上更像是一张快照。设置它不会更改你已有的 state 变量，但会触发重新渲染。 State是可变的，是一组用于反映组件UI变化的状态集合；\n而Props对于使用它的组件来说，是只读的，要想修改Props，只能通过该组件的父组件修改。\n在组件状态上移的场景中，父组件正是通过子组件的Props, 传递给子组件其所需要的状态。\nComponent&amp;props\nReact核心思想是组件化，其中 组件 通过属性(props) 和 状态(state)传递数据。\n组件化也是与分而治之类似的思想：\n\n如果我们将页面逻辑全部放在同一个页面中，处理起来将会非常复杂，不利于后期的管理拓展和维护。\n如果我们将页面逻辑拆分成一个个小的功能块，每个功能块完成自己这部分独立的功能，然后整合成一个页面，那么之后整个页面的管理和维护就变得非常容易了。\n\nprops 是组件对外的接口，state 是组件对内的接口。组件内可以引用其他组件，组件之间的引用形成了一个树状结构（组件树），如果下层组件需要使用上层组件的数据或方法，上层组件就可以通过下层组件的props属性进行传递，因此props是组件对外的接口。组件除了使用上层组件传递的数据外，自身也可能需要维护管理数据，这就是组件对内的接口state。根据对外接口props 和对内接口state，组件计算出对应界面的UI。\n定义组件最简单的方式就是编写 JavaScript 函数：\nfunction Name(props) {    return &lt;h1&gt;网站名称：{props.name}&lt;/h1&gt;;}function Url(props) {    return &lt;h1&gt;网站地址：{props.url}&lt;/h1&gt;;}function Nickname(props) {    return &lt;h1&gt;网站小名：{props.nickname}&lt;/h1&gt;;}function App() {    return (    &lt;div&gt;        &lt;Name name=\"菜鸟教程\" /&gt;        &lt;Url url=\"http://www.runoob.com\" /&gt;        &lt;Nickname nickname=\"Runoob\" /&gt;    &lt;/div&gt;    );} ReactDOM.render(     &lt;App /&gt;,    document.getElementById('example'));\n\nReact组件被声明一次\n但组件可以作为JSX中的React元素被多次使用\n当元素被使用时，它就成为该组件的一个实例，挂载在React的组件树中\n\n在JavaScript中，forEach() 和 map() 方法都是用于遍历数组的，但它们在使用目的和行为上有明显的区别。\n\n返回值:\n\nforEach(): 这个方法不返回任何值（即返回**undefined**）。它仅用于对数组的每个元素执行某些操作，比如打印元素或将元素的值修改。\nmap(): 这个方法返回一个新的数组，这个数组是由原数组中的每个元素调用一个指定的函数后的返回值组成的。这意味着**map()**是不会改变原数组的，它通过所提供的函数生成一个新的数组。\n\n\n用途:\n\n使用**forEach()**时，通常是想对数组中的每个元素执行一些操作，如更新或打印元素，而不关心返回值。\n**map()**则用于当你想基于原数组创建一个新数组时，每个元素都是通过指定函数变换后的结果。\n\n\n副作用与不可变性:\n\n**forEach()**可能会产生副作用，因为它经常用于修改原始数组或外部变量。\n**map()**遵循不可变性原则，不修改原始数组，而是返回一个全新的数组，这对于函数式编程特别有用，其中不可变性是一个重要概念。\n\n\n\n关于您提到的“React中只有map”，可能的含义是，在React的渲染逻辑中，经常使用**map()来遍历数据数组并返回一个元素数组，这些元素可以直接在JSX中渲染。这是因为map()**能够从原始数据数组创建一个新的元素数组，而不会改变原始数据，这对于React渲染优化和状态管理非常重要。\n对 React 的 Virtual DOM 的误解： https://www.zhihu.com/question/31809713\nVirtual DOM 虚拟DOM\n传统的web应用，操作DOM一般是直接更新操作的，但是我们知道DOM更新通常是比较昂贵的。而React为了尽可能减少对DOM的操作，提供了一种不同的而又强大的方式来更新DOM，代替直接的DOM操作。就是Virtual DOM,一个轻量级的虚拟的DOM，就是React抽象出来的一个对象，描述dom应该什么样子的，应该如何呈现。通过这个Virtual DOM去更新真实的DOM，由这个Virtual DOM管理真实DOM的更新。\n为什么通过这多一层的Virtual DOM操作就能更快呢？ 这是因为React有个diff算法，更新Virtual DOM并不保证马上影响真实的DOM，React会等到事件循环结束，然后利用这个diff算法，通过当前新的dom表述与之前的作比较，计算出最小的步骤更新真实的DOM。\nBy default, React DOM escapes any values embedded in JSX before rendering them. Thus it ensures that you can never inject anything that’s not explicitly written in your application. Everything is converted to a string before being rendered. This helps prevent XSS (cross-site-scripting) attacks.使用大括号，来在属性值中插入一个 JavaScript 表达式****\nReact DOM 在渲染所有输入内容之前，默认会进行转义\n。它可以确保在你的应用中，永远不会注入那些并非自己明确编写的内容。所有的内容在渲染之前都被转换成了字符串。这样可以有效地防止 XSS\n但是Virtual dom也不是在所有的情况下都是最快的，在比较性能的时候，要分清楚初始渲染、小量数据更新、大量数据更新这些不同的场合\n为什么需要虚拟DOM的一些原因：\n\n性能优化：\n\n直接操作真实DOM可能会导致频繁的重排（Reflow）和重绘（Repaint），这会影响页面性能。虚拟DOM充当了真实DOM的抽象层，允许对虚拟DOM进行频繁的更新操作，然后将更新后的虚拟DOM与实际DOM进行比较，最终只更新实际需要变更的部分。这样可以最小化DOM操作，减少重排和重绘，提高页面性能。\n\n\n批量更新：\n\n虚拟DOM可以进行批量更新，将多个更新合并为单个更新操作。相比直接操作真实DOM，这样可以减少更新频率，提高效率。\n\n\n框架状态管理：\n\n虚拟DOM使得框架可以更好地管理应用的状态变更。在React等框架中，通过比较前后两个虚拟DOM树的差异，可以确定需要更新的部分，从而实现状态变更的响应式更新。\n\n\n\ninnerHTML vs. Virtual DOM 的重绘性能消耗：\n\ninnerHTML: render html string O(template size) + 重新创建所有 DOM 元素 O(DOM size)\nVirtual DOM: render Virtual DOM + diff O(template size) + 必要的 DOM 更新 O(DOM change)\n\nVirtual DOM render + diff 显然比渲染 html 字符串要慢，但是！它依然是纯 js 层面的计算，比起后面的 DOM 操作来说，依然便宜了太多。可以看到，innerHTML 的总计算量不管是 js 计算还是 DOM 操作都是和整个界面的大小相关，但 Virtual DOM 的计算量里面，只有 js 计算和界面大小相关，DOM 操作是和数据的变动量相关的。前面说了，和 DOM 操作比起来，js 计算是极其便宜的。这才是为什么要有 Virtual DOM：它保证了 1）不管你的数据变化多少，每次重绘的性能都可以接受；2) 你依然可以用类似 innerHTML 的思路去写你的应用。\n作者：尤雨溪\n在DOM树上的节点被称为元素，在这里则不同，Virtual DOM上称为component。Virtual DOM的节点就是一个完整抽象的组件，它是由components组成。\n假设你的 HTML 文件某处有一个 &lt;div&gt;：\n&lt;div id=\"root\"&gt;&lt;/div&gt;\n我们将其称为“根” DOM 节点，因为该节点内的所有内容都将由 React DOM 管理。\n仅使用 React 构建的应用通常只有单一的根 DOM 节点。如果你在将 React 集成进一个已有应用，那么你可以在应用中包含任意多的独立根 DOM 节点。\n想要将一个 React 元素渲染到根 DOM 节点中，只需把它们一起传入 [ReactDOM.createRoot()](&lt;https://zh-hans.reactjs.org/docs/react-dom-client.html#createroot&gt;)：\n事件处理\n例如，传统的 HTML：\n&lt;button onclick=\"activateLasers()\"&gt;   Activate Lasers &lt;/button&gt;\n在 React 中略微不同：\n&lt;button onClick={activateLasers}&gt;  Activate Lasers &lt;/button&gt;\n在 React 中另一个不同点是你不能通过返回 false 的方式阻止默认行为。\ne 是一个合成事件。React 根据 W3C 规范来定义这些合成事件，所以你不需要担心跨浏览器的兼容性问题。React 事件与原生事件不完全相同。如果想了解更多，请查看 [SyntheticEvent](&lt;https://zh-hans.reactjs.org/docs/events.html&gt;) 参考指南。\n使用 React 时，你一般不需要使用 addEventListener 为已创建的 DOM 元素添加监听器。事实上，你只需要在该元素初始渲染的时候添加监听器即可。\n当你使用 ES6 class 语法定义一个组件的时候，通常的做法是将事件处理函数声明为 class 中的方法。例如，下面的 Toggle 组件会渲染一个让用户切换开关状态的按钮：\n\n*useCallback的作用其实是用来避免子组件不必要的reRender：**\n\n首先，假如我们不使用useCallback，在父组件中创建了一个名为handleClick的事件处理函数，根据需求我们需要把这个handleClick传给子组件，当父组件中的一些state变化后（这些state跟子组件没有关系），父组件会re Render，然后会重新创建名为handleClick函数实例，并传给子组件，这时即使用React.memo把子组件包裹起来，子组件也会重新渲染，因为props已经变化了，但这个渲染是无意义的\n如何优化呢？这时候就可以用useCallback了，我们用useCallback把函数包起来之后，在父组件中只有当deps变化的时候，才会创建新的handleClick实例，子组件才会跟着reRender（注意，必须要用React.memo把子组件包起来才有用，否则子组件还是会reRender。React.memo是类似于class组件中的Pure.Component的作用）\n对于这种deps不是经常变化的情况，我们用useCallback和React.memo的方式可以很好地避免子组件无效的reRender。但其实社区中对这个useCallback的使用也有争议，比如子组件中只是渲染了几个div，没有其他的大量计算，而浏览器去重新渲染几个dom的性能损耗其实也是非常小的，我们花了这么大的劲，使用了useCallback和React.memo，换来的收益很小，所以一些人认为就不用useCallback，就让浏览器去重新渲染好了。至于到底用不用，此处不深入讨论，我的建议是当子组件中的dom数量很多，或者有一些大量的计算操作，是可以进行这样的优化的。\nHook\nHook 就是 JavaScript 函数，但是使用它们会有两个额外的规则：\n\n只能在函数最外层调用 Hook。不要在循环、条件判断或者子函数中调用。\n只能在 React 的函数组件中调用 Hook。不要在其他 JavaScript 函数中调用。（还有一个地方可以调用 Hook —— 就是自定义的 Hook 中）\n\n是 React 16.8 的新增特性。它可以让你在**不编写 class 的情况下使用 state 以及其他的 React 特性。**在组件之间复用状态逻辑很难、大型组件很难拆分和重构，也很难测试。Hook 使你在无需修改组件结构的情况下复用状态逻辑。没有 class 继承、没有 this、没有生命周期、代码更加简洁、这就是使用 hooks 的意义\nHook 是一些可以让你在函数组件里“钩入” React state 及生命周期等特性的函数。\n为什么要使用 hooks\n难以理解的 class 、组件中必须去理解 javascript 与 this 的工作方式、需要绑定事件处理器、纯函数组件与 class 组件的差异存在分歧、甚至需要区分两种组件的使用场景；Hook 可以使你在非 class 的情况下可以使用更多的 React 特性。\nReact 提供了一些内置的 Hooks，比如 useState。您还可以创建自己的 Hooks 以在不同组件之间重用有状态行为。\nUsestate hook等价于写class &amp; render\n向 class 组件中添加局部的 state,state是异步的\n当您调用 时setState()，React 会将您提供的对象合并到当前状态。setState(x) 实际上会像 setState(n =&gt; x) 一样运行,  将state更新加入队列进行批处理\nconst [state, setState] = useState(initialState);\nsetState返回一个 state，以及更新 state 的函数。在初始渲染期间，返回的状态 (state) 与传入的第一个参数 (initialState) 值相同。\nsetState 函数用于更新 state。它接收一个新的 state 值并将组件的一次重新渲染加入队列。Or函数式更新\nuseRef 与 useState区别\nuseRef 能让你引用一个不需要渲染的值。\n2useState 返回 2 个属性或一个数组。一个是值或状态，另一个是更新状态的函数。相反，useRef用来储存persitent value 只返回一个值，即实际存储的数据。\n3当参考值改变时，它会被更新而不需要刷新或重新渲染。但是在 useState 中，组件必须再次渲染以更新状态或其值。\n何时使用 Refs 和 States\nRefs 在获取用户输入、DOM 元素属性和存储不断更新的值时很有用。ref 不适合用于存储期望显示在屏幕上的信息\n不管是父组件或是子组件都无法知道某个组件是有状态的还是无状态的，并且它们也并不关心它是函数组件还是 class 组件。\n这就是为什么称 state 为局部的或是封装的的原因。除了拥有并设置了它的组件，其他组件都无法访问。当你在定时器中操作 state 的时候，而 setState 更新就是同步的。\nuseEffect是什么？\n它允许你 将组件与外部系统同步。副作用钩子：用于处理组件中的副作用，用来取代生命周期函数。the function passed to useEffectfires after layout and paint, during a deferred event. This makes it suitable for the many common side effects, like setting up subscriptions and event handlers, because most types of work shouldn’t block the browser from updating the screen.\nTake a look at the useEffect Hook. At the end of the Hook, we’re returning a new function. This is the equivalent of the componentWillUnmount lifecycle method in a class-based React component. To learn more about the differences between functional components and class-based components check out this guide\nimport React, { useState, useEffect } from 'react';function Example() {  const [count, setCount] = useState(0);  useEffect(() =&gt; {    document.title = `You clicked ${count} times`;  }, [count]);  return (    &lt;div&gt;      &lt;p&gt;You clicked {count} times&lt;/p&gt;      &lt;button onClick={() =&gt; setCount(count + 1)}&gt;        Click me      &lt;/button&gt;    &lt;/div&gt;  );}\n当props.source变动时 执行 //func\nLifecycle\nComponent 组件的生命周期可分成三个状态：\n Mounting(挂载)：已插入真实DOM. Updating(更新)：正在被重新渲染\n Unmounting(卸载)：已移出真实DOM\n!https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d5b07601-37a4-4929-8939-6be7a2ef3977/Untitled.png\nindex.js为入口               ReactDOM.render(\n函数式组件\n\n*export*声明用于从 JavaScript 模块中导出值。然后可以使用[import](&lt;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/import&gt;)声明或动态导入将导出的值导入其他程序。导入绑定的值会在导出它的模块中发生变化——当模块更新它导出的绑定的值时，更新将在其导入值中可见。\n\nTypescript\nTypeScript was designed and developed by Microsoft and first released in 2012. The primary motivations for creating TypeScript were to address certain challenges and limitations faced by large-scale application development in JavaScript. Here are some key reasons behind the design and introduction of TypeScript:\n\nType Safety: JavaScript is a dynamically typed language, which means that variables can hold values of any type and types are checked at runtime. While this offers flexibility, it also makes it difficult to detect certain types of errors during development. TypeScript introduces static typing, allowing developers to catch errors related to incorrect types at compile time, well before the code is run.\nScalability: As web applications grew in complexity around the early 2010s, there was a clear need for tools that could better support the development of large-scale applications. TypeScript’s static typing system helps manage and maintain large codebases, making it easier for teams to work on complex projects and ensure code quality.\nTooling and Developer Experience: TypeScript was designed to enhance the developer experience, providing better tooling support for autocompletion, navigation, and refactoring. These features are particularly useful in large projects and can significantly improve productivity and reduce the likelihood of bugs.\nESNext Features: JavaScript’s evolution is managed through the ECMAScript (ES) standards, which evolve over time. TypeScript aimed to allow developers to use features from future ECMAScript standards while still compiling down to JavaScript that browsers could execute. This forward-compatibility was a significant draw for developers wanting to use the latest language features.\n\nTypescript 是一个强类型的 JavaScript 超集，支持ES6语法，支持面向对象编程的概念，如类、接口、继承、泛型等。Typescript并不直接在浏览器上运行，需要编译器编译成纯Javascript来运行。TS对JS的改进主要是静态类型检查TypeScript的核心原则之一是对值所具有的结构进行类型检查。 它有时被称做“鸭式辨型法”或“结构性子类型化”。 在TypeScript里，接口的作用就是为这些类型命名和为你的代码或第三方代码定义契约。\n安全链式调用\n`// 这里 Error对象定义的stack是可选参数，如果这样写的话编译器会提示\n// 出错 TS2532: Object is possibly ‘undefined’.\nreturn new Error().stack.split(‘\\n’);\n// 我们可以添加?操作符，当stack属性存在时，调用 stack.split。若stack不存在，则返回空\nreturn new Error().stack?.split(‘\\n’);`\n组件不仅能够支持当前的数据类型，同时也能支持未来的数据类型，这在创建大型系统时为你提供了十分灵活的功能。在像C#和Java这样的语言中，可以使用泛型来创建可重用的组件，一个组件可以支持多种类型的数据。 这样用户就可以以自己的数据类型来使用组件。\n泛型即是类型的形参\n接口和别名的区别\n\n都可以用来自定义类型\n类型别名支持联合和交叉类型定义\n别名不支持重复命名，接口可以\n\nVue\nVue 数据双向绑定原理是通过 数据劫持 + 发布者-订阅者模式 的方式来实现的，首先是通过 ES5 提供的 Object.defineProperty() 方法来劫持（监听）各属性的 getter、setter，并在当监听的属性发生变动时通知订阅者，是否需要更新，若更新就会执行对应的更新函数。详见 vue源码。\n\n常见的基于数据劫持的双向绑定有两种实现\n\n一个是目前Vue在用的 Object.defineProperty\n一个是ES2015中新增的 Proxy，而在Vue3.0版本后加入Proxy从而代替Object.defineProperty\n\n\n\njQuery强调的理念是写得少，做得多（write less, do more）。它简化了操作DOM的方法，让早期的程序员们能更方便的实现动画、修改CSS等各种操作，说它是JavaScript史上使用最广泛的一个库也不为过。jQuery独特的选择器、链式操作、事件处理机制和封装完善的Ajax都是其他JavaScript库望尘莫及的。\njQuery 的核心是一个文档对象模型(DOM) 操作库。DOM 是网页所有元素的树形结构表示。jQuery 简化了查找、选择和操作这些 DOM 元素的语法。例如，jQuery 可用于查找文档中具有特定属性的元素（例如，所有带有标签的元素[h1](https://en.wikipedia.org/wiki/HTML_element#heading)）、更改其一个或多个属性（例如color，visibility），或使其响应事件（例如，鼠标点击）。\njQuery 还提供了超越基本 DOM 元素选择和操作的事件处理范例。事件分配和事件回调函数定义是在代码中的单个位置中一步完成的。jQuery 还旨在合并其他常用的 JavaScript 功能（例如，隐藏元素时的淡入和淡出、通过操作CSS属性实现动画）。\n使用 jQuery 进行开发的原则是：\n\nJavaScript 和 HTML 的分离：jQuery 库提供了使用 JavaScript 将事件DOM 的HTML 事件属性完全分离\n\nAxios 基于promise用于浏览器和node.js的http客户端。\nUmi\n可插拔的企业级 React 应用程序框架。 “umi 是一个基于路由的框架，支持 next.js 式的常规路由和各种高级路由功能，比如路由级别的按需加载，支持多种插件如AntD\nAnt design\nAnt Design System 是企业级 UI 设计语言和 React UI 库的开源代码。它带有一组高质量的 React 组件，具有主题定制能力\nThe difference between Switch and Checkbox is that Switch will trigger a state change directly when you toggle it, while Checkbox is generally used for state marking, which should work in conjunction with submit operation.\nProtable\nProTable 在 antd 的 Table 上进行了一层封装，支持了一些预设，并且封装了一些行为。这里只列出与 antd Table 不同的 api。\nrequest\nrequest 是 ProTable 最重要的 API，request 会接收一个对象。对象中必须要有 data 和 success，如果需要手动分页 total 也是必需的。request 会接管 loading 的设置，同时在查询表单查询和 params 参数发生修改时重新执行。同时 查询表单的值和 params 参数也会带入。\nES6\nECMAScript 和 JavaScript 的关系是，前者是后者的规格，后者是前者的一种实现。\n新增:1.类 2.模块化\n在ES6中模块作为重要的组成部分被添加进来。模块的功能主要由 export 和 import 组成。每一个模块都有自己单独的作用域，模块之间的相互调用关系是通过 export 来规定模块对外暴露的接口，通过 import 来引用其它模块提供的接口。同时还为模块创造了命名空间，防止函数的命名冲突。\n3.箭头函数\n=&gt;不只是关键字 function 的简写，它还带来了其它好处。箭头函数与包围它的代码**共享同一个 this,**能帮你很好的解决this的指向问题。比如 var self = this;或 var that =this这种引用外围this的模式。但借助 =&gt;，就不需要这种模式了。(a,b) =&gt; a+b 相当于function(a,b) {a+b;}\n(param1, param2, …, paramN) =&gt; { statements }(param1, param2, …, paramN) =&gt; expression//相当于：(param1, param2, …, paramN) =&gt;{ return expression; }//加括号的函数体返回对象字面量表达式：params =&gt; ({foo: bar})\n由于JavaScript函数对this绑定的错误处理，下面的例子无法得到预期结果：\nvar obj = {    birth: 1990,    getAge:function () {var b =this.birth;// 1990var fn =function () {return new Date().getFullYear() -this.birth;// this指向window或undefined        };return fn();    }};\n现在，箭头函数完全修复了this的指向，this总是指向词法作用域，也就是外层调用者obj：\n4.使用模板字符串 var name = Your name is {last}.``\n在 ES6 中通过 ${}就可以完成字符串的拼接，只需要将变量放在大括号之中。\n5.解构赋值\n定义：ES6允许按照一定模式，从数组和对象中提取值，对变量进行赋值，这被称为解构（Destructuring）。解构的语法：\nconst {dispatch} = this.props;\n这段代码你可以认为是这样：\nconst dispatch = this.props.dispatch;\n7.Promise\nPromise 是异步编程的一种解决方案，比传统的解决方案 callback 更加的优雅。Promise 是一种用于处理异步操作的对象，它代表一个尚未完成的操作。Promise 可以具有成功（resolved）状态或失败（rejected）状态，允许将回调函数绑定到这两种状态，以处理操作结果。\nA Promise 是表示异步操作最终完成或失败的对象。由于大多数人都是已经创建的 Promise 的消费者\n// 创建一个 Promiseconst myPromise = new Promise((resolve, reject) =&gt; {  // 模拟异步操作，这里使用 setTimeout 延时 2 秒  setTimeout(() =&gt; {    const randomNumber = Math.random();    if (randomNumber &lt; 0.5) {      // 当随机数小于 0.5 时，解决 Promise 并传递一个值      resolve(\"Promise 被解决了，随机数为 \" + randomNumber);    } else {      // 当随机数大于等于 0.5 时，拒绝 Promise 并传递一个错误信息      reject(\"Promise 被拒绝了，随机数为 \" + randomNumber);    }  }, 2000); // 2 秒延时});// 处理 Promise 的解决和拒绝情况myPromise  .then((result) =&gt; {    // 当 Promise 被解决时执行的回调    console.log(\"解决: \" + result);  })  .catch((error) =&gt; {    // 当 Promise 被拒绝时执行的回调    console.error(\"拒绝: \" + error);  });\nPromise 很棒的一点就是链式调用（chaining）连续执行两个或者多个异步操作是一个常见的需求，在上一个操作执行成功之后，开始下一个的操作，并带着上一步操作所返回的结果。我们可以通过创造一个 Promise 链来实现这种需求。then() 函数会返回一个和原来不同的新的 Promise\n现在，我们可以把回调绑定到返回的 Promise 上，形成一个 Promise 链来避免原来写法的回调地狱\n使用 then() 和 async/await 是在 JavaScript 中处理异步操作的两种不同方法。\nWhich to Choose then() 和 async/await ?\n\nExisting Codebase: If your project already uses one approach consistently, it might be best to stick with that for consistency.\nPreference: Some developers prefer the more explicit nature of then(), while others find async/await syntax more readable.\nUse Case: For simpler scenarios, either approach is fine. For more complex asynchronous logic, async/await can provide cleaner code.\n\nIn modern JavaScript projects, async/await is often the preferred choice due to its cleaner syntax and improved readability. However, the choice ultimately depends on the context and the preferences of the development team.\n优点：\n\n提供更像同步的代码结构，使其更易于阅读和推理。\n使用 简化错误处理**try/catch** 处理多个异步操作时更容易使用。\n\n缺点：\n\n需要现代 JavaScript 环境或转译器（如 Babel）以实现兼容性。\n对于某些场景可能需要额外的工具，例如处理 Promise.all。\n\nasync function fetchData() {  try {    const response = await fetch('https://api.example.com/data');    if (!response.ok) {      throw new Error('Failed to fetch data');    }    const data = await response.json();    console.log('Data:', data);  } catch (error) {    console.error('Error:', error.message);  }}// Call the functionfetchData();\n8.let 与 const\n在之前 JS 是没有块级作用域的，const与 let 填补了这方便的空白，const与 let 都是块级作用域。\n在 TypeScript 函数里，如果我们定义了参数，则我们必须传入这些参数，除非将这些参数设置为可选，可选参数使用问号标识 ？\nconst定义常量与使用let 定义的变量相似：\n\n二者都是块级作用域\n都不能和它所在作用域内的其他变量或函数拥有相同的名称\n\n两者还有以下两点区别：\n\nconst声明的常量必须初始化，而let声明的变量不用\nconst 定义常量的值不能通过再赋值修改，也不能再次声明。而 let 定义的变量值可以修改。\n\nconst 的本质: const 定义的变量并非常量，并非不可变，它定义了一个常量引用一个值。使用 const 定义的对象或者数组，其实是可变的。下面的代码并不会报错：\n// 创建常量对象const car = {type:\"Fiat\", model:\"500\", color:\"white\"};// 修改属性:car.color = \"red\";\nJS中的异步并发\n异步就是彼此独立,在等待某事件的过程中继续做自己的事，不需要等待这一事件完成后再工作。线程就是实现异步的一个方式。异步是让调用方法的主线程不需要同步等待另一线程的完成，从而可以让主线程干其它的事情。异步是当一个调用请求发送给被调用者,而调用者不用等待其结果的返回而可以做其它的事情。实现异步可以采用多线程技术或则交给另外的进程来处理。\nWEB应用非常适合异步编程方式而不是多线程，因为网络请求资源，大部分时间是在io等待\nI/O密集型任务适合异步编程，因为在这种情况下，大部分时间都花在等待外部资源的响应（如网络请求、数据库查询等），而不是消耗CPU进行计算。多线程在这种情况下可能会浪费CPU资源，因为线程会在等待I/O完成期间处于空闲状态。\n异步编程允许单个线程在等待I/O的同时处理其他任务，从而提高整体的资源利用率。在异步编程中，当一个I/O操作被触发后，线程可以继续处理其他任务，而不会阻塞等待I/O操作完成。\n多线程通常适用于CPU密集型任务，这些任务需要大量的计算资源。在这种情况下，通过多线程可以充分利用多核处理器的计算能力，提高计算性能。\n下面是一些使用多线程的常见情况：\n\nCPU密集型任务：例如图像处理、视频编码、复杂的数学计算等。\n并行处理：如果您有大量相似的任务需要同时进行处理，可以将它们分配给多个线程以并行执行。\nUI应用程序：在桌面应用程序或游戏中，多线程可以确保用户界面保持响应性，而不会被阻塞在某个耗时的操作上。\n\n需要注意的是，多线程编程更加复杂和容易引入竞态条件和死锁等问题。\nJS,Python，Rust,C++ 等语言都提供了异步编程支持(单线程) ： Async，asyncio，await\nFetch API\nfetch().then((console)⇒{})fetch() 强制接受一个参数，即要获取的资源的路径。它返回一个 [Promise](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise)，该 Promise 会在服务器使用标头响应后，兑现为该请求的 [Response](https://developer.mozilla.org/zh-CN/docs/Web/API/Response)——即使服务器的响应是 HTTP 错误状态。你也可以传一个可选的第二个参数 init（参见 [Request](https://developer.mozilla.org/zh-CN/docs/Web/API/Request)）。\nFetch relies on JavaScript promises.\nThe fetch specification differs from Ajax in the following significant ways:\n\n当接收到一个代表错误的 HTTP 状态码时，从 fetch() 返回的 Promise 不会被标记为 reject，即使响应的 HTTP 状态码是 404 或 500。相反，它会将 Promise 状态标记为 resolve（如果响应的 HTTP 状态码不在 200 - 299 的范围内，则设置 resolve 返回值的 [ok](https://developer.mozilla.org/zh-CN/docs/Web/API/Response/ok) 属性为 false），仅当网络故障时或请求被阻止时，才会标记为 reject。\nfetch 不会发送跨域 cookie，除非你使用了 credentials 的初始化选项\n\n34全阿三去A·1\n\n\n\n客户端 JavaScript 在浏览器中运行，并且浏览器的主进程是单线程事件循环。如果我们尝试在单线程事件循环中执行长时间运行的操作，则会阻止该过程。从技术上讲这是不好的，因为过程在等待操作完成时会停止处理其他事件。\n例如，alert 语句被视为浏览器中 javascript 中的阻止代码之一。如果运行 alert，则在关闭 alert 对话框窗口之前，你将无法在浏览器中进行任何交互。为了防止阻塞长时间运行的操作，我们使用了回调。\nJavaScript 被认为是单线程脚本语言。单线程是指 JavaScript 一次执行一个代码块。当 JavaScript 忙于执行一个块时，它不可能移到下一个块。\n换句话说，我们可以认为 JavaScript 代码本质上总是阻塞的。但是这种阻塞性使我们无法在某些情况下编写代码，因为在这些情况下我们没有办法在执行某些特定任务后立即得到结果。\n我谈论的任务包括以下情况：\n\n通过对某些端点进行 API 调用来获取数据。\n通过发送网络请求从远程服务器获取一些资源（例如，文本文件、图像文件、二进制文件等）。\n\n为了处理这些情况，必须编写异步代码，而回调函数是处理这些情况的一种方法。所以从本质上上说，回调函数是异步的。\n不用回调不行么，可以啊，可以一直傻等，这也符合同步的原则。代码可读性好，就是性能搓。就是不人性啊，等的捉急啊，加上JS单线程的设计，你这样是要操作一个IO其他啥都不能干了，那不玩死人。\n于是有了回调，在JS中，回调最大的好处是 调用者和回调者分开了，刚好实现这个异步事件机制\n/table\nNode.js\nNode.js 和 React 是两个不同的 JavaScript 技术，用于不同的用途和环境。下面是它们之间的主要区别：\n\n用途:\n\nNode.js: Node.js 是一个服务器端运行时环境，用于构建服务器端应用程序，如 Web 服务器、API 服务器、后端服务等。它主要用于处理服务器端逻辑，与客户端浏览器没有直接关系。\nReact: React 是一个客户端库，用于构建用户界面。它主要用于前端开发，用于构建交互性强、响应式的用户界面，通常在浏览器中执行。\n\n\n技术领域:\n\nNode.js: 主要用于服务器端开发，可以用 JavaScript 编写服务器端应用逻辑，处理请求、响应、数据库操作等。\nReact: 主要用于前端开发，帮助构建用户界面和用户体验。\n\n\n环境:\n\nNode.js: 运行于服务器端环境，可以独立运行作为服务器，处理客户端请求。\nReact: 运行于浏览器端环境，用于呈现用户界面，通常与后端服务器（可能是使用 Node.js 构建的）交互以获取数据。\n\n\n\n前端工程化的完成离不开 Node.js\nDeno 旨在为现代程序员提供高效且安全的脚本环境。Deno 是使用 rust + google v8 engine 打造的下一代 javascript  runtime\nDeno 将始终作为单个可执行文件分发。给定一个 Deno 程序的 URL，它只需约 31 兆字节的压缩可执行文件即可运行。Deno 明确承担了运行时和包管理器的角色。它使用标准的浏览器兼容协议来加载模块：URL。\n除其他外，Deno 可以很好地替代以前可能使用 Bash 或 Python 编写的实用程序脚本。\nNpm(node package manger)  npm 永久安裝，npx 安裝後即移除. npm 會全局性的安裝 create-nuxt-app，這個 dependency 也會一直存在在你的本機 node_modules 下，如果使用 npx 命令，他會安裝在臨時安裝包上，等到項目初始化完成後就刪除，不用全局性的安裝，避免被汙染\n服务端渲染样式有两种方案，它们各有优缺点：\n\n内联样式：在渲染时无需额外请求样式文件，好处是减少额外的网络请求，缺点则是会使得 HTML 体积增大，影响首屏渲染速度，相关讨论参考：#39891\n整体导出：提前烘焙 antd 组件样式为 css 文件，在页面中时引入。好处是打开任意页面时如传统 css 方案一样都会复用同一套 css 文件以命中缓存，缺点是如果页面中存在多主题，则需要额外进行烘焙\n\n","tags":["计算机基础"]},{"title":"为什么我要写日记","url":"/2024/02/11/journal1/","content":"在今年1月创建了TonyChat这个repo后，我开始自学Typescript,Node.js等前端全栈的主流技术并设计出一种最适合 最可行的技术方案，我必须思考这个产品的主要功能是什么，客户的需求是什么，怎么与开源社区交流。对工作有完全的自主性是一件好事也是坏事， 一方面我可以灵活地安排时间只要完成工作，但是我不爱计划，随机应变的性格使得我被其他娱乐活动分心。所以生产力并没有约束之下那么高。\nHere’s my personal feelings: The world is cruel, and in order to become a superior or achieve success in the secular sense, you must have a clear understanding of its operating laws and principles. Under the guise of morality and law, the real world is a jungle society where people frantically seize benefits and gain influence by deceiving and exchanging benefits, until no one can shake them and become a “dictator” within a certain range. But when your abilities are not enough to support your ambitions, you need to remain humble, be careful of your actions, and keep working.\nSeeking power isn’t bad — ruthlessness is. The ruthless pursuit of power violates a core principle of ethics: Kant’s Categorical Imperative. As Kant put it, “Act only on that maxim whereby you can, at the same time, will that it should become a universal law.” In other words, don’t do it yourself if you don’t want everyone else to follow your example. A benighted self-interest is toxic; an enlightened one is empowering.\n品质是多方面的，既有好的一面，也有坏的一面。冷酷常常令人皱眉，但如果积极利用，它可以成为一种强大的工具。坚定决心、雄心壮志和坚定不移地专注于我们的目标可以推动我们走向成功。然而，保持平衡至关重要，确保我们的野心不会掩盖我们对他人的同理心和同情心。\n所以，让我们记住，正如光明需要黑暗才能被欣赏一样，无情的积极一面可以教会我们关于坚韧、毅力和摘星的宝贵教训。通过认识并接受品质的这种双重性，我们可以利用它们的力量来过上充实而成功的生活。人们通过面对困难、黑暗的时刻，才能真正成长和学会欣赏生活中的光明。困难和挑战可以帮助个体培养坚韧、智慧和感激之情。替代真正的瓶颈解决：实验设计、统计功力、团队对齐、长期架构。\n","tags":["personal growth"]},{"title":"3000元部署开源隐私\"大\"模型 on Mac mini 16gB","url":"/2025/03/19/localLlama/","content":"对于 20B以上的模型 Mac mini 16GB 还是不太够。 芯片虽然 CPU 单核强，NPU 功耗比也不错，但真要跑 LLM，最大瓶颈不是 CPU，而是内存带宽和总 RAM。你提到的内存带宽差异，才是关键：\nM4：带宽在 120GB/s 左右，跟普通 DDR5 核显 PC 差不多，根本不是“显卡级”。\nM3 Pro：256GB/s\nM3 Max：400GB/s\nM3 Ultra：800GB/s（双芯封装）\nAI 推理 = 带宽 + 显存 + 内存 + 并发 IO\n这个带宽决定了大模型参数读取速度，尤其是多头注意力层密集访问权重时，M4 根本顶不住。而且 16GB RAM 不仅容量小，统一内存结构还容易在多进程/模型切换时抖动，尤其你测试过加载 Qwen3-Coder-30B，swap 一上，性能完全废掉。\n更别说 GGUF 模型加载时内存占用不只是模型权重，还有 KV Cache、临时缓冲、线程开销，这些加起来对 16GB 系统简直是极限拉扯。 14B 以下的量化模型（尤其是 Q4_K、Q5_K、甚至 Q2_K），在 Mac mini 上勉强可跑，但要看你容忍的响应速度和内存边界。但 3000元的价格还要什么自行车\n在大模型都想上云、绑API的时代，还有多少人愿意在家用小机器上，部署隐私友好、本地离线、完全控制的模型系统？如果你愿意折腾，一台**不到3000元的二手/教育优惠 Mac mini M4（16GB RAM）**就能跑起目前大多数开源 20B 以下的量化模型，甚至还能处理语音、图像、代码。\n我在2024年中到2025年间密集试了各种框架，从 Transformers 到 llama.cpp，再到 Apple 的 MLX + LM Studio，下面是基于实际使用体验 + 社区 benchmark 对比 + 本地推理日志的一份部署总结，适合有硬件限制但又想最大化利用现有芯片能力的开发者。 这篇文章会分享我的选型思考、用途场景，以及在折腾过程中收获的一些经验。\n\n1. 为什么选择 Mac Mini M4？\n在挑选设备时，我主要考虑了以下几个因素：\n** 1.1 性能 vs. 能耗：M4 的性价比惊喜**\n相比 Intel/AMD 服务器或台式机，Apple Silicon 的能耗比确实很强。M4 拥有10 核 CPU + 10 核 GPU + 16 核 NPU，而且功耗低到离谱（官方标称 30W）。\n在选择硬件时，我研究了多个选项，包括自建 PC、云服务器和其他品牌的小型机，最终锁定了 Mac Mini M4。以下是我的理由。\nMac Mini M4 的核心亮点是 Apple Silicon 的 M4 芯片，其统一架构和高能效比在 AI 工作负载中表现出色。以下是 M4 的一些关键参数，以及它们如何提升 RAG 工作流的效率：\nCPU 核心：\nM4 配备 8 性能核心 + 4 效率核心（或更高版本的 12P+4E），在处理检索任务（如搜索和索引）时，性能核心负责高负载计算，而效率核心处理轻量任务如后台数据整理。\nGPU 核心：\nM4 的集成 GPU 提供了 16 核或更高的图形处理能力，并支持 Apple 的 Metal API。对于运行轻量化 LLM（如量化后的 LLaMA 2）或实时生成嵌入，这种 GPU 性能足以媲美中端独立显卡。\n统一内存架构（Unified Memory Architecture, UMA）：\n16GB 或 24GB 的统一内存使得 CPU 和 GPU 共享同一块高速内存池，避免了传统架构中频繁的数据拷贝与延迟问题。这对运行大型模型和向量检索的实时计算非常重要。\nNeural Engine（神经引擎）：\n每秒 15.8 万亿次运算（TOPS） 的专用 AI 加速器可以用来提升模型推理速度，尤其是在运行量化后的 LLM 时。\n存储速度：\n内置 NVMe SSD 提供 高达 7GB/s 的读写速度，大幅缩短数据检索和模型加载的时间。\n相比之下：\n\n如果选择 AMD 9600X + 192G 内存，不仅功耗高，后期还得考虑风扇噪音、散热问题。\nEpyc 服务器方案（768GB 内存那套）虽然可以跑更大的模型，但价格和功耗都太高，不适合个人玩家。\nMac Mini M4 价格加上国补 最低 3000 RMB，省去了折腾电源、散热、机箱的麻烦，而且 macOS 的生态也适合开发者。\n使用云服务器（如 AWS、GCP）运行 RAG 系统，每月成本可能高达数百美元，而本地运行的 Mac Mini 一次性投入后，几乎没有额外费用。\n综合来看，M4 是一个 低功耗高性价比的选择，特别适合日常推理小型 LLM（&lt;14B 参数规模）。\n\n\n好，这是一个面向技术读者、强调实际测试数据、部署体验和边际性价比的博客草稿。语气偏真实用户分享，内容结构松散但信息量高：\n\n用不到3000元部署开源“类大模型”：Mac mini 16GB的极限挑战与实践总结（截至2025-08-04）\n在大模型都想上云、绑API的时代，还有多少人愿意在家用小机器上，部署隐私友好、本地离线、完全控制的模型系统？如果你愿意折腾，一台**不到3000元的二手/教育优惠 Mac mini M4（16GB RAM）**就能跑起目前大多数开源 20B 以下的量化模型，甚至还能处理语音、图像、代码。\n我在2024年中到2025年间密集试了各种框架，从 Transformers 到 llama.cpp，再到 Apple 的 MLX + LM Studio，下面是基于实际使用体验 + 社区 benchmark 对比 + 本地推理日志的一份部署总结，适合有硬件限制但又想最大化利用现有芯片能力的开发者。\n\n📦 系统与环境配置\n\n\n设备：Mac mini M4，16GB 统一内存，256GB SSD\n\n\n系统：macOS 15，Terminal + zsh，iStat Menu 监控内存\n\n\n框架：\n\nLM Studio v0.2.12 (MLX 后端，支持 MLX safetensors 4bit)\nllama.cpp（配合 llama-swap，自建模型 proxy）\nOllama+OpenWebUI（用于快速集成 + 可视化测试）\nWhisper.cpp, Bark, llava.cpp，自编译支持本地音频和图文推理\nhuggingface-cli, gguf-convert.py, mlx-community-convert 脚本\n\n\n\n内存极限在哪里？\n在 Mac mini 16GB 上，操作系统本身约占 2.5GB，即便关闭 iCloud、Spotlight、Stage Manager 等服务，理论最大模型体积也就在 11.5~12GB 左右。实践中：\n\n开启 llama-swap 可释放模型之间的 KV Cache，降低峰值内存（非常推荐）\nMLX 加载更快，但占用 GPU RAM 多，不适合同时跑多模型\n\nGGuf模型部署内存计算\n估算一个量化模型在内存里的真实占用，不能只看磁盘上 GGUF 文件的大小，还要把模型权重解压到内存、KV cache、以及 llama.cpp 运行时的各种缓冲区都算进去。最核心的三个部分是：\n首先，模型权重本身。量化后 GGUF 文件里存的就是压缩过的整数权重，但 llama.cpp 加载时会把它们解码到紧凑的张量里。对于 2 bit × group quant（Q2_K），实测内存占用约是文件大小的 1.5 倍左右——比如一个 9.3 GB 的 Q2_K 文件，加载后差不多要占 14 GB 左右。这跟社区里常说“量化模型加载要 ∼(文件大小) × (bits/8) × 解压系数”吻合，也印证了“内存需求大约是磁盘大小的 2 – 4 倍”这个经验（WizardLM–7B 14 GB 文件加载要用 ∼30 GB 内存）(Artificial Intelligence Stack Exchange)。\n其次，KV cache。语句上下文越长，需要存的 key/value 对越多。每个 Transformer 层要为 context_length（例如 4096）生成一份 KV 张量，形状是 (context_length, head_count_kv, head_size)，数字一般以 FP16 存储。以 Qwen3-Coder-30B 为例：48 层、head_count_kv=4、head_size=128、可是 embedding_length=2048，其实就是\nlayers × context × embedding_length × 2 bytes  ≈ 48 × 4096 × 2048 × 2 B  ≈ 800 MiB\n如果你开更长的 context（像 16 K），KV cache 就会线性翻倍。\n最后，运行时缓冲区和线程开销，会把总量再推高个几十到几百 MB。包括前向中间激活、多线程数据结构、stack buffer，macOS 下的 mmap 也会多占点。\n把它们合并，得出大致公式：\n总内存 ≈ 权重内存 + KV cache 内存 + 其他缓冲权重内存 ≈ GGUF 文件大小 × 解压系数（Q2_K 下大约 1.5）KV cache ≈ 层数 × context_length × embedding_length × bytes_per_value其他缓冲 ≈ 数百 MB\n举个例子：\nQwen3-Coder-30B-A3B-Instruct-UD-Q2_K_XL.gguf 文件 9.3 GB，加上约 14 GB 权重解压、∼0.8 GB KV cache、∼0.2 GB 其他，就差不多需要 15 GB 以上才能稳定跑。Mac mini 16 GB 理论上勉强够，但是系统和其他进程还要占 2 – 4 GB，经常会触发 swap，性能就很糟。\n如果你改用 Q4_0（4 bit），解压系数掉到 ∼1.2，9.3 GB 文件只要 11 GB 左右；Q8_0（8 bit）基本是 1.0 倍，就是文件大小本身。但精度和速度也跟着走。\n这样你就能根据磁盘上不同 quant 格式的文件大小，按上述系数算出大概要多少内存，再比照自己硬件的可用 RAM，选最合适的模型量化等级。\n截止 2025-08，以下模型可稳定运行\n\n\n\n模型名\n格式\n大小\nTokens/s (生成)\n备注\n\n\n\n\ngemma-3n-E4B-it\nMLX 4bit\n2.5GB\n100+\n实测最流畅\n\n\ndeepseek-coder-1.3b-instruct\nGGUF Q4_K_M\n~2.8GB\n~80\nllama.cpp 跑\n\n\nqwen1.5-7b-chat\nGGUF Q4_K_M\n~5GB\n~35\n语义表现稳定\n\n\nunsloth/Qwen3-Coder-30B-A3B-Instruct\nGGUF Q2_K_S\n~11.8GB\n~10–13\n勉强可用，内存压线\n\n\nmistral-7b-instruct-v0.3\nGGUF Q4_K_M\n~5.8GB\n~30–40\nLLM 基线\n\n\ngemma3n 8bit\nMLX 8bit\n~5GB\n~65\n稳定运行，CPU占比稍高\n\n\nopenai/whisper-large-v3\nFP16\n~5.2GB\n~1×实时音频\nwhisper.cpp 编译，表现好\n\n\n\n❌ 无法稳定运行的模型\n大于15B的模型\n\n\n\n模型名\n格式\n原因\n\n\n\n\nllama3-70b-instruct\nGGUF\n模型文件 &gt;30GB，直接 OOM\n\n\ndeepseek-coder-33b\nGGUF Q2_K\nQ2_K 文件 &gt;14GB，模型加载后即 crash\n\n\nClaude 3 系列, Mixtral 12x7B\nSharded/非 GGUF\n无法加载或格式不兼容\n\n\n\n\n\n语音 + 图像能力评估\nWhisper Large v3\n\n编译 whisper.cpp with -O3 -DCOREML -DWHISPER_COREML_FULL\n实测推理速度为 1x 实时音频速度，可本地转写播客/会议录音\n\nBark / AudioLM\n\n推理时间长，不适合 16GB 内存场景，推荐 remote inference\n\nLLaVA + MiniGPT\n\nllava-llama-2-7b-lightning.Q4_K_M 可加载，但图像转文字约需 8–10s\nCoreML GPU 没显著加速，图像处理瓶颈主要在预处理阶段（clip embedding）\n\n在考虑在Mac系统上使用LLM构建和管理个人知识基础的工具时，LM Studio MLX和Ollama都是不错的选择，但具有自己的优势。让我们分解如何在功能，用例和易用性方面进行比较。有限的定制：Ollama 的简单性是以灵活性为代价的。如果您想要更好地控制模型行为、定制或与其他系统的集成，Ollama 可能不如 LM Studio MLX 强大。\n基本功能：虽然 Ollama 非常适合简单查询和管理小型知识库，但它没有 LM Studio MLX 那么多高级功能，例如复杂的数据结构、训练或多模型集成。\nLM Studio用自己的UI带来了整个包装。在MacOS上，它也是运行MLX型号的最简单选择之一。与人们似乎想到的相反，它也可以在后台运行并成为其他应用程序的后端（OpenAi API +他们自己的API）。至于其缺陷，它是“Electron APP”，它更加臃肿（您可能不需要所有内容），并且显然是封闭的（部分是由开源技术支持）。\n\n\nOllama + GGUF 量化模型（Q4、Q6）\n\nollama pull deepseek-coder:6b（本地代码助手）\nollama pull phi3:4b（轻量推理，快速响应）\nOllama 生态丰富，内置 RAG 功能，可以快速搭建个人知识库。\n\n\n\nMLX（Apple 自研的 PyTorch 替代品）\n\nApple 提供的 MLX 框架，可以让 LLM 充分利用 Mac 的 NPU，进一步提升推理性能。\nMLX 适合需要自己训练或者微调（fine-tune）小模型的用户，性能比 torch-metal 更稳定。\n\n\n\n\n** 1.3 价格 vs. 扩展性：最划算的 macOS 设备**\n买 Mac Mini M4 有几个隐藏的优点：\n\n\n比 Mac Studio / MacBook Pro 便宜得多\n\nMac Studio（M2 Ultra）起步价 20,000+，完全不划算。\nMacBook Pro 14/16 英寸价格高，但性能和 Mini 差不多。\n\n\n\n内存和存储可以外接扩展\n\n连接 Thunderbolt SSD，可以扩展大容量存储，不用买官方超贵的 SSD 版本。\n64GB 内存的版本确实更贵，但考虑到 macOS 的 Swap 机制，16GB 也能凑合。\n\n\n\nmacOS 生态对开发者友好\n\n自带 Unix 环境，比 Windows 好用（M4 也支持 Asahi Linux）。\nPython + Homebrew + Rust + Ollama 一条龙支持，很适合 LLM 和 AI 开发。\n\n\n\n综合来看，Mac Mini M4 在性能、价格、可扩展性之间找到了最优解，比 iMac、MacBook 更合适做 LLM 部署机。\n\n2. Mac Mini M4 在我的 AI 工作流中的作用\n买了 Mac Mini M4 之后，我主要用它来做 本地 AI 助手 + 个人知识库，具体用途包括：\n** 2.1 个人知识库（Local RAG）**\n\n通过 Ollama + LangChain 搭建**本地 RAG（检索增强生成）**系统。\n数据存储在 ChromaDB 或 Qdrant 里，配合 Mistral 7B 或 Phi-3 进行查询。\n实现一个离线可用的 AI 助手，不依赖 OpenAI API。\n\n** 2.2 本地代码助手**\n\n用 Deepseek Coder 作为 离线代码补全助手，在 VS Code 里运行 ollama run deepseek-coder:6b。\n结合 Copilot，提高代码写作效率，适合离线环境或数据隐私要求高的项目。\n\n** 2.3 轻量级 AI 训练**\n\n主要用 MLX 跑一些小模型的微调（Fine-tune）。\nMLX 可以充分利用 Mac 的 Neural Engine（NPU），比 CPU-only 方案快很多。\n\n\n3. 折腾的收获\n以 DeepSeek-R1-0528-Qwen3-8B-GGUF为例， 这个模型的多个量化版本（比如 Q4_K_XL、Q5_K_M、Q6_K 等），该如何选择最合适的一个用于部署在 ollama 或其他本地推理环境上。下面从几个关键维度来帮你分析选型。\n这些 Q4_K_M、Q5_K_S、Q6_K_XL 是 GGUF 格式模型的量化配置，代表不同精度和内存占用。数字代表位数（4bit、5bit、6bit…），K 表示使用 k-bit quantization 的算法变种（目前最佳实践），后缀 M/S/XL 表示精度差异与模型量化细节。\nQ4_0, Q4_1: 最基础的 4bit 量化，速度快，占用小，但准确率下降明显。\nQ4_K_M, Q4_K_XL: 属于 k-quant 家族中优化过的 4bit 版本，在精度和性能间折中较好。\nQ5_K_, Q6_K_: 精度更高，占用更多资源，适合需要较高准确度的任务。\nQ8_0, Q8_K_XL: 几乎是 FP16 级别的精度，占用大，推理慢但准确率接近原始模型。\n硬件资源约束（Mac mini M4 16GB）\n你在 Mac mini M4 上跑本地模型（假设无 GPU，只用 CPU + NE），实际能用的 RAM 不到 13GB（系统和应用会吃掉一部分）。所以这就限制了你最多只能加载到 Q6_K_XL 甚至 Q6_K，而 Q8_K_XL 体积 &gt;10GB，会非常吃内存和 swap，严重拖慢响应。\n\n\n\n版本名\n大小\n是否推荐（Mac mini 16GB）\n\n\n\n\nQ4_0\n4.79GB\n✅ 快速测试/微交互型任务\n\n\nQ4_K_M\n5.03GB\n✅ 推荐初始部署\n\n\nQ4_K_XL\n5.12GB\n✅ 平衡好性能和准确率\n\n\nQ5_K_S\n5.72GB\n✅ 若你追求更高准确率\n\n\nQ6_K_XL\n7.49GB\n⚠️ 临界，容易爆内存\n\n\nQ8_K_XL\n10.8GB\n❌ 可能无法加载或太慢\n\n\n\n这段时间折腾下来，我有几个感悟：\n\n\nMac 确实适合本地推理，但不适合训练大模型。\n\nMac Mini M4 在 7B LLM 及以下的推理 体验很好，但要跑更大的模型（&gt;14B），还是得上 PC + 高内存方案。\n微调（Fine-tune）虽然可以用 MLX，但远不如 Linux + A100/3090 服务器 来得高效。\n\n\n\nOllama 的体验远超想象，但下载速度需要优化。\n\nollama pull 速度后期骤降，可能是服务器/CDN 限流，需要手动优化下载策略（见我的 pull -&gt; stop -&gt; sleep 方案）。\n\n\n\n优化 Bash 版本（适用于 Linux/macOS）\n#!/bin/bashMODEL_NAME=\"deepseek-coder-v2:16b\"MAX_ATTEMPTS=10  # 最大尝试次数TIMEOUT=120      # 每次下载时间（秒）SLEEP_TIME=3     # 停止后等待时间trap \"echo 'Stopping...'; exit 1\" SIGINT SIGTERMattempt=1while [ $attempt -le $MAX_ATTEMPTS ]; do    # 检查是否已下载    if ollama list | awk '{print $1}' | grep -q \"^$MODEL_NAME$\"; then        echo \" Model $MODEL_NAME is fully downloaded.\"        exit 0    fi    echo \"⏳ Attempt $attempt: Pulling model $MODEL_NAME...\"        # 开始下载    ollama pull \"$MODEL_NAME\" &amp;    PID=$!    sleep $TIMEOUT    # 停止进程    echo \"⏹️ Stopping download...\"    kill $PID 2&gt;/dev/null    sleep $SLEEP_TIME    attempt=$((attempt+1))doneecho \"⚠️ Model download incomplete after $MAX_ATTEMPTS attempts. Please check manually.\"exit 1\n\nWindows PowerShell 版本\n$MODEL_NAME = \"deepseek-coder-v2:16b\"$MAX_ATTEMPTS = 10$TIMEOUT = 120$SLEEP_TIME = 3$attempt = 1while ($attempt -le $MAX_ATTEMPTS) {    # 检查是否已下载    $modelExists = ollama list | Select-String $MODEL_NAME    if ($modelExists) {        Write-Host \" Model $MODEL_NAME is fully downloaded.\"        exit 0    }    Write-Host \"⏳ Attempt $attempt: Pulling model $MODEL_NAME...\"    # 启动下载    Start-Process -NoNewWindow -FilePath \"ollama.exe\" -ArgumentList \"pull $MODEL_NAME\"    Start-Sleep -Seconds $TIMEOUT    # 终止下载    Write-Host \"⏹️ Stopping download...\"    Stop-Process -Name \"ollama\" -Force -ErrorAction SilentlyContinue    Start-Sleep -Seconds $SLEEP_TIME    $attempt++}Write-Host \"⚠️ Model download incomplete after $MAX_ATTEMPTS attempts. Please check manually.\"exit 1\n\n本地 LLM 体验很棒，可以完全替代部分 ChatGPT 使用场景。\n\n\nMac Mini M4 未来的价值很大。\n\nApple 的 Neural Engine（NPU） 越来越强，未来 M4 Pro、M5 可能会更适合 AI 部署。\n本地 AI+RAG 是趋势，个人设备上的 LLM 会变得越来越普及。\n\n\n\n\n\n🛠️ 实用技巧 &amp; 推荐社区\n\n善用 HuggingFace, catalyst, mlx-community 模型仓库，第一时间能获取最新 GGUF 或 MLX 转换版本。\n加入 r/LocalLLaMA 和 Discord 群体，跟踪模型发布节奏与 benchmark，对实际能跑的模型一目了然。\n多关注 TheBloke, Undi95, unsloth, ggerganov, mlx-examples 仓库，90% 的量化模型都从这些 repo 起源。\n利用 llama-cpp-server + llama-swap 自建 API proxy，可复用模型并避免 LM Studio 多模型冲突。\n💡 适合：你对 macOS/Linux 配置熟悉，只需要 1-2 个服务\n如果你只想跑 Open WebUI 或 AnythingLLM，可以手动安装依赖，比如：\n\nsh\n复制\n编辑\n安装 Node.js\nbrew install node\n安装 Open WebUI\ngit clone https://github.com/open-webui/open-webui.git\ncd open-webui\nnpm install\nnpm run dev\n📌 优势：\n不需要 Docker，减少资源占用\n可以自己控制环境，避免 Docker 额外的磁盘消耗\n4. 结论：Mac Mini M4，最具性价比的本地 AI 设备\n如果你的需求是 跑本地 AI 助手、LLM 推理、个人知识库，Mac Mini M4 绝对是目前最具性价比的 Apple 设备。\n📌 适合人群：\n想折腾本地 AI，体验 Ollama / MLX\n需要一个低功耗、高性价比的 macOS 设备\n主要跑 7B 以内 LLM，或做小型 RAG / 本地 AI 应用\n💡 不适合：\n❌ 训练大模型（建议上 Linux + 3090/A100）\n❌ 需要 64GB 以上内存（M4 Mini 最高只支持 36GB）\n目前来看，Mac Mini M4 是 低成本个人 LLM 部署最划算的选择，比服务器或高端 Mac 方案都更适合折腾！\n","tags":["LLM"]},{"title":"Learn Rust","url":"/2025/04/08/rust/","content":"Recently, I applied for a project in Tor organization, which aims to reimplement the** metrics-lib **in Rust, maintaining the same functionality while leveraging Rust’s advantages in terms of performance, memory safety, and concurrency. This aligns with the Tor Project’s broader initiative to modernize the metrics pipeline.**The goal of this project is to rewrite the current Java-based Tor Metrics Library in Rust. This library is a crucial component for parsing and analyzing Tor network descriptors used by various services and researchers. By reimplementing the library in Rust, we aim to improve performance, take advantage of Rust’s modern language features, and provide a more ergonomic API. This rewrite will also allow the Network Health Team (and others) to migrate some of their services to Rust. In addition to replicating current features, the new library will include some features from the DescriptorParser for exporting parsed descriptor data into formats like CSV, Parquet, or PostgreSQL tables, thereby enabling more advanced integrations and analysis capabilities.\nThe rust rewrite should provide the same parsing and validation functionalities provided by metrics-lib and in addition allow exporting of the documents in some external storage, like parquet files to be saved into object storage or a table on a postgresql database.\nTo modernize and improve maintainability, performance, and safety, this proposal outlines a full re-implementation of the library in Rust, a systems programming language that offers memory safety, zero-cost abstractions, and better integration with modern data pipelines. Such as:\n\nUse Rust’s ownership system for safe memory management\nImplement zero-copy parsing where possible\n\n\n\n\nLayer\nTech Stack\n\n\n\n\nLanguage\nRust\n\n\nNetwork IO\nreqwest, hyper, tokio\n\n\nParsing\nnom, pest\n\n\nStorage Output\nparquet, serde, postgres\n\n\nTesting &amp; CI/CD\ncargo test, GitLab CI/CD\n\n\nDocs &amp; Linting\nrustdoc, clippy, rustfmt\n\n\n\n2.1 Original structure\nThe library has evolved significantly from version 1.0.0 to 2.26.0, over the past 10 years.\nKey features include:\n\nData Sources:\n\nThe library primarily targets data archived by CollecTor (collector.torproject.org).\nThis data is typically stored in compressed tar archives (.tar.xz, .tar.gz, .tar) containing numerous individual data files.\nIt can also process individual, unarchived data files.\n\n\nSupported Data Types:metrics-lib can parse several distinct types of Tor network measurement data:\n\nRelay Descriptors:\n\nserver-descriptors: Detailed information published by relays about themselves (IP, ports, keys, bandwidth, policies, etc.). Parsed into RelayServerDescriptor.\nextra-info-descriptors: Additional information published by relays (bandwidth history, geoip data, etc.). Parsed into RelayExtraInfoDescriptor.\nmicrodescriptors: Smaller, more frequently updated versions of relay information used for client bootstrapping. Parsed into RelayMicrodescriptor.\n\n\nNetwork Status Consensus Documents:\n\nThese documents represent a snapshot of the Tor network state at a specific time, as agreed upon by the Directory Authorities.\nmicrodesc-consensus: Contains references to microdescriptors, relay flags, bandwidth weights, etc. Parsed into RelayMicrodescriptorConsensus. Contains NetworkStatusEntry objects for each relay listed.\nrelay-consensus: (Less common now, but historically used) Similar but based on server descriptors. Parsed into RelayServerDescriptorConsensus.\n\n\nExit Lists:\n\nSnapshots of relays identified as Exit nodes at a particular time, often generated by services like TorDNSEL. Parsed into ExitList. Contains ExitNodeEntry objects.\n\n\nBridge Data:\n\nSimilar data types but specifically for Tor Bridges (unlisted relays).\nbridge-server-descriptors: Parsed into BridgeServerDescriptor.\nbridge-extra-info-descriptors: Parsed into BridgeExtraInfoDescriptor.\nbridge-statuses: Analogous to consensus documents but for bridges, generated by the Bridge Authority. Parsed into BridgeAuthoritativeStatus.\n\n\n\n\nCore Design Pattern: Readers and Iterators:\n\nThe library uses a Reader pattern for each major data category (e.g., DescriptorReader, ConsensusReader, ExitListReader, BridgeDescriptorReader, BridgeStatusReader).\nYou instantiate a Reader, providing it the path to an archive file (e.g., .tar.xz) or a single data file.\nThe Reader acts as an iterator. You loop over the reader (e.g., for desc in DescriptorReader(path):), and it yields parsed data objects one by one.\nThis is efficient as it typically doesn’t load the entire archive or all data into memory at once (it likely streams through the archive).\n\n\nData Representation: Structured Objects:\n\nWhen the Reader yields an item, it’s a object (like RelayServerDescriptor, NetworkStatusEntry, ExitNodeEntry).\nThese objects have attributes corresponding to the fields parsed from the raw data files (e.g., desc.nickname, desc.fingerprint, consensus.valid_after_time, relay_status.flags, exit_node.exit_addresses).\n\n\nKey Features &amp; Benefits:\n\nAbstraction: Hides file format details (descriptor syntax, consensus structure, tarball handling).\nAutomation: Easily process large numbers of files within archives.\nStructured Data: Provides convenient object-oriented access to data fields.\nType Dispatching: The DescriptorReader automatically identifies the type of descriptor (@type annotation in the file) and returns the appropriate object type (RelayServerDescriptor, RelayExtraInfoDescriptor, etc.).\nError Handling: Includes mechanisms (like InvalidDescriptor exceptions) to handle malformed or unparseable files gracefully.\n\n\n\n","tags":["编程"]},{"title":"Coding philosophy","url":"/2024/10/10/notion/","content":"Coding philosophy\n我们编写的每一行代码都讲述一个故事。它反映了我们的思维过程、解决问题的方法以及我们对手头任务的理解。但除了功能之外，代码还体现了一种哲学——指导我们的决策并塑造我们创建的软件的一组原则。This blog delves into the fascinating world of coding philosophies. We’ll explore the diverse approaches developers take, from prioritizing elegance and simplicity to embracing pragmatism and efficiency. We’ll discuss the trade-offs inherent in each philosophy and how they influence everything from code readability and maintainability to project timelines and team dynamics.\n水平差的程序员往往对代码 “不讲究”，这其实是工程素养差的表现。\n那不讲究的代码是什么样的呢？比如，同样含义的变量使用不同的名字、对偶功能（如增删改查）命名不成体系、相似代码不断重复而没有重用、反常逻辑没有任何注释、子模块间头重脚轻、好好的封装被改出几个洞等等（关于如何命名可参考我之前的一篇文章）\n生命周期。生命周期越长的代码，一定要写的越干净；临时使用代码，比如小脚本，就可以不讲究一些。反过来，也正是干净的代码才能成就超长的生命周期。\n\n\n不要贬低你的工作\n因为是新人，新手程序员总是倾向于认为他们的工作没那么重要。又或者也许你是一个有经验的程序员，但是在一个让你感到不适应的新领域里工作。在我看来，一些最好的想法正是来自于新手程序员，他们能看到现有技术的可改进之处，而那些已经形成固有观念的人却看不到。\n无论如何，你的工作都是有价值的。在最坏的情况下，如果你的方法失败了，社区至少可以更好的了解为何这种方法行不通。（对社区的一点说明：这条是我们要做的，要对新人友好一些。）\n\n\n不用总是在压力下工作\n新技术每天都会出现，这可能会让你觉得如果放慢脚步，就会与这个世界脱节。然而并不是这样的。事实上，如果你能很好的休息，你就能更好的工作。你的思路将保持清晰，我发现当我不工作时，我的潜意识里会出现很多新想法。\n那些每天不断发布的内容大部分都是一些现有想法的翻版。真正革命性的东西只会每几年才发生一次。关于这个话题有一个不错的分享值得一看：吊床驱动开发\n\n\nSystem design\n技术创新与生产关系变革联系起来，并探讨了软件和运维架构背后的组织结构。这种观点可以为我们更好地理解技术变革的动力提供新的视角。让我们进一步探讨一下这些观点及其背后的逻辑。\n技术创新与生产关系的变革\n容器和容器编排的兴起：\n\n背景： 传统的虚拟机管理技术在面对大规模服务器时显得笨重和低效。\n需求： Google 等大型互联网公司需要一种更高效的管理方式，以便少数人能够管理数十万台服务器。\n解决方案： Kubernetes 的诞生使得容器编排成为可能，大大提高了资源利用率和管理效率。\n\n这一变化不仅仅是技术上的创新，更是生产关系的变革。容器技术和容器编排的兴起，使得运维团队能够更加高效地协作和管理庞大的计算资源，实现了真正的自动化和规模化运维。\n软件架构与团队组织架构\n软件架构反映团队组织（康威定律）：\n你提到了软件架构设计受到团队组织结构的影响，这正是康威定律的核心思想：\n“组织架构决定了系统架构。”（Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations.）\n这种架构的最大价值在于，它不仅仅是技术上的进步，更是对团队组织形式的一种优化，从而提升了整体的软件质量和开发效率。\n宏内核与微内核的争论\n操作系统内核设计的选择：\n\n宏内核： 由一个庞大的工程团队开发和维护，功能完备，但相对复杂。\n微内核： 由于资源有限，只专注于核心功能，其他功能由用户态软件实现。\n\n这种选择背后实际上是组织协作方式的不同。大型公司可以投入大量资源开发复杂的宏内核，而开源项目则更倾向于开发微内核，通过社区协作来实现功能扩展。\n实例分析\nIPVS、cgroups、eBPF 等技术：\n\n这些技术最初在用户态实现，但随着性能和功能需求的增加，逐渐进入了内核态。\n这种迁移反映了对性能优化和功能需求的不断追求，同时也展示了技术发展过程中组织和协作方式的演变。\n\n","categories":["感想"],"tags":["personal growth"]},{"title":"美剧硅谷的middle-out algorithm 压缩算法","url":"/2025/02/06/shannon/","content":"在当今数据驱动与互联网高速发展的时代，概率论、数理统计以及信息论构成了理解和应对不确定性、优化数据传输以及实现高效压缩算法的数学基础。这三大领域不仅为金融风险管理、临床试验设计、机器学习等提供了理论支持，同时也奠定了现代通信与数据压缩技术的基石。\n最近在观看美剧 Silicon Valley 时，我深有感触：剧中展示的技术——例如 Pied Piper 团队所研究的压缩算法，虽然戏剧化处理，但实际上反映了现实世界中信息理论的精髓，比如香农极限（Shannon Limit，也称为信息熵极限）。这使我联想到自己曾经研究过的信息论相关内容，尤其是自信息量、无损压缩以及带噪信道中的通信极限等问题。\nSo, 美剧硅谷中的The middle-out algorithm 压缩算法真能实现吗？我们或许需要一些背景知识\n概率论\n概率论是研究随机现象规律的数学分支，主要涉及概率、随机变量、概率分布等基本概念。它帮助我们量化随机事件发生的可能性。例如，正态分布、泊松分布和指数分布等模型被广泛用于描述现实世界中的不确定现象。最初源于17世纪的赌博问题，帕斯卡和费马等人的工作为这一学科奠定了基础。\n数理统计\n数理统计则是应用概率论来收集、分析、解释数据，并进行推断和决策的学科。它包含了抽样理论、参数估计、假设检验、回归分析等方法，帮助我们从数据中提取信息，并对总体情况进行科学推断。高斯、费舍尔等人在19世纪末到20世纪初的工作，使得数理统计逐步形成了完整的理论体系。\n统计方法（例如假设检验、回归分析、抽样技术）使我们能够从大量数据中提取有价值的信息，支撑科学决策与实验设计。\n信息论\n信息论是20世纪中叶由香农创立的学科，主要研究信息的量化、存储和传输。其核心概念包括熵（衡量不确定性）、自信息量（衡量单个事件所携带的信息大小）以及互信息（衡量两个变量之间的信息共享）。香农提出的香农极限（Shannon Limit）揭示了在带噪信道中实现无误差通信的理论上限，这一理论不仅对通信工程产生深远影响，也为数据压缩、密码学、机器学习等领域提供了理论依据。\n概率论、数理统计与信息论之间的联系\n\n\n理论基础\n概率论为数理统计提供了坚实的理论基础。统计学中的许多方法，例如假设检验、参数估计和回归分析，都依赖于概率模型。没有概率论的严谨定义和计算方法，统计推断就无从谈起。\n\n\n信息论的依托\n信息论中的熵、互信息等概念同样是建立在概率分布的基础上。例如，一个离散随机变量 (X) 的熵定义为\n\n这不仅衡量了 (X) 的不确定性，也是无损压缩理论的理论下界。最大熵原理在统计建模和模型选择中也有广泛应用，它主张在满足已知条件下，选择熵最大的分布以避免引入不必要的假设。\n带噪信道通信：香农极限揭示了在给定带宽和信噪比下，信道的最大信息传输速率。设计高效的信道编码（如 LDPC 码）可以帮助系统接近这一极限，从而在5G、卫星通信等领域实现无误差传输（Shannon, 1948）。\n\n\n实际应用中的融合\n\n数据压缩与通信：在数据压缩中，我们追求尽可能接近信息熵下界的编码效率。理想情况下，无损压缩的平均码长应接近源信息熵。而在带噪信道通信中，香农极限为设计信道编码提供了理论上限，指导我们在有限带宽和信噪比下实现尽可能高的传输速率。\n数据压缩能够实现是因为多数现实世界的数据都有统计冗余。例如，字母“e”在英语中比字母“z”更加常用，字母“q”后面是“z”的可能性非常小。无损数据压缩通常利用了统计冗余，这样就能更加简练地、但仍然是完整地表示发送方的数据。真正随机的数据是不可能通过压缩来减小文件大小的。在这种情况下，数据没有可预测的模式或重复性,每个数据点都是独立的。\n\n\n\n数据压缩\n1. 有损压缩（Lossy Compression）:\n有损压缩是一种方法，其中一些数据被故意丢失，以减小文件的大小。这种方法通常用于图像、音频和视频数据，其中一些信息损失后人的感知不易察觉。\n原理：\n\n去除冗余信息： 有损压缩算法检测并删除不太重要的数据，如音频中的高频噪音或图像中的微小细节。\n量化： 在音频和视频压缩中，数值可以被量化为较低的精度，从而减小数据量。\n子采样和色度抽样： 在图像和视频压缩中，可以减小像素数量或颜色信息的精度。\n离散余弦变换（DCT）： 在图像和音频压缩中，DCT 可将数据转换为频域表示，以减少高频成分。\n\n2. 无损压缩（Lossless Compression）:\n无损压缩是一种方法，其中压缩后的数据可以完全还原为原始数据，不会丢失任何信息。这种方法适用于需要维护数据完整性的情况，如文档、数据库和程序文件。\n原理：\n\n重复数据删除： 无损压缩算法通常检测和删除数据中的重复内容，例如相邻的相同字符或字符串。\n字典编码： 该方法使用字典或字典表，将常见的字符串存储为字典条目，然后用较短的引用替换原始数据。\n霍夫曼编码： 霍夫曼编码是一种变长编码技术，将常见字符映射到较短的编码，不常见字符映射到较长的编码。\n算术编码： 算术编码使用概率模型来将字符序列转换为一个大的数字，以减小数据的大小。\n压缩算法的基本流程\n目前成熟的无损压缩算法一般分为两大步骤：\n\n\n建模：利用统计模型（如概率分布、字典构建、上下文模型）对数据进行建模。\n编码：利用熵编码（如 Huffman 编码、算术编码或现代的上下文自适应二进制算术编码）实现数据压缩。\n\nmiddle-out 的概念解析\n“Middle-out”这一命名暗示了算法在数据处理过程中可能采取了一种不同于传统自左至右或者自右至左的策略，而是从数据的“中间”开始处理，然后向两端扩展。可能的假设包括：\n\n数据分割与局部统计：如果数据在中间部分具有更明显的冗余或统计规律，先对中间部分进行压缩再向外扩展，可能在某些特定数据结构中取得更好的局部压缩效果。\n并行处理：从数据中心向两端分割可能利于并行处理，在多核系统上提高压缩速度，但这主要针对速度优化而非压缩比的根本提升。\n动态字典更新：一种设想是算法在“中间”构建核心字典或模型，然后将该字典扩展到整个数据中，利用全局与局部信息的结合来提升压缩效率。但这种方法本质上仍受限于数据的统计特性和信息熵。\n但是\n数据依赖性：大多数数据文件的冗余分布并非在中间部分集中，而是与数据结构、文件格式密切相关。针对某一类数据进行“中间优先”的处理可能适用于某些特定场景，但难以普适。\n算法复杂度：若需要在中间先构建全局统计模型，再向两侧扩展，算法设计和实现上的复杂性会大幅增加，而且实际收益可能不明显。\n无损数据压缩的压缩率不足以处理庞大体积的音视频数据，但如果允许一定程度的保真度损失，那么还可以实现进一步的压缩。\n在无损压缩中，我们希望将数据编码为尽可能短的比特串。根据香农的第一定理，一个符号序列的最优编码长度下界正是其信息熵  \n然而，经常有一些文件不能被有损数据压缩压缩，实际上对于不含可以辨别样式的数据任何压缩算法都不能压缩。另外，试图压缩已经经过压缩的数据通常得到的结果实际上是增加数据。\n\n实际上，有损数据压缩也会最终达到不能工作的地步。例如一个极端的例子：压缩算法每次去掉文件最后一个字节，那么经过这个算法不断的压缩直至文件变空，压缩算法将不能继续工作。\n由于可以帮助减少如硬盘空间与连接带宽这样的昂贵资源的消耗，所以压缩非常重要，然而压缩需要消耗信息处理资源，这也可能是费用昂贵的。所以数据压缩机制的设计需要在压缩能力、失真度、所需计算资源以及其它需要考虑的不同因素之间进行折衷。\n对于任何形式的通信来说，只有当信息的发送方和接受方都能够理解编码机制的时候压缩数据通信才能够工作。例如，只有当接受方知道这篇文章需要用汉语字符解释的时候这篇文章才有意义。同样，只有当接受方知道编码方法的时候他才能够理解压缩数据。\n计算表示“hello world”这个字符串所需的信息熵，我们需要分析字符串中每个字符的概率分布,  将英文字母及符号看作信息的基本单位, 那么这个字符串中出现了’h’,’e’,’l’,’o’,’ ‘,’w’,’r’,’d’,8个字符其出现频率\n比特字符\n字符串长度为 11 个字符，因此总信息熵为：\n比特\n通信双方达成共识\n为了表示“hello world”这个字符串，通信双方需要：\n\n\n字符集共识：双方需约定使用的字符集（如 ASCII 或 Unicode）。\n\n\n编码方式共识：双方需约定编码方式（如 UTF-8、霍夫曼编码）。\n\n\n信息熵共识：双方需理解字符串的熵值（约 2.842 比特/字符），以确定最小传输资源需求。\n\n统计推断与机器学习：现代机器学习模型（例如贝叶斯网络、朴素贝叶斯分类器）都深受概率论和统计学的启发，而交叉验证、A/B 测试等统计方法则保证了这些模型在实际应用中的鲁棒性和准确性。\n\n\n\n发展历程\n\n概率论：最早起源于17世纪的赌博问题，由帕斯卡、费马等人提出初步理论，随后随着数学的发展不断完善。\n数理统计：在19世纪末到20世纪初，高斯、费舍尔等人的工作使统计学逐步形成体系，应用范围也从农业试验扩展到医学、工程等多个领域。\n信息论：由香农在1948年提出，其原始目标是解决通信领域中信息传输的效率问题。随着时间的推移，信息论的应用范围不断扩大，如今已深入到数据压缩、密码学、机器学习、神经科学等众多领域。\n\n另外，随机过程与时间序列分析中的马尔可夫链、布朗运动等理论在金融统计中的应用也日益广泛，它们帮助我们建模和预测股市波动、汇率变化等动态系统的行为。\n应用案例：从理论到实践\n金融与风险管理\n现实世界充满了随机现象，例如股市波动、气候变化和疾病传播。概率论为我们提供了一套系统的方法去量化和描述这些不确定性，使我们能够对未来进行预测并做出决策。通过概率模型和统计方法，我们可以评估潜在风险。例如，在金融领域，金融机构利用VaR（风险价值）模型对资产组合进行风险管理，从而制定更为稳健的投资策略。\n金融市场充满随机波动。利用概率分布（如正态分布、t分布）和 Monte Carlo 模拟，金融工程师可以对资产收益进行建模，评估投资组合的风险。例如，2008年金融危机期间，尾部风险的低估促使研究者重新审视和改进风险模型（Embrechts et al., 2001）。\n医学与生物统计\n在新药研发过程中，临床试验必须设计得科学严谨。统计方法如 t 检验、卡方检验以及抽样方法确保了试验结果的可靠性，并帮助研究者在有限样本中提取信息。FDA 审查报告中明确指出，统计效能（power）常被设置为至少80%以确保试验的有效性（Friedman et al., 2010）。\n机器学习与人工智能\n现代机器学习算法广泛应用于图像识别、自然语言处理等领域。基于概率图模型、朴素贝叶斯分类器的理论支持，结合交叉验证和 A/B 测试等统计方法，深度学习模型在 ImageNet 挑战赛中取得了95%以上的识别准确率（Goodfellow et al., 2016）。\n信息论在数据压缩与通信中的应用\n\n无损压缩：无损压缩算法（如 Huffman 编码、算术编码）利用自信息量和熵的理论设计，力图将数据压缩到接近其理论下界。剧中 Pied Piper 的压缩算法虽然经过戏剧化处理，但实际上反映了真实世界中对压缩算法效率追求的热情和技术挑战。\n带噪信道通信：香农极限揭示了在给定带宽和信噪比下，信道的最大信息传输速率。设计高效的信道编码（如 LDPC 码）可以帮助系统接近这一极限，从而在5G、卫星通信等领域实现无误差传输（Shannon, 1948）。\n\n概率论、数理统计与信息论不仅为我们提供了描述随机现象和信息传输的理论基础，更在金融、医学、机器学习以及通信等多个领域中得到了广泛应用。通过对自信息量、熵以及香农极限等核心概念的学习，我们不仅能够理解数据的内在结构，还能设计出高效的数据压缩算法和抗噪通信系统。正如美剧 Silicon Valley 中对技术的戏剧化演绎所展示的那样，作为年轻的 SDE 和创业者，深入掌握这些学科的原理，将为未来的创新与技术发展提供无限可能。\nSo, 美剧硅谷中的The middle-out algorithm 压缩算法真能实现吗？\n文件能压缩的程度受信息量的限制，基本上不太可能出现一个无损压缩算法超过香农极限，而且middleout之后相当于使用分治法他们可能会继续将两半分成一半\n事实上Dropbox正在研究一种实际且相当革命性的中间压缩算法。 https://blogs.dropbox.com/tech/2016/07/lepton-image-compression-saving-22-losslessly-from-images-images-at-15mbs/\n参考文献\n\nEmbrechts, P., Klüppelberg, C., &amp; Mikosch, T. (2001). Modelling Extremal Events for Insurance and Finance. Springer.\nFriedman, L. M., Furberg, C., DeMets, D. L., Reboussin, D. M., &amp; Granger, C. B. (2010). Fundamentals of Clinical Trials. Springer.\nGoodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.\nShannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal, 27(3), 379–423.\n\n","categories":["随谈"],"tags":["概率论","信息论","算法"]},{"title":"网络安全实践","url":"/2025/01/06/security/","content":"Web Security\n在广阔的互联网领域，信息源源不断，交易瞬息万变，一场无声的战斗正在上演。网络威胁潜伏在阴影中，试图利用漏洞并破坏我们建造的数字堡垒。这就是网络安全的领域，一个动态且不断发展的领域，它是用户与渗透到网络世界的无数风险之间的守护者。\nConsider this: Beyond the visible web lies an untamed and uncharted area akin to the Wild West, where data bandits and cyber outlaws reign supreme. These ne’er-do-wells constantly innovate their nefarious methods to breach firewalls, hijack sessions, and steal sensitive data. Web security is the marshal that stands in their way, brandishing the latest cryptographic shields and a sharp strategy to enforce the law of the land.\n**对称加密 (DES, 3DES, AES)：如果要加密一串消息，很自然的想法是，加密和解密用的同一个密码。如同现实生活中，一把钥匙既可以锁上一把锁，也可以打开一把锁，这就是密码学中的对称加密 (Symmetric Encryption)：加密和解密用的是同一串密钥 (Secret Key)。实际应用中的数据加密，通常都是使用的对称加密，因为**对称加密算法很多对硬件特别友好，所以在硬件加密模块上运行效率非常高。\n非对称加密 (RSA), 非对称加密是为了解决对称加密无法解决的问题。例如，怎么才能保证使用密钥的人是可信的呢？\n公钥加密私钥解，私钥签名公钥验。\n\n\n公钥加密，私钥解密：\n这是非对称加密的基本应用场景。当A想向B发送加密信息时，A使用B的公钥对信息进行加密。由于公钥可以公开，A无需担心公钥的安全性。B收到加密信息后，使用自己的私钥进行解密，因为只有B持有对应的私钥，所以只有B能够解密并读取信息内容。这一过程确保了信息的机密性。\n\n\n私钥签名，公钥验证：\n数字签名技术用于验证信息的完整性和发送方的身份。当A发送信息给B时，A使用自己的私钥对信息的摘要进行加密，生成数字签名。B收到信息和签名后，使用A的公钥对签名进行解密，验证签名的正确性。如果验证通过，B可以确信信息没有被篡改，且确实来自持有对应私钥的A，增强了信息的完整性和不可抵赖性。\n\n\n消息认证码系统由三种算法组成：\n\n\n密钥生成算法从密钥空间中均匀随机地选择一个密钥。\n\n\n签名算法根据密钥和消息有效地返回标签。\n\n\n验证算法在给定相同密钥和标签的情况下，可以高效地验证消息的真实性。也就是说，当消息和标签未被篡改或伪造时，返回接受，否则返回拒绝\n。\n\n\n安全的消息认证代码必须能够抵御攻击者伪造任意、选定或所有消息的标签的企图，包括在已知消息或选定消息的情况下。在不知道密钥的情况下，计算给定消\nMITM\n以下是MITM攻击的一些常见形式：\n\n窃听（Eavesdropping）：\n中间人通过监听网络通信，获取传输的敏感信息，如用户名、密码等。\n数据篡改（Data Modification）：\n中间人可以修改传输的数据，导致接收方收到被篡改过的信息，可能会对系统产生负面影响。\n欺骗（Spoofing）：\n攻击者可以伪装成合法的通信方，使得通信的两端都认为他们正在与合法的对方通信。\n中继攻击（Relay Attack）：\n攻击者在通信的两端之间传递信息，以模拟两者直接通信，同时记录或修改信息。\nSSL Stripping：\n中间人可能试图降级加密通信，将加密连接转换为非加密连接，以便更容易地窃听敏感信息。\n\nMITM攻击可以在各种通信协议中发生，包括HTTP、HTTPS、SMTP等。防范MITM攻击的方法包括使用加密通信（如HTTPS）、数字签名、使用安全的认证机制等。使用公共Wi-Fi网络时，特别需要注意MITM攻击，因为这样的网络更容易受到攻击者的窃听。\nHacker劫持user DNS解析到黑客的服务器之后，如果黑客服务器上没有合法签发的SSL证书，浏览器会提示这个DNS指向的服务器不可信任，然后弹出警告画面阻止你访问，所以要劫持DNS+合法证书两个问题同时解决才算完美的中间人攻击。用户电脑上的证书大部分由操作系统颁发，可以离线信任链\n单独说DH协商算法不能防止中间人没有意义。因为密钥协商算法要解决的是在不可靠链路上安全地协商密钥。而通信双方身份认证又是另外一个体系的东西。\n在 MITM 攻击中，攻击者拦截客户端和服务器之间的通信，冒充双方拦截或更改它们之间交换的数据。\n虽然 HTTPS 对传输中的数据进行加密，但攻击者可能会插入客户端和服务器之间，拦截加密流量并使用各种技术对其进行解密，例如获取受损的 SSL/TLS 证书或利用 SSL/TLS 协议本身的漏洞。\nMITM:\n\n服务器向客户端发送公钥。\n攻击者截获公钥，保留在自己手上。\n然后攻击者自己生成一个【伪造的】公钥，发给客户端。\n客户端收到伪造的公钥后，生成加密hash值发给服务器。\n攻击者获得加密hash值，用自己的私钥解密获得真秘钥。\n同时生成假的加密hash值，发给服务器。\n服务器用私钥解密获得假秘钥。\n服务器用加秘钥加密传输信息\n\n可以看出我们这里数据是明文传输的，存在窃听风险。但是我们为了阐述数字签名机制是如何运转的，故意将保证信息机密性的机制省略了。\n如果想要保证数据的机密性，我们常见的做法是，通信双方通过非对称加密安全交换对称加密的密钥，后续通信过程的数据都使用对称加密保证数据机密性。\n并且「签名」的作用本身也不是用来保证数据的机密性，而是用于验证数据来源的防止数据被篡改的，也就是确认发送者的身份。接受者 Alice 收到后，取下数字签名，同时用 Bob 的公钥解密，得到「摘要1」，证明确实是 Bob 发的。\n再对邮件内容使用相同的散列函数计算「摘要2」，与上面得到的「摘要1」进行对比，两者一致就说明信息未被篡改。\nHTTPS\n主要三个作用：1. 加密通信 2.认证服务器的身份，防止mitm 窃听 3. 防止篡改，integrity\nHTTPS（Hypertext Transfer Protocol Secure）使用了混合加密机制，包括非对称加密和对称加密，以确保安全的数据传输。ssl3.0 1998\nTLS（传输层加密协议） 1.2 升级成 TLS 1.3，TLS 1.3 大幅度简化了握手的步骤，完成 TLS 握手只要 1 RTT，而且安全性更高。在 TLS 1.2 的握手中，一般是需要 4 次握手，先要通过 Client Hello （第 1 次握手）和 Server Hello（第 2 次握手） 消息协商出后续使用的加密算法，再互相交换公钥（第 3 和 第 4 次握手）\n\n客户端发起请求：\n\n客户端向服务器发起 HTTPS 请求。\n\n\n服务器响应并发送证书：\n\n服务器响应请求并发送 SSL/TLS 证书。证书中包含服务器的公钥，该公钥由受信任的证书颁发机构（CA）签名。\n\n\n客户端验证证书：\n\n客户端验证服务器的证书，确保证书由受信任的 CA 签发，证书未过期，且证书中的域名与请求的域名匹配。\n验证证书链：客户端通过验证证书链来确认证书是否由受信任的 CA 签发。证书链由服务器证书、中间证书（如果有）、根证书组成。\n检查证书有效期：客户端检查证书的有效期，确保当前日期在证书的有效期内。\n验证证书的域名：客户端验证证书中的域名是否与请求的域名匹配。\n检查证书吊销状态：客户端可以通过在线证书状态协议（OCSP）或证书吊销列表（CRL）来检查证书是否被吊销。\n\n\n协商 SSL/TLS 版本和加密算法：\n\n客户端和服务器协商选择双方都支持的 SSL/TLS 版本和加密算法套件（cipher suite）。这些算法套件定义了具体使用的对称加密算法、非对称加密算法、哈希算法等。\n\n\n密钥交换和会话密钥生成：\n\nEphemeral Diffie-Hellman（DHE）或 Ephemeral Elliptic Curve Diffie-Hellman（ECDHE）：客户端和服务器使用临时的 Diffie-Hellman 或椭圆曲线 Diffie-Hellman 参数生成一个共享的预主密钥（pre-master secret）。这涉及使用服务器的临时公钥（通常是服务器基于 ECC 生成的）和客户端生成的临时公钥。\nRSA：如果使用 RSA 密钥交换，客户端生成一个随机的预主密钥，并使用服务器证书中的公钥对其进行加密，然后发送给服务器。服务器使用其私钥解密预主密钥。\n\n\n生成会话密钥：\n\n双方使用共享的预主密钥生成会话密钥。会话密钥用于对通信数据进行对称加密。\n\n\n数据加密：\n\n使用协商好的对称加密算法（如 AES、ChaCha20 等），客户端和服务器使用会话密钥对传输的数据进行加密和解密。\n\n\n完整性验证：\n\n使用消息认证码（MAC）或哈希函数（如 SHA-256）来验证数据的完整性，确保数据在传输过程中未被篡改。\n证书验证的技术\n浏览器验证证书: 浏览器验证证书的合法性，包括：\n证书颁发机构: 浏览器检查证书是否由一个受信任的证书颁发机构（CA）颁发。\n证书有效期: 浏览器检查证书是否在有效期内。\n证书与域名匹配: 浏览器检查证书是否与网站的域名匹配。\n证书签名: 浏览器检查证书的签名是否正确。\n浏览器检查证书链: 浏览器检查证书链，确保证书是由一个受信任的 CA 颁发的。\n浏览器验证证书的公钥: 浏览器验证证书的公钥，确保它与服务器的公钥匹配。\n\n\n\n证书验证使用了以下技术：\nX.509: X.509 是一种证书格式，用于存储证书信息。\nSSL/TLS: SSL/TLS 是一种加密协议，用于建立安全连接。\n公钥加密: 公钥加密是一种加密算法，用于加密数据。\n数字签名: 数字签名是一种加密算法，用于验证证书的合法性。\n证书验证的工具\n证书验证可以使用以下工具：\n浏览器: 浏览器内置了证书验证功能。\nopenssl: openssl 是一种命令行工具，用于验证证书。\ncertutil: certutil 是一种命令行工具，用于验证证书。\n安全外壳协议( SSH **)**是一种加密 网络协议，用于在不安全的网络上安全地运行网络服务。[1]其最著名的应用是 remote login and command-line execution.\nSSH 应用程序基于客户端-服务器架构，将SSH 客户端实例与SSH 服务器连接。[2] SSH 作为分层协议套件运行，包含三个主要分层组件：传输层提供服务器身份验证、机密性和完整性；用户认证协议向服务器验证用户；连接协议将加密隧道复用为多个逻辑通信通道。[1]\nSSH 是在类 Unix操作系统上设计的，作为Telnet和不安全的远程Unix shell协议的替代品\nAcross the Wall\n12.15问题：router设置为ipv6 解析时间长 infra建设没跟上，放弃。 想完全保证安全性和国内访问速率：刷openwrt，smartdns实现国内国外 dns 分流，需要另一台软路由。 折衷方案：全局代理？\n代理模式： HTTP/socks系统代理 （开发者决定 大部分只使用browser）， TUN tap代理， 真VPN\nTUN： 手机代理，软路由的模式\nTUN虚拟网卡只能说非常接近 VPN还不是真正的VPN 因为我们用的ss vmess trojan等主流的翻墙协议 都无法封装网络层的数据包 最直观的感受就是ping命令这个网络层的工具 当我们使用clash的tun模式ping谷歌的话 会返回一个假的延迟 这个一毫秒的延迟是直接从虚拟网卡返回的 并且 如果使用下节要讲的fakeIP模式的话 还会直接返回一个假的IP 因为ss vmess等协议无法代理网络层的icmp协议 而ping就是icmp协议的工具 真正的VPN可以代理网络层 所以我使用真正的VPN wireguard是可以ping通谷歌的 可以看到延迟是真实的\n目前来讲 tun模式是比较完美的客户端代理方式 既能实现在网络层接管系统所有流量 又能在此基础上实现分流 美中不足的地方就是使用ping命令 来测试网络延迟的时候就不太方便了 如果你平时只是用来浏览网页的话 第一种系统代理的方式也是一个不错的选择 至于真VPN的话 并不推荐用来科学上网 更适合有内网穿透需求的用户\nVPN协议和NAT协议都是通过重新构建一个IP首部来实现的，但他们的实现又有区别，VPN是将内部IP数据报加密后打包成外部IP数据报的数据部分，它的主要目的是为了数据的保密性，而NAT是纯进行地址转换，它的目的是为了解决本地编址的内部网络与外网通信的问题。\nVPN的实现主要使用了两种基本技术：隧道传输 和 加密技术\nVPN 通常处于网络层（第三层）或数据链路层（第二层）：\n\nIP层的VPN，如IPsec或L2TP，通常工作在网络层，它们通过创建虚拟的私有网络来路由IP数据包。\n数据链路层的VPN，如PPTP，工作在数据链路层，通常用于点对点通信。\n\nVirtual Private Network 译为虚拟专用网络或者虚拟私有网络 什么网络是私有网络 你家里的局域网就是你的私有网络 我们无法直接和你没有公网ip的电脑进行通信 除非到你家里连上你家的路由器 那我们就能在同一个私有网络里 那什么是虚拟 顾名思义 虚拟的意思就是虚拟 就是物理不存在 虚拟私有网络的意思就是我不需要物理的跑到你家里去 物理的连接上你家的路由器 就可以实现与你没有公网IP的电脑进行通信 而要实现这个功能的话 就必须要能够封装网络层的数据包 只有能够封装网络层的数据包 才能实现异地组网 才能实现内网穿透 才能实现虚拟的和你在同一内网里 才能称之为VPN\nVPN是一种技术 而不是某个具体的协议 VPN有很多技术实现 PPTP VPN IPSEC VPN OPENVPN WIREGUARD等等 具体实现细节都不太一样\n但有一个是必须要支持的 就是封装网络层 如果ss协议也能够封装网络层 实现异地组网 那也能称之为VPN\n当然 我这里并不是说VPN就更高级 相反的 对于科学上网来讲 它可是一点都不高级 因为VPN并不是为翻墙而生的 只是因为它能对数据进行加密 顺便实现了翻墙功能 相较于ss这种专门为翻墙而生 将流量特征隐藏的协议 VPN的流量清晰明了的写着它就是VPN的流量 而且VPN分流很不方便 所以用来科学上网并不合适 这里就不再深入了 以后有机会出内网穿透的教程再来详细介绍\n\n\nClash 的 DNS和代理两个模块是独立的. 日常终端PC使用 Clash for Windows 开启Systerm Proxy( HTTPS/SOCKS5代理) 并不需要 DNS enable: true, TUN 模式 是虚拟了网卡, 接管一切流量. OpenClash 实现虚拟网关用的是 TUN 模式, 而 Fake-IP /Redir-Host 的TUN模式、游戏模式 是(TAP 模式 很绕口 而 TAP 模式更推荐使用 Redir-Host 模式\nOpenClash 的绕过大陆IP 主要由两个模块组成 . 使用一份国内站点域名表 2w多条，添加至 Dnsmasq 中，在这个域名表中的 将会使用 默认114.114.114.114 来进行解析为真实的 IP ，在 iptables 中添加中国大陆 IP 段（ ipset ；直接绕过的规则并不使用 Clash 核心的 DNS 解析. 由此解决了 所有流量都经 Clash 核心转发，性能损耗过大 的问题\n插件设置-DNS设置中 *本地 DNS 劫持 使用防火墙转发 才能有黑白名单不走代理的局域网IP/MAC 选项. 我不清楚为何Redir-Host 模式 两种转发都能选黑白名单. Fake-IP 模式只在使用防火墙转发才能选择黑白名单. issue区有两种方法 [Feature] fakeIP模式下访问控制也有“不走代理的局域网设备 IP” #2506 [Feature] Fake-IP模式下对于MAC/IP黑白名单、绕过大陆IP，可否提供更多选项？ #3319 也有回复说 绕过中国大陆 IP 的实现是靠 Dnsmasq 先 DNS 分流判断实现的, 选择使用防火墙转发会使此功能失效\n上游 Clash 官方已弃用 Redir-Host (redir-host mode no longer available from Premium 2023.02.16) Clash.Meta 还在使用两者; Clash for Windows 不换核情况下 遵循上游 Clash 默认TUN Mode 是Fake IP Mode ; 两者的区别只是 Fake-IP-filter: - ‘+.*’ 绕过所有域名 就变成 Redir-Host Mode\nFake-IP 模式 用假IP映射是为了使用 Rule 域名规则, 需要代理域名的解析均传递真实域名到代理服务器上 所以并无必要获得无污染DNS的解析 IP. Clash.Meta 的嗅探功能是为了把 Redir-Host 模式的 “真实IP” 变回域名. (实现CDN优化 解锁Netflix 等. 所以 Fake-IP 模式只需要配置 nameserver 并无必要配置 fallback 以及 fallback filter ; 配置nameserver 仅为了直连和fakeip filter 内的国内加速.\n\nWARP原理\nWARP是CloudFlare提供的一项基于WireGuard的网络流量安全及加速服务，能够让你通过连接到CloudFlare的边缘节点实现隐私保护及链路优化。WireGuard 是由 Jason Donenfeld 等人用 C 语言编写的一个开源 VPN 协议，被视为下一代 VPN 协议，旨在解决许多困扰 IPSec/IKEv2、OpenVPN 或 L2TP 等其他WireGuard 就是采用 UDP 转发流量的 VPN 工具。他最大的优点也就是最大的缺点，采用 UDP 转发流量确实是能够有效的干扰墙的封锁，但是其稳定性实在是不敢恭维。\n其连接入口为双栈（IPv4/IPv6均可），且连接后能够获取到由CF提供基于NAT的IPv4和IPv6地址，因此我们的单栈服务器可以尝试连接到WARP来获取额外的网络连通性支持。这样我们就可以让仅具有IPv6的服务器访问IPv4，也能让仅具有IPv4的服务器获得IPv6的访问能力。\n\n为仅IPv6服务器添加IPv4\n\n原理如图，IPv4的流量均被WARP网卡接管，实现了让IPv4的流量通过WARP访问外部网络。\n\n为仅IPv4服务器添加IPv6\n\n原理如图，IPv6的流量均被WARP网卡接管，实现了让IPv6的流量通过WARP访问外部网络。\n\n双栈服务器置换网络\n\n有时我们的服务器本身就是双栈的，但是由于种种原因我们可能并不想使用其中的某一种网络，这时也可以通过WARP接管其中的一部分网络连接隐藏自己的IP地址。至于这样做的目的，最大的意义是减少一些滥用严重机房出现验证码的概率；同时部分内容提供商将WARP的落地IP视为真实用户的原生IP对待，能够解除一些基于IP识别的封锁。\nWARP是建立在Cloudflare 1.1.1.1的免费DNS服务器上。从技术上讲，Warp本质上就是一款VPN服务，Warp使用来自1.1.1.1的DNS服务器，并对它们之间的所有流量进行加密。\n通常，当你访问http://Baidu.com时，URL会被翻译成网站所在服务器的IP地址，由ISP的DNS服务器托管。当你使用Warp时，Warp将手机上的DNS服务器固定为1.1.1.1。因此，所有请求都转到Cloudflare的安全服务器。\nWarp在此基础上增加了一个额外的流程，使得设备和Cloudflare服务器之间的所有流量都是加密流量。但Warp是基于WireGuard隧道的UDP协议，中国大陆绝大部分运营商都会对这类流量进行惩罚式、限速式的限制策略，导致Warp在大陆使用上突发很高，但均速很低。(只能用于failover 😅 再加上Cloudflare的许多IP被国内Block，使得Warp在大陆接近一个不可用的状态。\nClash是一个开源的多协议代理工具，可以用于实现网络流量的代理和转发。它支持多种代理协议（如Shadowsocks、VMess、Trojan、Socks5等）和路由规则，能够实现灵活的网络流量控制和代理功能。\n当使用 clash 作为系统代理或者直接将浏览器的代理设置为 clash 时, 请求会交给 clash 来处理, 包括 dns 解析, 当然 clash 拿到请求之后, 并不会先 dns 解析 ip, 它会先根据规则一条一条匹配, 如果有命中规则, 则根据相应的规则去处理.\n这里要注意一下 IP-CIDR, 如果后面没有 no-resolve, 则需要 dns 解析 ip, 解析到 ip (可能被污染)后再根据此条进行对比, 如果没命中则进行下一条规则. 如果命中了, 走代理, 则不管它有没有被污染, 请求都会转发到节点服务器; 如果走直连, 则可能会被污染影响. 当然加上 no-resolve 之后, 就会让 clash 不进行 dns 解析, 所以此条 IP-CIDR 的直接 pass 掉, 可以这么说, 只有当浏览器直接对 ip 地址发起请求的时候, IP-CIDR 才会有效果.\n接下来再讨论下手机上的 qx / surge 等工具, 或者 clash 开启了 enhanced mode 后, dns 解析这一块发生的变化.\n由于请求并不是直接交给代理去处理, 所以浏览器需要先构建 dns 查询请求, 请求来到了 qx 或者软路由上面的 clash (透明代理)后, 被它劫持, 它有两种处理方式, redir-host 和 fake-ip, 前者已经被遗弃, 因为前者是要去获取 dns 解析的, dns 解析可能会解析到污染的 ip, 并且大多时候并不需要它进行 dns 解析. 所以 fake-ip 是主流, fake-ip 直接返回一条假的 ip, 并且对 ip 和域名进行了映射关系缓存, 这样浏览器拿到假的 ip 后进行 tcp 连接, 请求再次到达 qx / clash, 被截获后, 找到对应的域名, 接下来就是针对域名的规则匹配.\ntrojan是近些年兴起的网络工具，项目官网 https://github.com/trojan-gfw。与**强调加密**和混淆的SS/SSR等工具不同，trojan将通信流量伪装成互联网上最常见的https流量，从而有效防止流量被检测和干扰。在敏感时期，基本上只有**trojan和 v2ray伪装 能提供稳如狗的体验**。\n想要长期稳定高效的科学上网，socks5 类型的代理基本是必须要掌握的。Clash 支持的代理类型有 ss、vmess、socks5、http 和 snell\nSOCKS是一种Internet 协议，它通过代理服务器在客户端和服务器之间交换网络数据包。SOCKS5可选择提供身份验证，因此只有授权用户才能访问服务器。实际上，SOCKS 服务器将 TCP 连接代理到任意 IP 地址，并提供一种转发 UDP 数据包的方法。\nSOCKS 执行在OSI 模型的第 5 层（会话层，表示层和传输层之间的中间层）。SOCKS 服务器接受 TCP 端口 1080 上的传入客户端连接，如RFC  1928中所定义。\nSOCKS使用握手协议来通知代理软件其客户端试图进行的SOCKS连接，然后尽可能透明地进行操作，而常规代理可能会解释和重写报头（例如，使用另一种底层协议，例如FTP；然而，HTTP代理只是将HTTP请求转发到所需的HTTP服务器）。虽然HTTP代理有不同的使用模式，HTTP CONNECT方法允许转发TCP连接；然而，SOCKS代理还可以转发UDP流量（仅SOCKS5），而HTTP代理不能。HTTP代理通常更了解HTTP协议，执行更高层次的过滤（虽然通常只用于GET和POST方法，而不用于CONNECT方法）。\nsocks5 类型的代理服务器在网络层级上是工作于应用层的会话层，很多流量都无法代理，因从即便是开了所谓的全局，也不能给游戏加速，毕竟游戏的网络传输一般都是跑在传输层的。像 Ping 和 Trace 这些 ICMP 命令自然也是无法通过代理的。（当然也有方法可以用软件强制接管虚拟网卡达到真全局的目的，比如 SSTAP，tun2socks 等等）\n如果客户端和服务器都可以独立发包，但是偶尔发生延迟可以容忍（比如：在线的纸牌游戏，许多MMO类的游戏），那么可考虑使用TCP长连接如果客户端和服务器都可以独立发包，而且无法忍受延迟（比如：大多数的多人动作类游戏，一些MMO类游戏），那么考虑使用UDP加速器原理 加速器的原理很简单，就是UDP代理\n主要难在两点，其一是怎么处理游戏客户端到加速器服务器之间的UDP连接，其二是怎么让游戏客户端去连接这个加速器（一般游戏客户端是没有设置代理服务器的功能的）\n处理UDP有两种思路，一种是协议套娃，将游戏的UDP包外面套一层TCP（UDP over TCP ），到了目的地再把TCP解包成UDP，最后在发送到游戏服务器，返回的数据包也做同样处理；另外一种是伪造TCP（FakeTCP），对UDP数据包加上伪造的TCP包头，让其看起来像是TCP协议，欺骗运营商。\n主动检测\nHTTP所有没有正确结构和密码的连接都将被重定向到预设端点,因此,如果可疑探针连接(或者只是您的粉丝连接到您的博客XD),木马服务器的行为与该端点完全相同(默认情况下)。\n被动检测\n因为流量受到保护TLS(用户有责任使用有效的证书),所以如果你正在访问一个HTTP站点,流量看起来\n是一样的(握手后HTTPS只有一个);如果您没有访问某个站点,那么流量看起来与“保持活动状态”或“保持活动状态“相同。因此,木马还可以绕过ISP限制。RTT TLS HTTP HTTPS WebSocket QoS\n常见端口扫描技术\n3.3.2.1. 全扫描\n扫描主机尝试使用三次握手与目标主机的某个端口建立正规的连接，若成功建立连接，则端口处于开放状态，反之处于关闭状态。\n全扫描实现简单，且以较低的权限就可以进行该操作。但是在流量日志中会有大量明显的记录。\n3.3.2.2. 半扫描\n半扫描也称SYN扫描，在半扫描中，仅发送SYN数据段，如果应答为RST，则端口处于关闭状态，若应答为SYN/ACK，则端口处于监听状态。不过这种方式需要较高的权限，而且现在的大部分防火墙已经开始对这种扫描方式做处理。\n3.3.2.3. FIN扫描\nFIN扫描是向目标发送一个FIN数据包，如果是开放的端口，会返回RST数据包，关闭的端口则不会返回数据包，可以通过这种方式来判断端口是否打开。\n这种方式并不在TCP三次握手的状态中，所以不会被记录，相对SYN扫描要更隐蔽一些。\nHTTPS加密\n是的，HTTPS（Hypertext Transfer Protocol Secure）使用了混合加密机制，包括非对称加密和对称加密，以确保安全的数据传输。\nTLS 1.2 升级成 TLS 1.3，TLS 1.3 大幅度简化了握手的步骤，完成 TLS 握手只要 1 RTT，而且安全性更高。在 TLS 1.2 的握手中，一般是需要 4 次握手，先要通过 Client Hello （第 1 次握手）和 Server Hello（第 2 次握手） 消息协商出后续使用的加密算法，再互相交换公钥（第 3 和 第 4 次握手）\nSSL握手的步骤如下：\n\nSSL或TLS客户端先向服务端发送一个加密通信请求，叫做ClientHello请求。该请求包含以下信息：\n\n客户端支持的SSL或者TLS版本\n客户端生成的随机数，用于生成后续通信的随机字符串（“对话密钥”）\n客户端支持的加密算法\n\n\nSSL或TLS服务端收到客户端请求后，向客户端发出响应，叫做ServerHello。该响应包含以下信息：\n\n服务端从客户端提供的SSL或TLS列表中选择的版本\nSesstion ID 和 另外生成的随机数\n服务端的数字证书（如果服务端需要用于客户端身份验证的数字证书，则服务端发送一个客户端证书请求，其中包含受支持的证书类型列表和可接受的认证机构(CAs)的专有名称。）\n确认使用的加密算法\n\n\n客户端收到服务端响应后，首先校验服务端发来的数字证书决定是否继续通信。\nTLS 第三次握手    客户端验证完证书后，认为可信则继续往下走。 接着，客户端就会生成一个新的随机数 (pre-master)，用服务器的 RSA 公钥加密该随机数，通过「Client Key Exchange」消息传给服务端。 服务端收到后，用 RSA 私钥解密，得到客户端发来的随机数 (pre-master)。 至此，客户端和服务端双方都共享了三个随机数，分别是 Client Random、Server Random、pre-master。 于是，双方根据已经得到的三个随机数，生成会话密钥（Master Secret），它是对称密钥，用于对后续的 HTTP 请求/响应的数据加解密。 生成完「会话密钥」后，然后客户端发一个「Change Cipher Spec」，告诉服务端开始使用加密方式发送消息。\n如果服务端发送了一个客户端证书请求，客户端将会发送一个用客户端私钥加密的随机字符串和客户端的数字证书，或者没有数字证书的警告。在某些强制客户端证书的实现中，如果客户端没有数字证书，则握手会失败.\n服务端接受并验证客户端证书\n客户端向服务端发送一条完成的消息，该消息使用密钥加密，表示握手的客户端部分已经完成。\n服务端向客户端发送一条完成的消息，该消息使用密钥加密，表示握手的服务端部分已经完成\n在SSL或TLS会话期间，服务端和客户端现在可以交换使用共享密钥对称加密的消息\n\n\nA rainbow table is a precomputed table of passwords and their hashes,\n彩虹表对包含大量盐的单向哈希无效。例如，考虑使用以下函数生成的密码哈希（其中“ + ”是串联运算符）：\nsaltedhash(password) = hash(password + salt)\n要么\nsaltedhash(password) = hash(hash(password) + salt)\nsalt 值不是秘密的，可以随机生成并与密码哈希一起存储。大盐值通过确保每个用户的密码被唯一地散列来防止预计算攻击，包括彩虹表。这意味着具有相同密码的两个用户将具有不同的密码哈希值（假设使用不同的盐）。为了成功，攻击者需要为每个可能的盐值预先计算表。salt 必须足够大，否则攻击者可以为每个 salt 值制作一个表。对于使用 12 位盐的旧Unix 密码，这将需要 4096 个表，这会显着增加攻击者的成本，但对于 TB 硬盘驱动器来说并非不切实际。SHA2-crypt和bcrypt方法——用于Linux、BSD Unixes 和Solaris — 有 128 位的盐。[4]这些较大的盐值使得针对这些系统的预计算攻击对于几乎任何长度的密码都不可行。即使攻击者可以每秒生成一百万张表，他们仍然需要数十亿年才能为所有可能的盐生成表。 Or by key  strengthening\nInjection—     Could try to “sanitise” (clean/make safe) data input, but there is a better solution. • Do not create SQL (or similar statements) by adding together strings. • Can use special routines designed to produce these statements. • Languages designed for the web contain functions to help with this. • In Java, PreparedStatement is a class to do this.\nSocket\nHTTP是构建在TCP协议之上的，而TCP可以使用Socket来实现网络通信。基于这一点，可以使用Socket编程来创建HTTP客户端或服务器。Socket API提供了对底层网络通信的访问，可以使用不同的编程语言（如Python、Java、C++等）的Socket库来实现HTTP客户端和服务器。Socket APi 也可以实现UDP通信\n对于HTTP客户端：\n\n客户端可以使用Socket建立与服务器的TCP连接。\n客户端通过Socket发送HTTP请求（如GET、POST等）给服务器。\n服务器接收到请求后，通过Socket发送HTTP响应给客户端。\n客户端通过Socket接收服务器的响应数据。\n\n对于HTTP服务器：\n\n服务器可以使用Socket监听指定端口，接收客户端的连接请求。\n服务器接收到连接后，创建Socket连接到客户端。\n服务器通过Socket接收来自客户端的HTTP请求。\n服务器处理请求，并通过Socket发送HTTP响应给客户端。\n\n分层模型中的精华思想之一是封装（Encapsulation）。在网络协议的分层设计中，封装指的是每一层向上一层提供服务时，将上一层的数据进行封装，隐藏了底层协议的实现细节，形成一个黑盒子，上层协议不需要了解下层协议的任何细节。封装使得分层模型中的每一层都像一个黑盒，只暴露出相应的接口和功能，而隐藏了内部实现的细节，这样有利于提高系统的可靠性、可维护性和可扩展性，同时促进了协议的标准化和互操作性。\n一个是 fread/fwrite 读写，一个是 recv 和 send 读写（在 Linux 下你用 read 和 write 的话，文件和 socket 两者都能读写，只是无法直接设置一些特殊的 flag）\n一般的文件以及 socket 客户端读写的都是数据，而 socket 服务端 accept 读出来的是可以读写的客户端文件。\n\n监听套接字 (Listening Socket)：这是服务器端的套接字，通过调用**bind、listen和accept函数来建立。监听套接字用于等待客户端的连接请求，当客户端请求连接时，accept**函数会返回一个新的已完成连接套接字。\n已完成连接套接字 (Connected Socket)：这是服务器端的套接字，也是客户端的套接字。它们通过**connect函数（客户端）或accept函数（服务器端）建立连接后，用于实际的数据传输。这些套接字可以通过read和write**函数来进行数据的读取和写入。\n\n成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。\n严格来讲，“网关”是一个逻辑概念，【不要】把它当成具体的网络设备。充当“网关”的东东，可能是：路由器 or XX层交换机 or XX层防火墙 or 代理服务器 …\n“网关”也分不同的层次。如果不加定语，通常指的是“3层网关”（网络层网关）。列几种比较常见的，供参考：\n路由器充当网关——3层（网络层）\n3层交换机充当网关——3层（网络层）\n4层交换机充当网关——4层（传输层）\n应用层防火墙充当网关——7层（应用层）\n代理服务器充当网关——（取决于代理的层次，参见前一个小节）\n“隧道协议”可以做到更灵活的包裹——既可以对层次相隔很远的协议进行包裹，也可以对同一层的协议进行包裹，甚至可以“倒挂”——所谓的“倒挂”就是让【上】层反过来包裹【下】层。\n举例：\n俺曾经写过一篇《如何让【不支持】代理的网络软件，通过代理进行联网（不同平台的 N 种方法）》，其中介绍了“HTTP 代理”的两种模式：“转发模式 ＆ 隧道模式”。对于“HTTP 代理”的隧道模式，可以实现【TCP over HTTP】（把 TCP 协议打包到 HTTP 协议内部）\n##Network层\n网络层的两种交换技术：电路交换（有连接） VS 分组交换（无连接）。\nIP/ARP 是根据IP地址获取MAC地址的一种协议。/ICMP\n\nIP地址： IP协议使用IP地址来唯一标识网络上的每个设备。IPv4和IPv6是两个常见的IP地址版本。IPv4使用32位地址，而IPv6使用128位地址，提供了更广泛的地址空间以应对互联网的增长需求。\n面向无连接： IP是一种面向无连接的协议，这意味着每个数据包（或数据报）都是独立的，不需要在通信之前建立连接。每个数据包独立传输，因此不会维护通信状态。\n不可靠传输： IP提供了不可靠的传输，这意味着它不保证数据包的传输顺序、可靠性或交付。数据包可能会在传输过程中丢失、重复、延迟或乱序，因此上层协议（如TCP）负责处理可靠性和顺序问题。\n\n\n根据RFC 791，IP地址是一个32位的二进制数字，通常表示为四个八位字节的点分十进制表示法，如xxx.xxx.xxx.xxx。\n\nA类地址：\n\nA类地址的第一个字节的最高位始终为0，这表示A类地址的范围是1.0.0.0到126.0.0.0。\n这类地址通常用于大型网络，因为其范围允许约1670万个主机地址。\n\n\nB类地址：\n\nB类地址的前两个字节的最高两位始终为10，这表示B类地址的范围是128.0.0.0到191.255.0.0。\nB类地址通常用于中等规模的网络，可容纳约6.5万个主机地址。\n\n\nC类地址：\n\nC类地址的前三个字节的最高三位始终为110，这表示C类地址的范围是192.0.0.0到223.255.255.0。\nC类地址通常用于小型网络，每个C类网络可以容纳约254个主机地址。\n\n\n\n在实际网络中，已经有许多其他IP地址分配方案和规则，包括子网掩码、无类域间路由（CIDR）等，这些使得IP地址的分配更加灵活和高效。所以，不再严格使用A、B、C类地址来划分网络规模。\nVLSM(可变长子网掩码) 是为了有效的使用无类别域间路由（CIDR）和路由汇聚(route summary)来控制路由表的大小，它是网络管理员常用的IP寻址技术\nCIDR无类网络是一种相对于有类网络的网络，无类网络IP地址的掩码是变长的。\n在有类网络的基础上，拿出一部分主机ID作为子网ID。\n例如：\nIP地址为192.168.250.44 子网掩码不能是小于24位。\n因为这是一个C类地址（前3Bytes是网络号），子网掩码只能大于24位。\n而掩码255.255.248.0（21位）是不符合规定的。\n·\n如果一个网络中的主机有100台，\n那么，可以用子网掩码/25来划分这个C类网络（“192.168.250.0/24”）：\n划分成192.168.250.0/25 和192.168.250.128/25两个子网。\n—主机192.168.250.44/25 属于子网192.168.250.0/25。\n**路由器用于不同网络之间的通信，进行跨网络的路由决策，而交换机用于内部网络的局域网络通信，将数据帧从一个接口转发到另一个接口。**在许多网络中，路由器和交换机通常是一起使用的，以实现内部通信和与外部网络的连接。\n路由器（Router）：\n\n网络层设备： 路由器位于OSI模型的网络层，负责在不同网络之间进行数据包的转发和路由选择。\n跨网络通信： 路由器用于将数据包从一个网络传送到另一个网络，通常在不同IP子网之间执行路由操作。\n决策基于IP地址： 路由器的路由决策是基于目标IP地址进行的，它查找路由表以确定数据包应该被转发到哪个接口或下一个路由器。\n网络分割和隔离： 路由器可以分隔不同的网络，提供网络隔离和安全性。\n网络地址转换（NAT）： 一些路由器支持NAT，允许多个设备共享一个公共IP地址。\n\n交换机（Switch）：\n\n数据链路层设备： 交换机位于OSI模型的数据链路层，主要用于在局域网络（LAN）内的设备之间进行数据帧的交换。\n内部局域网络通信： 交换机用于在同一网络内的设备之间传输数据，通常在相同IP子网内工作。\n决策基于MAC地址： 交换机的决策是基于目标设备的MAC地址进行的，它使用MAC地址表来确定数据帧应该被发送到哪个接口。\n高性能： 交换机通常提供高性能的数据交换，因为它们在硬件级别进行操作，不需要进行复杂的路由选择。\n无状态： 交换机通常是无状态设备，不存储关于通信的历史信息，而路由器可能会维护路由表和状态信息。\n\n爬\nhttps://websec.readthedocs.io/zh/latest/info/site.html\nrobots.txt 也不是强制的规范，而是一种内容网站和搜索引擎之间博弈的产物。对于一个搜索引擎来说，遵守或者不遵守只关乎你作为一个搜索引擎的声誉，大多数时候还是遵守的，比如说百度上至今不能搜索淘宝宝贝，因为淘宝主动屏蔽了百度。内容站点也不是啥白月光，无非也是想要搜索引擎带来的流量，但是又不想爬虫占用服务器资源。\nzhihu.com/robots.txt\nUser-agent: GooglebotDisallow: /appview/Disallow: /loginDisallow: /logoutDisallow: /resetpasswordDisallow: /termsDisallow: /searchAllow: /search-specialDisallow: /notificationsDisallow: /settingsDisallow: /inboxDisallow: /admin_inboxDisallow: /*?guide*\n数据提取：\n\n请求发送与响应获取：爬虫程序通过发送 HTTP/HTTPS 请求获取网页内容，通常使用库或框架（如Python的requests、Scrapy等）来发送请求，并接收并解析网页服务器返回的响应。\n网页解析与DOM操作：爬虫需要解析 HTML 或 XML 格式的网页内容以提取所需的信息。通常使用解析器（如Beautiful Soup、lxml等）来解析网页结构，并使用 XPath 或 CSS 选择器等技术定位和提取所需的数据。\n动态网页抓取：针对使用 JavaScript 动态加载内容的网页，爬虫需要模拟浏览器行为，如使用无头浏览器（Headless Browser）或类似工具（如Selenium）来渲染 JavaScript，并获取动态生成的内容。 模拟浏览器行为：对于需要处理动态内容或使用 JavaScript 渲染的网页，模拟浏览器行为更有优势，甚至可能需要处理 WebSocket 连接。\n并发 分布式： ip代理池\n\n\n反爬虫技术：\n\n动态内容生成：网站使用动态生成内容或者 Ajax 请求，这使得爬虫难以捕获完整的页面数据。\n网站结构变化：经常更改网站的页面结构、URL格式或数据的位置，使得爬虫难以获取持续准确的数据。\n使用登录验证：限制对需要用户登录的内容的访问，要求爬虫模拟登录行为才能获取数据。\nrobots.txt 文件：用于向搜索引擎爬虫提供指导，告知哪些页面可以被索引，哪些页面不应该被索引。\nIP 黑名单：阻止来自特定IP地址的请求，这些IP地址可能是已知的恶意爬虫。\n用户代理检测：识别请求中的用户代理（User-Agent），并阻止或限制来自爬虫的请求。\n验证码：通过向用户展示验证码，要求用户进行验证以确认其身份，从而阻止自动化爬虫。\n频率限制：限制对网站的访问频率，例如限制某个IP地址或用户在特定时间段内的请求次数。\nJavaScript 加载：某些爬虫不支持JavaScript，网站可以利用JavaScript动态加载内容，使得爬虫难以获取完整的页面内容。\n\nService worker 本质上充当 Web 应用程序、浏览器与网络（可用时）之间的代理服务器。这个 API 旨在创建有效的离线体验，它会拦截网络请求并根据网络是否可用来采取适当的动作、更新来自服务器的的资源。它还提供入口以推送通知和访问后台同步 API。\nService workers 也可以用来做这些事情：\n\n后台数据同步\n响应来自其他源的资源请求\n集中接收计算成本高的数据更新，比如地理位置和陀螺仪信息，这样多个页面就可以利用同一组数据\n在客户端进行 CoffeeScript、LESS、CJS/AMD 等模块编译和依赖管理（用于开发目的）\n后台服务钩子\n自定义模板用于特定 URL 模式\n性能增强，比如预取用户可能需要的资源，比如相册中的后面数张图片\n\n未来 service worker 能够用来做更多使 web 平台接近原生应用的事。值得关注的是，其他标准也能并且将会使用 service worker，例如：\n\n后台同步：启动一个 service worker 即使没有用户访问特定站点，也可以更新缓存\n响应推送：启动一个 service worker 向用户发送一条信息通知新的内容可用\n\n","tags":["网络安全"]},{"title":"你的网络你做主？—— 浅谈宽带限制上传、TR069 与 PCDN 那些事","url":"/2024/12/02/surveillance/","content":"前几年受“提速降费”政策影响，国内高速宽带发展非常快。然而此一时彼一时，“互联网经济”这个话题落幕，运营商开始重新圈地。“打击 PCDN ”似乎就是吹响运营商利益至上的号角。\n按照惯例，政策执行后续都会扩大化。说是禁 PCDN ，目前已经有扩大化迹象，比如顺带被封禁的 PT 和 NAS 服务。这两个服务同样容易产生较大网络流量，所以容易被误伤。不过事情还不仅如此。网络监控会进一步收紧，运营商会全面监控网络流量，这种监控是否进一步扩大化不详。\n最近，不少网友反映，家里的宽带网关似乎变得“不太安分”了。以中国移动为例，后台进程里多了几个不认识的应用，比如 com.chinamobile.smartgateway.andlink、com.chinamobile.smartgateway.cmccdpi、com.chinamobile.smartgateway.appcore 和 com.cmcc.wuyougateway。这些应用是什么？它们在干什么？运营商为什么要这么做？今天我们就来聊聊这个话题。\n中国移动在为用户安装宽带时，总会好心地“赠送”一个光猫(调制解调器)。这看似是个不错的促销手段，但通常情况下，这个“赠品”会给用户带来许多麻烦。\n赠送的“光猫”安装了许多不必要的功能。光猫的本职工作是进行光电转换，然而中国移动的“光猫”不仅承担着光电转换的任务，还同时进行拨号上网、路由、地址转换(NAT)、端口映射、文件服务器(FTP)等等额外的功能。这些功能让本就廉价而算力低下的光猫运行缓慢，不堪重负。最可气的是，中国移动为了节约维护成本，将光猫的绝大多数设置项锁定，防止用户修改。\n用户想要拿到光猫的控制权，这是一个很常见的需求，尤其是在对网络有一定了解和个性化需求的用户群体中。然而，由于运营商出于网络稳定、业务安全和管理便捷等方面的考虑，往往会对光猫进行限制，用户很难完全掌控自己的设备。为了重新“拥有”光猫，我决定夺回对光猫的控制权。\n“神秘”的应用们：功能与争议\n2019年10月28日，有人在 Pastebin 公开了一份华为路由器的某个配置文件， 其中就有 com.chinamobile.smartgateway.cmccdpi 插件。\n2022 年8月，多个技术论坛（如知乎、贴吧、恩山无线论坛等）开始涌现用户对中国移动宽带服务的投诉和技术讨论。一些用户尝试通过第三方手段获取超级管理员权限（如解密密码、破解光猫固件）。另一些用户干脆自购路由器，尝试完全绕过光猫的管理干预。\n用户发现通过 4in6 隧道接入的家庭网络通常会叠加多层 NAT，导致端口映射功能受限，无法高效实现公网通信。用户发现移动宽带在 IPv6 网络部署中引入了 4in6 隧道技术，通过将 IPv4 流量封装到 IPv6 中传输。删除 TR-069 是否有效？\nTR-069 协议是宽带运营商常用的远程管理工具，但部分用户指出，即使禁用该协议，光猫依然接受远程指令，表明可能存在其他未知远程管理机制。\n超级密码的再下发：\n用户多次修改光猫管理员密码后，运营商会周期性地通过远程下发随机生成的超级密码覆盖原密码，这让用户感到隐私受侵。\n2023年4月23日，谷子猫发布了博文，表示移动的光猫含有一些特殊插件，分别是：\ncom.cmcc.wuyougateway\ncom.chinamobile.smartgateway.andlink\ncom.chinamobile.smartgateway.cmccdpi\ncom.chinamobile.smartgateway.appcore\n它们似乎被称为「软探针」，能够生成给用户观看的「家庭网络使用概览」，其中含有社交、游戏、视频等应用类型的流量占比， 并且还有「网络安全防护情况」，能为用户检测安全风险，比如「检测不良内容网站」。当使用桥接，通过路由器上网后， 上述功能就会失效。\n所以 …cmccdpi 插件最后的 dpi，应该就是指深度包检测（DPI），或许还有 DNS 请求记录之类的功能。有这样的功能， 才能起到提供信息以及保护用户的作用。但是这些数据除了呈现给用户，是否还分享给第三方，暂不可知。\nTR069：运营商的“远程控制”\n这些应用是如何被安装到你的网关里的呢？答案就是 TR069。\nTR069 是一种标准化的远程管理协议，允许运营商远程配置和管理用户家中的网关设备。通过 TR069，运营商可以下发配置、更新固件、诊断故障，当然也可以安装和启动应用。\nTR069 的存在，让运营商可以更高效地管理大规模的网络设备，及时修复漏洞、推送更新，保障网络稳定运行。但同时，也赋予了运营商极大的控制权，用户在某种程度上失去了对自己网关的掌控。\nPCDN 大潮下的运营商考量\n这一切的背后，是 PCN（Peer Content Network，对等内容网络）和 CDN（Content Delivery Network，内容分发网络）技术的发展和普及。\n随着互联网流量的爆炸式增长，传统的中心化内容分发模式已经难以满足需求。中国多地互联网服务提供商（ISP）对基于PCDN（个人云分布式网络）的相关业务采取了严厉措施，包括限速、封停宽带、甚至要求用户签署保证书恢复部分网络服务。这种强制性政策主要出于以下几点考量：\n网络负担与成本问题：PCDN通过利用家庭宽带的上行带宽分发内容，可能导致网络拥堵，特别是在上行带宽较小的小区网络中，还会加大运营商在骨干网扩容和运维方面的成本​\n商业竞争与监管合规性：运营商的核心收入来源之一是提供CDN（内容分发网络）服务，而PCDN的广泛使用可能蚕食这一市场。此外，根据中国工信部的相关规定，未取得经营许可证的个人或企业不得提供CDN服务，因此个人通过PCDN设备牟利被视为违法\n短视频博主“影视飓风”引发了一场关于视频画质的热议。他在微博上透露，由于多种原因，其与视频清晰度相关的内容被要求在全网下架。这起事件迅速吸引了大量关注，引发了人们对视频平台画质压缩策略的讨论。\n影视飓风此前发布了一则科普视频《清晰度不如4年前！视频变糊是你的错觉吗？》，指出一些视频平台为了降低流量成本，采用降低码率和改变编码格式等措施。这种动态调节码率的做法在视频热门时尤为常见，目的是节省带宽，但这可能导致画质下降\n此外，DPI 技术在 PCN/CDN 架构中也扮演着重要角色。通过分析网络流量，运营商可以更精准地进行内容调度和缓存，提升 PCN/CDN 的效率。同时，DPI 也可以用于识别和过滤非法内容，保障网络安全。\n如何摆脱控制\n改光猫桥接，最麻烦的就是超密的获取。\n一些省份或者运营商，是网络上可查的通用帐号密码。这就比较好办\n还有一些地方，超密是自动下发的，甚至会定期自动更改。这就很麻烦。\n而且运营商非常与时俱进，很多网络上容易获取的办法，他们都已经想办法屏蔽掉了。因此，即便是同运营商同型号设备，或许网络上找到的办法，也未必能在你的手里成功。\n总得来说就是获取超级密码、开telnet、改桥接、改地区、禁用TR069。注意 用户需要具备一定的技术知识和技能，才能正确地配置和管理光猫。\n用户：知情权与选择权\n然而，在运营商追求效率和商业利益的同时，用户的知情权和选择权也应该得到尊重。\n运营商应该向用户清晰地说明智能网关应用的功能、作用以及数据使用方式，获得用户的明确授权。用户应该有权选择是否启用这些功能，并有权卸载或禁用不需要的应用。\n此外，运营商也应该加强安全防护，保护用户数据安全，防止隐私泄露。只有在用户信任的基础上，运营商的这些举措才能真正发挥作用，实现用户、运营商和整个互联网生态的多赢。\n","tags":["网络安全"]},{"title":"信任的本质","url":"/2024/06/06/trust/","content":"一战期间，战争被和平打破。\n那是在 1914 年「西方战线」的圣诞节，\n尽管有着严格的军令不准他们与敌方士兵谈笑风生，\n英国和德国的军人们却都离开了他们的战壕，穿过了无人地带，\n聚集在了一起，来埋葬他们的战友、交换礼物、有的还玩起了游戏。\n相比之下，现在已经是 2024 年，世界已经保持了好几十年的表面和平，\n我们，诶？我们信任彼此的能力简直太差了。有调查表明，在过去的四十年当\n中，越来越少的人说他们「信任彼此」。所以，我们难免会有这样的疑问：\n为什么朋友会变成敌人，即使在和平年代也不例外呢？\n为什么敌人也会变成朋友，而且这种事居然在战争年代也会发生？\n我个人觉得博弈论可以帮助我们解释为什么「不信任」会像病毒一般\n蔓延开来，以及我们怎样改变它！好了，如果你想了解这些…\n这有一块很有意思的游戏帮你了解这些 https://dccxi.com/trust/\n信任的本质是一种基于期待和预期的信心和依赖。它是人际关系中的一种情感和态度，涉及到对他人的可靠性、诚实性、能力和意图的相信和依赖。信任的本质可以从以下几个方面来理解：\n1、信任是对他人行为的预期：信任建立在我们对他人的行为和表现有一定的了解和观察上。基于过去的经验和观察，我们形成了对他人行为的预期，相信他们会按照我们的期望和共同的价值观来行动。\n2、信任是对他人能力和诚信的相信：信任包括对他人的能力和诚信的相信和依赖。我们相信对方具备完成某项任务或履行承诺的能力，并且相信他们会按照道义和道德的原则行事。\n3、信任是建立在风险和不确定性下的依赖：信任存在于风险和不确定性的环境中。当我们信任他人时，我们愿意将自己的利益和安全置于他人的掌控之下，意味着我们对可能的风险和不确定性有所接受并有所依赖。\n4、信任是相互的和双向的：信任是相互建立的，需要双方之间的互动和共同努力。它需要建立在相互理解、尊重和支持的基础上，并且需要双方都表现出诚实、透明和可靠的行为。\n仅仅重复别人动作的复读机可以与他人良好地相处，而自私的老油条们只能骗他们自己！不光如此，当他们需要对阵复读机这种只会以其人之道还治其人之身的人的时候，一定会让老油条们尝到作茧自缚的滋味。看起来博弈论的道理好像在告诉我们这些事情：像复读机这样「己所不欲勿施于人」的人生哲学不仅仅是道德层面上的真理，同时也是科学上的真理。然而…\n你看到了，你如果不进行足够多轮的比赛，（这里是五轮或者更少）老油条就会处于绝对的优势。\n1985 年的调查显示，当美国人被问到他们有多少亲密朋友的时候，最常见的回答是「三个」。而在 2014 年，这个数字是「零」。现在，我们拥有越来越少不同阶级，不同种族，不同经济状况，不同政治理念的朋友，原因只是因为我们所拥有的朋友数量变少了，仅此而已。而且，刚刚你自己也发现了一个道理，人与人之间越来越少的「重复互动」，所带来的影响就是不信任的加剧扩散。\n（不不，大众传媒不算是重复互动：必须是个人与个人之间进行的双向互动才行）\n同样的事情发生了：当**「双赢」**的报酬被降低，老油条开始处于优势地位。对于这种现象，博弈论里面有两个很恰当的概念可以参考：\n「零和游戏」， 指的是游戏的双方都悲观的相信己方得到的东西必然来自与对方的失去，反之亦然。\n「非零和游戏」，指的是游戏双方都努力创造双赢的局面（或者至少防止双输的出现）！如果没有非零和游戏，人与人之间的信任便不可能得到传播。\n博弈论向我们揭示了要发展信任人们所需要掌握的三件事：\n1. 重复的互动信任是保持人际关系长久的基石，但信任能够建立的前提，是你要知道你们未来会有重复的互动。\n2. 「双赢」并非不可能你必须进行非零和游戏，而在这样的游戏博弈中，并不要求两个玩家都会变得更好，达到双赢，但至少必须要有达成双赢的可能性。\n3. 低概率的误解如果误解发生得过多，信任就会瓦解。但是，如果有一点点误会的时候，它将使宽容蔓延开来。\n我们今天的问题并不仅仅是人们正在失去信任，而是我们的环境正在违背信任进化的规律。\n「我们仅仅是我们周遭环境的产物」，这个观念可能看起来可能有点犬儒或者天真无邪，但是博弈论也在提醒着，我们就是彼此的环境啊。短期来讲，游戏决定玩家，但是长期来说，决定游戏的正是我们这些玩家\n可信度与错误处理方式：\n\n认识错误并承担责任：当一个人能够承认并意识到自己的错误，并愿意为其行为承担责任时，这种态度显示了成熟和诚实的品质。这种人更有可能值得信任，因为他们对自己的行为负责。\n学习和改进：犯错后，重要的是从错误中学到教训，并采取行动避免再次犯同样的错误。一个能够从错误中学习、成长并改进的人更容易获得信任，因为他们表现出积极的态度和进取心。\n诚实与透明：诚实和透明度对于建立信任至关重要。即使犯了错误，但能够坦诚地沟通和分享情况，让他人了解真相，也会增加他人对其信任的可能性。\n一次错误不代表全部：犯错并不意味着这个人会一直犯错。重要的是看他们是否愿意改变和成长，以及是否从过去的错误中吸取了经验教训。\n\n值得信任的特质：\n\n一致性：表现出持续一致的品德和行为，比如坦诚、诚实和负责任。\n诚信：表现出真诚和诚实的态度，即使犯了错误也能坦诚面对。\n责任感：能够为自己的行为负责，并愿意纠正错误。\n成长和学习：从错误中学习并改进自己的行为，展现出成长和进步的态度。\n\n总体来说，一个人是否更值得信任不仅仅取决于他们是否犯过错误，而更在于他们如何面对并从中成长。那些能够诚实面对自己的错误、承担责任并改进的人，更有可能赢得信任。\n人都会犯错，犯错概率 0% 的时候：公平的复读机 获胜！在 1% 到 9% 之间：宽容的复读鸭获胜！在 10% 到 49% 之间:不公平也不宽容的老油条 获胜。到了 50%，没有人会一直赢。\n这就是为什么我们说「误解」是达成信任的一个有趣的壁垒：一个小误解可以带来更多的宽容，但是太多的误解带来的却是大范围的不信任！我个人认为，现代科技进步使得的媒介越来越发达，虽然一定程度上帮助了人们增加了交流机会，但它带给人们的误解却其实更多。\n在东亚文化中，失败往往被视为一种耻辱。然而，这种观念正在改变，越来越多的人认识到失败是学习和成长的一部分。经历过失败的人往往更具有适应能力和韧性，更值得信任。\n对于创业者或任何领域的专业人士来说，失败和错误是不可避免的一部分。它们提供了宝贵的经验教训，让人们能够更加深刻地理解问题、改进方法，并最终获得更好的结果。因此，经历失败和错误的人可能更具有适应能力、韧性和学习能力，这些品质对于一个值得信赖的创业者或团队成员来说至关重要。\n","categories":["随谈"],"tags":["价值"]},{"title":"Vibe coding的是软件开发的未来吗？","url":"/2025/09/05/vibe/","content":"Vibe coding是人工智能研究员 Andrej Karpathy 于 2025 年推广的一种软件开发方法开发人员使用自然语言指导大型语言模型 (LLM) 生成、优化和调试代码，将重点从手动编码转移到更具对话性、更高级的问题解决和应用程序开发方法。\n把“功能交付速度”当成产品开发的最大瓶颈，是一种误解。\nVibe 这类编码工具的真正优势在于：不需要工程团队就能快速搭建出东西。很多宣传也是这样说的：\n“有人在没有雇任何软件工程师的情况下，仅靠 AI 打造出有效的科技业务，并做到百万美元规模。”\n听上去很诱人。但我们需要把问题拆开来看。\n\n原型设计 vs. 产品\n在原型阶段，Vibe 确实很有用。它能让想法快速落地，判断是否值得继续。但原型的本质是一次性的。\n即便方向正确，原型也不能直接转化为产品。因为原型的目标是“便宜、快”，而不是“稳定、持久”。它可能很丑，有 bug，甚至会崩溃，但这都没关系——它只是用来传达想法。\n产品则完全不同。\n它必须持续交付价值。\n\n体验差，用户会转身去找替代品。\nbug 太多，用户会放弃。\n一旦彻底崩溃，更没人会为它付费。\n\n最终，质量是不可妥协的。客户留存才是产品的生死线。\n\n成功产品的路径\n很多成功的数字产品并不是线性演进出来的，而是无数实验后的结果。\n拿亚马逊来说：从卖书开始，后来进入音乐、视频、硬件、云，似乎是一条“必然的”路径。但如果你看他们的文化，会发现背后是成千上万的试验和迭代。\n关键点：没人能预先知道哪个功能或方向会成功。\n我们只能不断尝试，把有效的保留，把无效的砍掉。\n\nGmail 的故事\nPaul Buchheit 因在短短几个小时内构建了第一个 GMail 原型而闻名。之后，他又在 AdSense 上重复了这一技巧。所有这些代码都必须手动编写，就像你懂的，用手写一样。 是的，我们正在谈论谷歌的两款惊艳产品。然而，如果你仔细阅读这个故事，就会发现这绝不是一个精心策划的计划的执行。 “在Gmail 发布前的两年半开发过程中，我们做错了很多事情。” “到发布时，我们重写了前端大约六次，后端重写了三次。” “我只是编写代码，发布功能，然后观察反响。通常情况下，每个人（包括我）最终都会讨厌它（尤其是我的想法），但我们总能从经验中吸取教训，并且能够迅速转向其他想法。” 保罗·布赫海特.\n换句话说，他们探索过无数个死胡同，推翻了无数个，最终还是走了出来。事先什么都不知道。 它适用于产品功能，也适用于整个产品 同样的模式也适用于产品，而不仅仅是产品功能。例如，我们可以继续使用谷歌。它以一系列符合其整合全球信息的使命的产品而闻名。搜索、Gmail、Google Workspace 等等。 然而，除了搜索之外，许多此类产品最初都只是实验性质的。上文提到的 Gmail 和 AdSense 就是两个值得注意的例子。\n\n这种逻辑不仅适用于某个功能，也适用于整个产品线。\nGoogle 除了搜索以外，大量产品最初都是实验：Gmail、AdSense、甚至 Workspace。\n成功的背后有十几个失败的，失败的背后还有成百上千个从未公开的尝试。\nGoogle Reader、Microsoft Math Solver…—但它们都已经消失了。Tech companies依旧不断试错，因为那是找到真正“长青产品”的唯一方式。\n你这段补充正好把前文的逻辑闭环补上了，我帮你压缩和打磨一下，让结尾更锋利，更能打出主题：\n\n验证与沟通\n产品开发的本质就是持续探索：先验证想法，再验证改动是否带来增长、收入或留存。\n问题在于——验证需要时间。\nLeah Tharin 分享过她在数千万用户产品中的经验：\n即便流量巨大，团队的瓶颈也在等待实验达到统计显著性。修改首页文案，几个小时就能看出结果；但调整漏斗深处的复杂功能，往往需要数周，甚至数月。\n想象一下，如果开发速度缩短十倍，增长会更快吗？除了落地页这种最简单的测试，大部分实验依旧需要等待数据沉淀。你也不能无限增加实验数量，否则只会让指标噪音更大、难以解释。\n真正的瓶颈在于：我们不知道该做什么。没有哪个团队能一开始就锁定有效路径。\n那如果方向明确呢？理论上，确定范围、签合同、开始开发，一切顺理成章。\n可实际情况是：\n\n我们常常在全速构建错误的东西。\n或者因为沟通不清，返工成本翻倍。\n开发了大量不必要的功能，事后才发现价值极低。\n\n难道我们就不能用我们经常吹嘘的20年经验，用通俗易懂的语言告诉大家成本是多少吗？不行。我们还不知道合作会是什么样子，而单是这一点，就足以比任何与范围直接相关的因素，对实际成本产生更大的影响。 沟通不畅会导致返工。沟通质量差，或者更确切地说，缺乏沟通，往往会使工作量增加100%，这种情况并不罕见。 再加上所有不必要的功能的开发，情况就更加糟糕了。事后想想，这些增值工作可能只占总投入的几个百分点。 加快开发速度只会加剧问题。返工成本并非线性增长。一旦你重新开始返工，它就像一个复利，只不过是反向的。想想它对成本的影响吧。\nfor example：初始工作量 100 小时，每次返工增加 30%，四轮下来就是 285 小时。加快最开始那 100 小时的开发速度，对整体影响几乎可以忽略。AI 的重写只能推迟问题，糟糕的抽象、监控缺失、集成脆弱——迟早会爆炸。\n\n编码速度从来不是瓶颈\nVibe coding 承诺两件事：\n\n\n我们能更快拿到代码\n\n\n我们不需要雇昂贵的工程师\n如果目标只是“把界面堆出来”，当然 vibe coding 是捷径。\n但产品开发从来不是“写完代码就完事”，而是 能否持续、可靠、规模化地解决用户问题。这依赖验证实验、长期可维护的系统、合理的激励结构，以及对增长、收入、留存等指标的清晰把握。写代码快，本来就不是瓶颈。\n听起来很诱人，但它们只是在回答一个容易的问题，而避开了真正的难题：我们到底要解决的是什么问题？\n\n\n所以，问题从来不在“写代码太慢”，而在于：开发速度从来不是决定性的短板\n\n我们不知道哪些方向值得尝试（验证的瓶颈）。\n我们没能清楚对齐（沟通的瓶颈）。\n\n"},{"title":"算法DSA","url":"/2024/05/24/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","content":"\n算法题目已经成为了公司筛人的一种方式，大厂的每一轮面试基本都会有几道算法题，甚至有的公司笔试全部都是算法题。其他题目答的都差不多，那你算法题做不出来，可能就被淘汰了。\n所以为啥要刷算法题呢？一方面是帮助你学习和理解算法，但另一方面也是像背公式、背八股文一样，增加你面试时碰到原题的概率。像面试无非就那么几个重点：树、动态规划、深度 / 广度优先搜索、链表、数组、排序、栈、队列、哈希、字符串等。你要先完成专项练习中一些简单的题目，理解其背后的算法和数据结构。之后，再举一反三，练习更多相关的题目，当你能做到用同一个算法解决一类共性问题，做到 多题一解 时，才算是真正理解了。\n总之,刷算法题是一个漫长而艰辛的过程,但如果坚持下去,一定会收获丰硕的成果。保持积极乐观的心态,循序渐进地提升自己,相信您一定能够成为出色的软件工程师。\n伪随机数是用确定性的算法计算出来自[0,1]均匀分布的随机数序列。并不真正的随机，但具有类似于随机数的统计特征，如均匀性、独立性等。在计算伪随机数时，若使用的初值（种子）不变，那么伪随机数的数序也不变。伪随机数可以用计算机大量生成，在模拟研究中为了提高模拟效率，一般采用伪随机数代替真正的随机数。\n    **伪随机数**\n\n算法与编程竞赛：大多数算法题和编程竞赛中使用伪随机数生成器就足够了。它们生成速度快，且可以通过设置种子来重现结果，便于调试。\n模拟与仿真：如蒙特卡罗模拟等，需要大量随机数的应用场景，伪随机数的效率和可控性非常重要。\n游戏开发：游戏中的随机事件、物品掉落、敌人生成等，大多数情况下使用伪随机数生成器。\n测试与调试：生成可重复的随机测试数据，便于发现和修复问题。\n     **真随机数**\n\n安全与密码学：在密码学、加密密钥生成等场景，对随机性的要求极高，真随机数生成器（如硬件随机数生成器）更适用。\n科学实验：某些科学研究和实验需要真正的随机性以避免偏差。\n二元关系（Binary Relations）\n二元关系是定义在两个集合之间的关系，可以表示成有序对的集合。它在以下方面有重要应用：\n\n数据库管理：二元关系用于定义关系数据库中的表。每个表都是一个二元关系，行表示记录，列表示属性。\n图论：图论中，边是顶点之间的二元关系。它们用于表示网络、通信路径、社交网络等。\n函数：函数是特殊的二元关系，其中每个输入对应一个唯一的输出。函数广泛用于数学建模和计算。\n\n偏序（Partial Order）\n偏序是一种特殊的二元关系，满足反射性、反对称性和传递性，用于表示元素之间的部分排序关系。它在以下方面有应用：\n\n任务调度：在任务调度和项目管理中，偏序用于表示任务的依赖关系。例如，任务A必须在任务B之前完成。\n数据结构：偏序用于定义数据结构如堆和优先队列。在这些结构中，元素以偏序关系组织。\n分类系统：在分类学中，偏序用于表示分类系统的层级关系。例如，物种分类中的从属关系。\n版本控制：在版本控制系统中，不同版本之间的依赖关系可以用偏序来表示。\n\n**全序（Total Order）**全序是一种特殊的偏序，满足每一对元素都是可比较的，即任何两个元素之间都有确定的顺序关系。全序在以下方面有应用：\n\n排序算法：全序用于定义排序算法的排序关系。常见的排序算法如快速排序、归并排序都依赖全序关系来确定元素的排列顺序。\n搜索算法：二分搜索等高效搜索算法依赖于元素的全序性。\n经济学：在经济学中，全序用于表示消费者偏好和商品价格等，使得商品和服务可以按优劣排序。\n决策理论：全序用于决策理论中的选择偏好，帮助决策者在不同选项之间进行比较和选择。\n\n贪心选择的一般特征：贪心选择性质和最优子结构性质。\n贪心算法和动态规划算法都要求问题具有最优子结构性质，这是两类算法的一个共同点。大多数时候，能用贪心算法求解的问题，都可以用动态规划算法求解。但是能用动态规划求解的，不一定能用贪心算法进行求解。\n找到最优子结构 =&gt; 动态规划解最值问题，状态转移方程\n\n最优子结构性质。如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质（即满足最优化原理）。最优子结构性质为动态规划算法解决问题提供了重要线索。\n无后效性。即子问题的解一旦确定，就不再改变，不受在这之后、包含它的更大的问题的求解决策影响。解的计算不依赖于问题的后续阶段，只依赖于当前阶段的状态。这使得我们可以独立地解决子问题，而不必关心它们如何被组合起来形成更大的问题的解决方案。\n子问题重叠性质。子问题重叠性质是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。动态规划算法正是利用了这种子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率，降低了时间复杂度。\n\nDynamic Programing\n动态规划（Dynamic Programming，简称DP）是一种求解最优化问题的方法。它通过将问题分解成更小的子问题，利用子问题的解来构造原问题的解。动态规划的核心思想是避免重复计算，通过存储子问题的结果来提高效率。下面是动态规划在多个领域的应用和示例代码。\nhttps://leetcode.cn/circle/discuss/tXLS3i/\n动态规划（入门/背包/状态机/划分/区间/状压/数位/树形/数据结构优化）\n记忆化搜索\n在求解动态规划的问题时，记忆化搜索和递推，都确保了同一状态至多只被求解一次。而它们实现这一点的方式则略有不同：递推通过设置明确的访问顺序来避免重复访问，记忆化搜索虽然没有明确规定访问顺序，但通过给已经访问过的状态打标记的方式，也达到了同样的目的。\n与递推相比，记忆化搜索因为不用明确规定访问顺序，在实现难度上有时低于递推，且能比较方便地处理边界情况，这是记忆化搜索的一大优势。但与此同时，记忆化搜索难以使用滚动数组等优化，且由于存在递归，运行效率会低于递推。因此应该视题目选择更适合的实现方式。\n\n把这道题的 dp 状态和方程写出来\n根据它们写出 dfs 函数\n添加记忆化数组\n\n状态压缩 DP（状压 DP）\n§9.1 排列型 ① 相邻无关\n从记忆化搜索到递推，从集合论到位运算，这里总结了常见的位运算技巧分类。\n\n暴力做法是枚举所有排列，对每个排列计算和题目有关的值，时间复杂度（通常来说）是 O(n⋅n!)。这种方法可以解决 n≤10 的问题。\n\n状态压缩\n\n状态压缩 DP：简称为「状压 DP」，是一种应用在「小规模数据,  n≤20」的数组 / 字符串上，结合「二进制」的性质来进行状态定义与状态转移的动态规划方法。和「二进制枚举子集算法」一样，我们通过一个「 n 位长度的二进制数」来表示「由 n 个物品所组成的集合中所有物品的选择状态」。状压 DP 可以把时间复杂度（通常来说）优化至 O(n⋅2^n)。   一维状态是集合，对状态进行操作或者状态之间进行转移，也就是要对集合进行操作。 因为我们使用二进制数来定义集合状态，所以对集合进行操作，就是对二进制数进行位运算操作。\n\n使用一个 int 类型的变量代替 visited数组\n\n状压 DP 通常有两种定义方式：\n\n定义 f[S] 表示已经排列好的元素（下标）集合为 S 时，和题目有关的最优值。通过枚举当前位置要填的元素（下标）来转移。\n定义 f[S] 表示可以选的元素（下标）集合为 S 时，和题目有关的最优值。通过枚举当前位置要填的元素（下标）来转移。\n\n注：部分题目由于爆搜+剪枝也能过，难度分仅供参考。\n\n优美的排列\n\n1. 二进制数和集合的一一对应关系。如集合s=9时，其二进制数为10001，对应2^4位、2^0位为1，可以表示集合中包含元素0 4。对于本题来说，下标是从1开始的，所以将2^0位为1视为表示1在集合内，即灵神说的本题二进制的最低位表示 1。s.bit_count()统计了s中有多少个1，即代表集合中有多少个元素。2. 判断元素i是否在集合s内：(s &gt;&gt; i) &amp; 1如s的二进制为:1 0 0 0 0，i=4，判断i是否在s内即判断s的2^4位是否为1，只需把s右移4位后和1相与。 本题的下标是从1开始的，要减1之后才能和二进制中的位相对应，所以灵神用的 (s &gt;&gt; (j-1) &amp; 1)，这个表达式为0表示元素j不在集合s中。3. 向集合s中添加元素i。s | (1 &lt;&lt; i)1&lt;&lt;i是设置2^i位为1，s|(1&lt;&lt;i)即设置s的2^i位等于1。灵神的dfs(s | (1 &lt;&lt; (j - 1)))即向集合中添加元素j。4. u表示全集。即从1到n所有数都选上的集合。\n定义 dfs(S,i) 表示在可以选的下标集合为 S，上一个选的数的下标是 i 时，可以构造出多少个特别排列。\n\n特别的排列：给你一个下标从 0 开始的整数数组 nums ，它包含 n 个 互不相同 的正整数。如果 nums 的一个排列满足以下条件，我们称它是一个特别的排列：对于 0 &lt;= i &lt; n - 1 的下标 i ，要么 nums[i] % nums[i+1] == 0 ，要么 nums[i+1] % nums[i] == 0 。\n\n栈是递归的底层实现， 我们用栈实现队列，用队列实现栈来掌握的栈与队列的基本操作。\n接着，通过括号匹配问题、字符串去重问题、删除字串问题，逆波兰表达式问题来系统讲解了栈在系统中的应用，以及使用技巧。\n通过求滑动窗口最大值，以及前K个高频元素介绍了两种队列：单调队列和优先级队列，这是特殊场景解决问题的利器，是一定要掌握的。\n\n正常循环的情况下，数组的滚动（游标移动）是向后的，引入栈的时候，则可以有了向前滚动的机会（有了一定的反悔的机会），然后这样子就能够解决一些局部的问题（比如说，寻找相邻的大的数字）。由于栈还可以对于没有价值（已经发现了大的数字）的东西删除，这样子的遗忘功能，简化了搜索空间，问题空间。\n\n当一个算法完全不进行多余的运算，那么它是一个时间复杂度最低的算法。但我们往往会对一些结果进行重复的计算，那么栈的引入就是为了解决这样的问题，栈存储了一些重要的运算结果，用于和接下来的元素进行比较。\n单调队列\n可以查询区间最值（不能维护区间k大，因为队列中很有可能没有k个元 素）\n优化DP \t优化动态规划方面问题的一种特殊数据结构，且多数情况是与定长连续子区间问题相关联。\n单调栈       对于某个元素i：\n左边区间第一个比它小的数，第一个比它大的数\n确定这个元素是否是区间最值\n右边区间第一个大于它的值\n到 右边区间第一个大于它的值 的距离\n确定以该元素为最值的最长区间\n单调递增栈：只有比栈顶元素小的元素才能直接进栈，否则需要先将栈中比当前元素小的元素出栈，再将当前元素入栈。这样就保证了：栈中保留的都是比当前入栈元素大的值，并且从栈顶到栈底的元素值是单调递增的。\ndef monotoneIncreasingStack(nums):    stack = []    for num in nums:        while stack and num &gt;= stack[-1]:            stack.pop()        stack.append(num)\n2.1 寻找左侧第一个比当前元素大的元素 #\n从左到右遍历元素，构造单调递增栈（从栈顶到栈底递增）：\n\n一个元素左侧第一个比它大的元素就是将其「插入单调递增栈」时的栈顶元素。\n如果插入时的栈为空，则说明左侧不存在比当前元素大的元素。\n\n单调队列实际上是单调栈的的升级版。单调栈只支持访问尾部，而单调队列两端都可以。\n单调队列是指：队列中元素之间的关系具有单调性，而且，队首和队尾都可以进行出队操作，只有队尾可以进行入队操作。\n307. 区域和检索 - 数组可修改\nhttps://leetcode.cn/problems/range-sum-query-mutable/description/\n线段树是算法竞赛中常用的用来维护 区间信息 的数据结构。\n线段树可以在O(logn)的时间复杂度内实现单点修改、区间修改、区间查询（区间求和，求区间最大值，求区间最小值）等操作。\n如果要求修改区间 [l,r]，把所有包含在区间 [l,r] 中的节点都遍历一次、修改一次，时间复杂度无法承受。我们这里要引入一个叫做 「懒惰标记」 的东西。 懒惰标记，简单来说，就是通过延迟对节点信息的更改，从而减少可能不必要的操作次数。每次执行修改时，我们通过打标记的方法表明该节点对应的区间在某一次操作中被更改，但不更新该节点的子节点的信息。实质性的修改则在下一次访问带有标记的节点时才进行。\n事实上，树状数组能解决的问题是线段树能解决的问题的子集\n树状数组（Fenwick Tree，或称 Binary Indexed Tree，BIT）是一种数据结构，支持快速的前缀和计算和单点更新操作，常用于处理数列和二维矩阵中的区间查询和更新问题。因为我们总能将[1,n]拆分成不多于logn段区间，来快速求解信息，所以树状数组的时间复杂度是 O(log⁡n)，适合处理动态数据。\n树状数组利用树的结构来存储数列的部分和，使得我们可以快速进行前缀和查询和单点更新。树状数组中的每个节点存储一部分区间的和，节点的索引使用二进制数的最低位1来确定。\n\n树状数组中，每个索引 i 存储的值代表从 i 到 i - (i &amp; -i) + 1 之间元素的和。\n\n树状数组的索引通过**最低有效位（Least Significant Bit, LSB）**进行操作，tree索引从1开始\n\n单点更新：更新数组中某个位置的值，同时更新与该位置相关的树状数组节点。\n前缀和查询：查询数组中前 i 个元素的和。\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n滑动窗口\n3个核心问题\n1. 什么时候应该扩大窗口？\n\n目标： 我们的目标是找到满足特定条件的最优窗口（可能是最大/最小长度，或者包含特定元素组合等）。\n扩大时机： 当当前窗口还不满足条件时，我们需要扩大窗口来探索更多可能性。这意味着将窗口的右边界向右移动，纳入更多元素。\n\n2. 什么时候应该缩小窗口？\n\n目标： 一旦窗口满足条件，我们就要考虑是否可以缩小窗口，以便找到更优解或排除无效解。\n缩小时机：\n\n窗口不再满足条件: 当移动左边界导致窗口不再满足条件时，我们需要立即停止缩小，并将左边界回退一步。\n优化目标: 即使窗口满足条件，我们也可能需要缩小窗口来寻找更优解。例如，寻找最小窗口时，我们会尝试缩小窗口，看看是否存在更小的满足条件的窗口。\n\n\n\n3. 什么时候应该更新答案？\n\n寻找最长解时，我们通常在扩大窗口（移动右指针）后更新答案。寻找最短解时，我们通常在缩小窗口（移动左指针）前更新答案。\n示例问题：\n\n最长问题：找出最长的无重复字符子串。\n最短问题：找出包含所有目标字符的最短子串。\n\n\n\n滑动窗口指的是这样一类问题的求解方法，在数组上通过双指针同向移动而解决的一类问题。将嵌套的循环问题，转换为单循环问题，降低时间复杂度为O（n）。\n\n内层循环 for window[c] 用来缩小窗口。虽然它看起来是嵌套在外层循环中的，但实际上每个字符只会被从窗口中移出一次。也就是说，左指针 left 最多移动 n 次。\n\n：寻找满足xx最长子串/子数组/子序列\n\n1.当不满足条件时，拓展右边界，当满足条件时，缩短左边界，最后得到一个解并暂存\n2.循环第一步，又得到一个解，将其与第一个解相对比，得到最优解并暂存，以此类推。\n最长有效括号可以用滑动窗口解决吗？\n“最长有效括号” 问题，经典的解法是用栈或动态规划，但用滑动窗口并不能很好地解决。\n为什么滑动窗口不适合？\n\n滑动窗口的本质是寻找满足条件的连续子区间。 在“最长有效括号”问题中，有效括号的子串并不一定连续。例如：“()(())” 中，最长有效括号是 “()(())”，它包含了两个独立的有效括号子串。\n滑动窗口难以处理括号的嵌套关系。 滑动窗口通常在满足某个条件时才缩小窗口，而判断括号的有效性需要考虑括号的配对关系，这在滑动窗口中难以实现。\n\nsolution ： 1.dp 2.栈  3.双向扫描\n栈写法：思路 我们需要储存最后一个不匹配的右括号下标，所以初始化stack元素为-1， 利用栈来保持对括号匹配的追踪，来处理嵌套和不连续的括号子串\n当遇到右括号时a) 如果栈不为空：\n\n这意味着我们成功匹配了一对括号。\n现在栈顶元素代表的是当前有效括号子串之前的位置。\n因此，当前下标减去栈顶元素就是当前有效括号子串的长度。\n\nb) 如果栈为空：\n\n这意味着我们遇到了一个多余的右括号，它不能与任何左括号匹配。\n在这种情况下，我们将当前下标压入栈中，作为新的起始点。\n这个操作相当于重置了计数，因为之前的所有括号都不再可能形成有效的括号串。\n\n347. 前 K 个高频元素\n最大（小）堆是指在树中，存在一个结点而且该结点有儿子结点，该结点的data域值都不小于（大于）其儿子结点的data域值，并且它是一个完全二叉树。\n对于 topk 问题：最大堆求topk小，最小堆求 topk 大。\ntopk小：构建一个 k 个数的最大堆，当读取的数小于根节点时，替换根节点，重新塑造最大堆 topk大：构建一个 k 个数的最小堆，当读取的数大于根节点时，替换根节点，重新塑造最小堆 eg. leetcode 215\n借助 哈希表 来建立数字和其出现次数的映射，遍历一遍数组统计元素的频率 维护一个元素数目为 k的最小堆why？\n每次都将新的元素与堆顶元素（堆中频率最小的元素）进行比较 如果新的元素的频率比堆顶端的元素大，则弹出堆顶端的元素，将新的元素添加进堆中 最终，堆中的 kk 个元素即为前 kk 个高频元素\n是使用小顶堆呢，还是大顶堆？\n有的同学一想，题目要求前 K 个高频元素，那么果断用大顶堆啊。\n那么问题来了，定义一个大小为k的大顶堆，在每次移动更新大顶堆的时候，每次弹出都把最大的元素弹出去了，那么怎么保留下来前K个高频元素呢。\n所以我们要用小顶堆，因为要统计最大前k个元素，只有小顶堆每次将最小的元素弹出，最后小顶堆里积累的才是前k个最大元素。\n**原地修改输入数组**\nhttps://blog.csdn.net/A233666/article/details/113956814\n如果不是原地修改的话，我们直接 new 一个 int[] 数组，把去重之后的元素放进这个新数组中，然后返回这个新数组即可。\n但是原地删除，不允许我们 new 新数组，只能在原数组上操作，然后返回一个长度，这样就可以通过返回的长度和原始数组得到我们去重后的元素有哪些了。\n这种需求在数组相关的算法题中时非常常见的，通用解法就是我们前文 双指针技巧 中的快慢指针技巧。\n我们让慢指针 slow 走在后面，快指针 fast 走在前面探路，找到一个不重复的元素就告诉 slow 并让 slow 前进一步。这样当 fast 指针遍历完整个数组 nums 后，nums[0..slow] 就是不重复元素。\n最大化最小值/ 最小化最大值问题\n基本题型: 给定n个整数序列，将其划分为m个连续子序列，求这m个子序列的和的最大化最小值 或者最小化最大值问题。 Leetcode 410\n解题思路: 二分法\n二分查找细节https://leetcode.cn/problems/binary-search/solution/er-fen-cha-zhao-xiang-jie-by-labuladong/\nwhile 中是 &lt; 还是 &lt;=?\n答：left==right时是否需要终止循环，是否找到\nint right = nums.size(); // 定义target在左闭右开的区间里，即：[left, right)        while (left &lt; right) { // 因为left == right的时候，在[left, right)是无效的空间，所以使用 &lt;\n//二分查找int binary_search(int[] nums, int target) {    int left = 0, right = nums.length - 1;    while(left &lt;= right) {        int mid = left + (right - left) / 2;        if (nums[mid] &lt; target) {            left = mid + 1;        } else if (nums[mid] &gt; target) {            right = mid - 1;        } else if(nums[mid] == target) {            // 直接返回            return mid;        }    }    // 直接返回    return -1;}\n组合数学\nCatalan 数列\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n可以应用于以下问题：\n\n\n有 2n个人排成一行进入剧场。入场费 5 元。其中只有 n个人有一张 5 元钞票，另外 n 人只有 10 元钞票，剧院无其它钞票，问有多少种方法使得只要有 10 元的人买票，售票处就有 5 元的钞票找零？\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n\n\n一位大城市的律师在她住所以北 n 个街区和以东 n 个街区处工作。每天她走 2n 个街区去上班。如果她从不穿越（但可以碰到）从家到办公室的对角线，那么有多少条可能的道路？\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n\n\n在圆上选择 2n 个点，将这些点成对连接起来使得所得到的  条线段不相交的方法数？\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n\n\n一个栈（无穷大）的进栈序列为1,2,3…n  有多少个不同的出栈序列？\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n\n\nn个结点可构造多少个不同的二叉树？\ndata:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\n\n\n\n**97. 交错字符串**\n给定三个字符串 s1、s2、s3，请你帮忙验证 s3 是否是由 s1 和 s2 **交错 组成的。\n两个字符串 s 和 t 交错 的定义与过程如下，其中每个字符串都会被分割成若干 非空 子字符串：\n\ns = s1 + s2 + ... + sn\nt = t1 + t2 + ... + tm\n|n - m| &lt;= 1\n交错 是 s1 + t1 + s2 + t2 + s3 + t3 + ... 或者 t1 + s1 + t2 + s2 + t3 + s3 + ...\n\n怎么想到DP解法而不是双指针 有感：\n斐波那契数列：\n\nDFS解法是递归地计算第 n 个斐波那契数。\n该问题可以转换为DP，通过存储每个子问题的解来减少重复计算，例如使用数组或哈希表来存储已计算的结果。\n\nNP-完全问题：\n\n一些问题，如旅行商问题（TSP）等属于NP-完全问题，DFS可能可以用来找到解，但难以以多项式时间转换为DP。\n\n大部分能暴力递归式（dfs）解决的问题就在形式上是dp的了，你只要把暴力递归式的输入参数当成状态来看待，真正的难点在于把暴力递归的状态进行压缩合并变到多项式大小的状态集合，所以不是你意识不到他是不是dp问题，而是你没有足够的经验和思路去把一个算法最简单的状态集合设计出来. When doing leetcode, using dp的难点是你就算知道了用dp，也可能想不出状态转移方程\n有时即使你意识到问题可以用DP解决，也可能遇到难以找到状态转移方程的困难。这可能需要更多的经验、练习和对问题的探索，以便设计出最优的状态集合和转移方程。\n• 动态规划可以被看作是优化后的暴力递归版本。它通常通过存储子问题的解并避免重复计算来提高效率，从而将指数级的时间复杂度降低为多项式级别。\n字符串的题 dfs 可作为一种解法。 遇到字符串(字串，子数组，子序列)题，先想DP.…\nGraph\nFloyd 算法       是用来求任意两个结点之间的最短路的。\n复杂度比较高，但是常数小，容易实现（只有三个 for）。\n适用于任何图，不管有向无向，边权正负，但是最短路必须存在。（不能有个负环）\nDijkstra 算法的基本思路。它使用优先队列来管理节点，不断选择距离源节点最近的节点，并更新与其相邻节点的距离，直到所有节点都被访问过或者最短路径已知。\n一个连通图的生成树是一个极小连通子图，它含有图中全部顶点，但只有足以构成一棵树的n-1条边。\n生成树是对应连通图来说，而生成森林是对应非连通图来说的。如果一个图有n个顶点和小于n-1条边，则是非连通图；如果它多于n-1条边，则一定有环，但有n-1条边的图不一定是生成树。\n并查集，在一些有N个元素的集合应用问题中，我们通常是在开始时让每个元素构成一个单元素的集合，然后按一定顺序将属于同一组的元素所在的集合合并，其间要查找一个元素在哪个集合中。\n并查集应用\n\n求连通分量：依次对每个边的两个顶点进行并查集合并，可以使得每个连通分量的root相同，从而得出每个连通分量。\n查找环：合并过程中，如果发现一条边的两个顶点已经合并过，说明这两个顶点之前已经通过其他路径合并，再加上这条边，图中就出现了环。\n求最小生成树：贪心思想，从小到大排序所有边，使用并查集依次合并，并跳过形成环的边，即可得到最小生成树。\n\n拓扑排序\n使用Kahn算法，实际上就是一种BFS算法。在解决有向无环图的时候比较有用。\n● 计算入度，将所有入度为0的顶点加入队列\n● 取出队列顶点，更新入度，重复操作\n回溯法+剪枝/ DFS\n带访问标记func dfs(graph [][]int, visited []bool, node int) {    if visited[node] {        return    }    visited[node] = true    // 处理当前节点    fmt.Println(node)    // 遍历相邻节点    for _, neighbor := range graph[node] {        dfs(graph, visited, neighbor)    }}\n回溯算法模板（适用于排列、组合、子集等问题）：func backtrack(choices []int, path []int, used []bool, result *[][]int) {    if /* 满足条件 */ {        *result = append(*result, append([]int{}, path...))        return    }    for i, choice := range choices {        if used[i] {            continue        }        path = append(path, choice)   //做选择        used[i] = true        backtrack(choices, path, used, result)          // 撤销选择（回溯）        path = path[:len(path)-1]        used[i] = false    }}\n这一类问题都需要先画出树形图，然后编码实现。\n编码通过 深度优先遍历 实现，使用一个列表，在 深度优先遍历 变化的过程中，遍历所有可能的列表并判断当前列表是否符合题目的要求\n如果题目要求，结果集不计算顺序，此时需要按顺序搜索，才能做到不重不漏。「力扣」第 47 题（ 全排列 II ）、「力扣」第 15 题（ 三数之和 ）也使用了类似的思想，使得结果集没有重复。\nRecursive algorithms can be both in-place and not-in-place, depending on how they are implemented. In computer science, an “in-place” algorithm is one that uses a constant amount of extra memory or auxiliary data structures to perform its operations, regardless of the size of the input data. On the other hand, a “not-in-place” algorithm uses additional memory that grows with the input size. 因此，虽然精确的空间复杂度分析（O(1)、O(n) 等）对于理论讨论和比较很有用，但现实世界的考虑通常会导致对算法的内存使用情况进行更细致的评估。目标是在空间效率和算法简单性之间取得平衡，使代码更容易理解和维护，同时仍然实现可接受的性能。\n树和二叉树\n两种递归本质的理解也殊途同归一下：\n自顶向下的递归和自底向上的递归是两种不同的思考方式，它们在设计和实现递归算法时各有优缺点。自顶向下的方法通常更直观（n！），更容易理解，因为它与“分而治之”的策略相似。然而，对于非常大的问题，它可能会导致大量的重复计算，因为每个子问题都可能需要被多次解决。\n可以用 记忆化搜索（Memoization）：通过记录已经计算过的子问题的结果，避免重复计算，从而提升效率。\n自底向上的方法则从基本情况出发（Fibonacci），通过组合基本情况来解决更复杂的问题，这有助于减少重复计算。然而，它可能需要更多的初始步骤来确定问题的边界和基本情况。了解这两种方法可以帮助我们根据具体问题选择合适的实现方式。\n\n树本身是一种简单化的图\n自顶向下/自下向上本质上对应着dfs（深度优先）/bfs（广度优先）\n\n\n先序遍历（前序遍历）：先访问当前节点，然后递归地访问左子树和右子树。它是DFS的一种体现，常用于创建复制树、输出树结构等。\n中序遍历：先访问左子树，然后访问当前节点，最后访问右子树。兼具 DFS 和 BFS 的思想，在二叉搜索树中，它能得到有序的结果（类似 BFS 的层次感），但遍历过程仍然是深度优先的。\n后序遍历：先递归地访问左子树和右子树，然后访问节点本身。这不完全对应BFS。后序遍历常用于先处理子节点再处理父节点的情形，如树形DP、计算树的高度等，其实更倾向于DFS的一种应用。\n\n快速排序与二叉树的前序遍历类比：\n快速排序的过程可以类比为二叉树的前序遍历，因为快速排序通过选取一个基准值（pivot），将数组分为两部分，并递归地对子数组进行排序。这个过程可以类比为前序遍历，先处理当前节点（即当前的基准值），然后递归地处理左右子树（较小和较大的元素）。\n归并排序与二叉树的后序遍历类比：\n归并排序的过程可以类比为二叉树的后序遍历，因为归并排序将数组递归地分割为更小的子数组，然后合并这些子数组。这个过程类似于后序遍历，先递归地处理左右子树（将数组分割为更小的部分），然后在递归回溯时进行合并操作（将两个有序的子数组合并为一个有序的数组）。\n虽然这种类比有助于理解快速排序和归并排序的工作原理\n动归/DFS/回溯算法都可以看做二叉树问题的扩展，只是它们的关注点不同：\n\n动态规划：动态规划是一种将问题分解为子问题，并以自底向上的方式解决的方法。每个子问题的解决方案被记录下来，以避免重复计算。你可以将动态规划问题视为填充一张二维表格，其中每个格子代表一个子问题的解，从而形成一种树状的结构。这与二叉树的概念有些类似。\n回溯：回溯是一种深度优先的搜索方法，通常用于解决排列、组合、子集等问题。你可以将回溯过程看作在一个决策树上的遍历，每个节点代表一个选择，通过遍历树上的路径来寻找解。你的理解关于回溯关注于节点间的「树枝」是正确的。\n\n回溯算法是系统地搜索问题的解的方法。\n某个问题的所有可能解的称为问题的解空间，若解空间是有限的，则可将解空间映射成树结构。\n任何解空间可以映射成树结构的问题，都可以使用回溯法。\n回溯法是能够在树结构里搜索到通往特定终点的一条或者多条特定路径。\n回溯算法的基本思想是：从一条路往前走，能进则进，不能进则退回来，换一条路再试，从而搜索到抵达特定终点的一条或者多条特定路径。\n值得注意，回溯法以深度优先搜索的方式搜索解空间，并且在搜索过程中用剪枝函数避免无效搜索。\n\nDFS：深度优先搜索是一种遍历图或树的方法。它从一个起始节点出发，沿着一个路径一直向下遍历，直到无法继续为止，然后回溯并探索其他分支。你的理解关于DFS关注于单个「节点」也是正确的。\n\n如上面所言，深度优先搜索是特定于图结构的一种搜索算法，回溯算法是特定于树结构的搜索算法。\n遍历\n\n中序遍历：递归，栈，移动右子树使用pre指针遍历\nMorris 遍历是一种不使用递归和栈，而是利用线索二叉树（Threaded Binary Tree）的思想来实现二叉树的遍历，包括中序遍历、前序遍历和后序遍历。Morris 遍历的优点在于它使用的**空间复杂度是 O(1)，**并且不会破坏原来的树结构。（leetcode99\nMorris 遍历算法的关键在于如何建立临时的线索连接，从而在遍历过程中完成左右子树节点的跳转，而不需要额外的空间。\n\n这个算法的核心思想是在遍历过程中修改树的结构，将节点的右子树指向后继节点，然后再恢复树的结构，以便能够顺序遍历节点。这种方法在空间效率上具有显著优势，但需要小心处理节点的指针，以避免陷入无限循环。\nBST AVL\nBST二叉查找树（排序树），若它的左子树不空，则左子树上所有的结点的值均不大于它根结点的值；　　若它的左子树不空，则左子树上所有的结点的值均不小于它根结点的值；\n最重要的性质是：二叉搜索树的中序遍历是有序的\n当节点数目一定，保持树的左右两端保持平衡，树的查找效率最高。\n这种左右子树的高度相差不超过 1 的树为平衡二叉树。\n性质：\n\n可以是空树。\n假如不是空树，任何一个结点的左子树与右子树都是平衡二叉树，高度之差的绝对值不超过 1 。在一棵平衡二叉树中，节点的平衡因子只能取 0 、1 或者 -1 ，分别对应着左右子树等高，左子树比较高，右子树比较高。\n\n以递归解决二叉树这种对称数据结构的策略，称为对称性递归。可以用**对称性递归解决的二叉树问题大多是判断性问题(bool类型函数),**这一类问题又可以分为以下两类：https://leetcode.cn/problems/shu-de-zi-jie-gou-lcof/solution/yi-pian-wen-zhang-dai-ni-chi-tou-dui-che-uhgs/\n1、不需要构造辅助函数。这一类题目有两种情况：第一种是单树问题，且不需要用到子树的某一部分(比如根节点左子树的右子树)，只要利用根节点左右子树的对称性即可进行递归。第二种是双树问题，即本身题目要求比较两棵树，那么不需要构造新函数。该类型题目如下：\n\n相同的树 翻转二叉树\n二叉树的最大深度\n平衡二叉树\n二叉树的直径\n合并二叉树 另一个树的子树 单值二叉树\n\n2、需要构造辅助函数。这类题目通常只用根节点子树对称性无法完全解决问题，必须要用到子树的某一部分进行递归，即要调用辅助函数比较两个部分子树。形式上主函数参数列表只有一个根节点，辅助函数参数列表有两个节点。该类型题目如下：\n\n对称二叉树 剑指 Offer 26. 树的子结构\n\n100. 相同的树，并注意与这道题的区别：剑指 Offer 26. 树的子结构。与字符串对比的话，子树就相当于字符串的子串（要求连续），树的子结构就相当于字符串的子序列（不要求连续）\n为什么还需要非线性结构呢？ 答案是为了高效地兼顾静态操作和动态操作，我们一般使用树去管理需要大量动态操作的数据\n堆排序的基本思想是先将待排序的序列构建成一个堆，然后依次从堆顶取出最值（最大值或最小值），将其与堆的最后一个元素交换，并将堆的大小减一，然后再通过一系列操作使得剩余的元素重新构建成一个堆。重复执行此过程，直到堆为空，从而得到一个有序的序列。\n堆排序的主要步骤如下：\n\n构建初始堆：将待排序序列构建成一个初始堆，即满足堆的特性。\n交换和调整：将堆顶元素与堆的最后一个元素交换位置，并将堆的大小减一。然后通过向下调整（或向上调整）操作，使剩余元素重新构建成一个堆。\n重复执行步骤2，直到堆为空。\n\n由于完全二叉树的性质，堆排序可以高效地在数组中进行操作，因为堆的结构可以直接映射到数组的索引上，不需要显式使用指针。2i, 2i+1\n垂直遍历leetcode.314\n   3  /\\ /  \\ 9  20    /\\   /  \\  15   7  输入： {3,9,20,#,#,15,7}输出： [[9],[3,15],[20],[7]]public List&lt;List&lt;Integer&gt;&gt; verticalOrder(TreeNode root) {        // Write your code here        List&lt;List&lt;Integer&gt;&gt; results = new ArrayList&lt;&gt;();        if (root == null) {            return results;        }        Map&lt;Integer, List&lt;Integer&gt;&gt; map = new TreeMap&lt;Integer, List&lt;Integer&gt;&gt;();        Queue&lt;Integer&gt; qCol = new LinkedList&lt;&gt;();        Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;();        queue.offer(root);        qCol.offer(0);        while(!queue.isEmpty()) {            TreeNode curr = queue.poll();            int col = qCol.poll();            if(!map.containsKey(col)) {                map.put(col, new ArrayList&lt;Integer&gt;(Arrays.asList(curr.val)));            } else {                map.get(col).add(curr.val);            }            if(curr.left != null) {                queue.offer(curr.left);                qCol.offer(col - 1);            }            if(curr.right != null) {                queue.offer(curr.right);                qCol.offer(col + 1);            }        }        for(int n : map.keySet()) {            results.add(map.get(n));        }        return results;    }\n字典序（lexicographical order）是对字符串或序列的一种排序方式，类似于字典中单词的排列顺序。字典序的应用广泛，尤其在字符串处理和排序相关的领域。前缀树（Trie）是一种用于高效存储和检索字符串的树形数据结构，它与字典序有紧密的关系。以下是字典序的应用及其与前缀树的关系：\n字典序的应用\n\n排序：\n\n在编程中，许多排序算法（如快速排序、归并排序）都可以用于对字符串数组按字典序进行排序。\n数据库索引常常基于字典序来快速查找记录。\n\n\n字符串匹配：\n\n在查找字符串或子字符串时，可以利用字典序来优化搜索过程。\n\n\n字典序列（Lexicographic Permutations）：\n\n生成某个字符串的所有排列并按字典序排序。在某些算法问题中，这种排列方式有助于系统地探索所有可能的情况。\n\n\n自动补全和拼写检查：\n\n在输入法、搜索引擎和文本编辑器中，按字典序列出的自动补全选项和拼写建议，提高用户体验。\n\n\n\n前缀树（Trie）与字典序的关系\n字典序在字符串处理和排序中有广泛的应用，前缀树是一种高效的数据结构，特别适用于处理字典序相关的问题。通过前缀树，可以快速进行字符串的前缀匹配、自动补全和按字典序的遍历，有助于提升各种应用的性能和效率。\n\n结构：\n\n前缀树的每个节点表示一个字符，从根节点到某个节点的路径表示一个字符串。\n前缀树按字典序排列子节点，即从根节点遍历时，子节点按字符的字典序排列。\n\n\n\n前缀和构造多叉树\n前缀和处理数组区间问题，快速得到某个子数组的和\nTrie（发音类似 “try”）或者说 前缀树 字典树是一种树形数据结构，用于高效地存储和检索字符串数据集中的键。这一数据结构有相当多的应用情景，例如自动补完和拼写检查。\n前缀和是一种重要的预处理，能大大降低查询的时间复杂度。我们可以简单理解为“数列的前 n 项的和”。这个概念其实很容易理解，即一个数组中，第 n 位存储的是数组前 n 个数字的和。\n通过一个例子来进行说明会更清晰。题目描述：有一个长度为 N 的整数数组 A，要求返回一个新的数组 B，其中 B 的第 i 个数 B[i]是原数组 A 前 i 项和。\n一共有几个和为 k 的子数组。给定一个二进制数组 nums , 找到含有相同数量的 0 和 1 的最长连续子数组，并返回该子数组的长度。\n比如说，我给你输入一个数组 nums，然后又要求给区间 nums[2..6] 全部加 1，再给 nums[3..9] 全部减 3，再给 nums[0..4] 全部加 2，再给…\n一通操作猛如虎，然后问你，最后 nums 数组的值是什么？\n本文讲一个和前缀和思想非常类似的算法技巧「差分数组」，差分数组的主要适用场景是频繁对原始数组的某个区间的元素进行增减。\n原数组就是差分数组的前缀和\n2559. 统计范围内的元音字符串数\n给你一个下标从 0 开始的字符串数组 words 以及一个二维整数数组 queries 。\n每个查询 queries[i] = [li, ri] 会要求我们统计在 words 中下标在 li 到 ri 范围内（包含 这两个值）并且以元音开头和结尾的字符串的数目。\n返回一个整数数组，其中数组的第 i 个元素对应第 i 个查询的答案。\n**注意：**元音字母是 'a'、'e'、'i'、'o' 和 'u' 。\nhttps://leetcode.cn/problems/count-vowel-strings-in-ranges/\n2575. 找出字符串的可整除数组\n给你一个下标从 0 开始的字符串 word ，长度为 n ，由从 0 到 9 的数字组成。另给你一个正整数 m 。\nword 的 可整除数组 div  是一个长度为 n 的整数数组，并满足：\n\n如果 word[0,...,i] 所表示的 数值 能被 m 整除，div[i] = 1\n否则，div[i] = 0\n\n返回 **word 的可整除数组。\n思路： 如何想到递归求模\nfirst：\n\n(a + b) % p = (a % p + b % p) % p （1）\n(a - b) % p = (a % p - b % p) % p （2）\n(a * b) % p = (a % p * b % p) % p （3）\na ^ b % p = ((a % p)^b) % p （4）\n\nsecond：\n\n记N[i]为word[0 ~ i]表示的值。\n记n[i]为word[i]表示的数。\n不难得出 : N[i] = N[i - 1] * 10 + n[i]\n在此假设 : N[i - 1] = p * m + q(即余数是q)\n那么 : N[i] % m = (p * m * 10) % m + (q * 10 + n[i ] ) % m\n其中 : (p * m * 10) % m必能整除, 因此只要看后半部分\n\n补码（英语：2’s complement）是一种用二进制表示有符号数的方法，也是一种将数字的正负号变号的方式，常在计算机科学中使用。补码以有符号比特的二进制数定义。\n正数和0的补码就是该数字本身再补上最高比特0。负数的补码则是将其绝对值按位取反再加1。\n补码系统的最大优点是可以在加法或减法处理中，不需因为数字的正负而使用不同的计算方式。只要一种加法电路就可以处理各种有号数加法，而且减法可以用一个数加上另一个数的补码来表示，因此只要有加法电路及补码电路即可完成各种有号数加法及减法，在电路设计上相当方便。几乎所有现代CPU都使用补码来表示和处理整数。这包括 x86, ARM, MIPS 等各种架构。\n另外，补码系统的0就只有一个表示方式，这和反码系统不同（在反码系统中，0有二种表示方式），因此在判断数字是否为0时，只要比较一次即可。\n右侧的表是一些8-bit补码系统的整数。它的可表示的范围包括-128到127，总共258个整数。n位系统，范围[-2^(n-1),2^(n-1)-1].\n例如：10/3=10-3-3-3=1mod3 而减法又可做补码相加，所以所有四则运算的基础都是由加法而来。\n以8位二进制数为例，A-B=A-B+256=A+(255-B+1)，11111111-B就是二进制取反，这个式子解释了为什么补码等于反码+1.\n对于一个 n 位二进制数x，其补码表示是(2^n - x)\n求-5的补码表示：1. 按位取反然后加一\n2. (256-5) mod 256\n\n251 的二进制表示是：11111011\n\n平方根\n历史上至少有过两个问题，它们看起来非常困难，非常不像 P 问题，但在人们的不懈努力之下，最终还是成功地加入了 P 问题的大家庭。其中一个是线性规划（linear programming），它是一种起源于二战时期的运筹学模型。1947 年，乔治·丹齐格（George Dantzig）提出了一种非常漂亮的算法叫作“单纯形法”（simplex algorithm），它在随机数据中的表现极为不错，但在最坏情况下却需要耗费指数级的时间。因此，很长一段时间，人们都在怀疑，线性规划是否有多项式级的算法。直到 1979 年，人们才迎来了线性规划的第一个多项式级的算法，它是由前苏联数学家列昂尼德·哈奇扬（Leonid Khachiyan）提出的。\n另外一个问题则是质数判定问题（primality test）：判断一个正整数是否是质数（prime），或者说判断一个正整数是不是无法分成两个更小的正整数之积。人们曾经提出过各种质数判定的多项式级算法，但它们要么是基于概率的，要么是基于某些假设的，要么是有一定适用范围的。2002 年，来自印度理工学院坎普尔分校的阿格拉瓦尔（M. Agrawal）、卡亚勒（N. Kayal）和萨克斯泰纳（N. Saxena）发表了一篇重要的论文《PRIMES is in P》，给出了第一个确定性的、时间复杂度为多项式级别的质数判定算法，质数判定问题便也归入了 P 问题的集合。很容易看出，找出一个多项式级的答案验核算法，再怎么也比找出一个多项式级的答案获取算法更容易。\n为了练习函数与循环，判断一个数是否为质数：我们来实现一个平方根函数：用牛顿法实现平方根函数。\n\n法一：牛顿迭代法的本质是借助泰勒级数，从初始值开始快速向零点逼近。\n计算机通常使用循环来计算 x 的平方根。从某个猜测的值 z 开始，我们可以根据 z² 与 x 的近似度来调整 z，产生一个更好的猜测：\nz -= (z*z - x) / (2*z)    long c=x;        while(c*c&gt;x){            c = (c+x/c)/2;//怎么得出来的？        }    return (int)c;\n法二：二分查找逼近\n在数学中，数根(又称位数根或数字根Digital root)是自然数的一种性质，换句话说，每个自然数都有一个数根。数根是将一正整数的各个位数相加（即横向相加），若加完后的值大于10的话，则继续将各位数进行横向相加直到其值小于十为止[1]，或是，将一数字重复做数字和，直到其值小于十为止，则所得的值为该数的数根。\n例如54817的数根为7，因为5+4+8+1+7=25，25大于10则再加一次，2+5=7，7小于十，则7为54817的数根。\n数根是一种对数值进行处理的方法，它可以帮助我们快速计算数字的特性，如同余、整除性等，并且可以作为验证计算正确性的方法。主要用途如下：\n\n计算模运算的同余：\n\n通过计算数根，可以方便地进行模运算的同余判断。在处理大数字时，数根可以简化计算，节省时间和计算资源。\n\n\n验证计算正确性：\n\n数字的数根可以用作验证计算结果的正确性的方法。例如，通过对两个数字的数根进行运算，可以检查它们的和的数根是否等于原数字的数根之和。\n\n\n判断整除性：\n\n数根能够帮助判断一个数是否能被3或9整除。如果一个数字的数根能被3或9整除，那么原始的数字也能被3或9整除。\n\n\n\n堆的三种操作：\n1删除堆顶元素的方法： 常见操作是用数组尾部元素替换堆顶，这里不直接删除堆顶，因为所有的元素会向前移动一位，会破坏了堆的结构\n然后进行下移操作，将新的堆顶和它的子节点进行交换，直到子节点大于等于这个新的堆顶，删除堆顶的时间复杂度为O(logk)\n2堆化：就是将任意数组调整为堆的结构。\n\n任意数组都可以看做一颗完全二叉树\n从当前这个完全二叉树的最后一个非叶子节点开始进行元素下沉（siftDown）操作，逐步将这颗二叉树调整为堆结构     buildHeap 第二种 从堆的顶部（数组的开头）开始，并对每个项目调用 siftUp。\n\n3 插入\n\n法一：交换法\n法三:哨兵法\n\nimport java.util.ArrayList;public class Heap {  private ArrayList&lt;Integer&gt; data;  private boolean isMaxHeap;  public Heap(boolean isMaxHeap) {      data = new ArrayList&lt;&gt;();      this.isMaxHeap = isMaxHeap;  }// 建堆public void buildHeap(int[] arr) {    for (int num : arr) {        data.add(num);    }    for (int i = parent(data.size() - 1); i &gt;= 0; i--) {        siftDown(i);    }}// 插入 在堆中插入新元素后维护堆的性质public void insert(int num) {    data.add(num);    siftUp(data.size() - 1);}// 删除最大值public int delete() {    int res = data.get(0);    data.set(0, data.get(data.size() - 1));    data.remove(data.size() - 1);    siftDown(0);    return res;}// 查询最大/小值public int peek() {    return data.get(0);}private void siftUp(int i) {    while (i &gt; 0 &amp;&amp; data.get(parent(i)).compareTo(data.get(i)) &gt; 0) {        swap(i, parent(i));        i = parent(i);    }}private void siftDown(int i) {    int maxIndex = i;    int left = leftChild(i);    if (left &lt; data.size() &amp;&amp; compare(data.get(left), data.get(maxIndex)) &gt; 0) {        maxIndex = left;    }    int right = rightChild(i);    if (right &lt; data.size() &amp;&amp; compare(data.get(right), data.get(maxIndex)) &gt; 0) {        maxIndex = right;    }    if (i != maxIndex) {        swap(i, maxIndex);        siftDown(maxIndex);    }}对于任意节点 i，其左子节点的索引为 2i+1，右子节点的索引为 2i+2，而父节点的索引为 floor((i-1)/2)。private int parent(int i) {    return (i - 1) / 2;}private int leftChild(int i) {    return 2 * i + 1;}private int rightChild(int i) {    return 2 * i + 2;}private void swap(int i, int j) {    int temp = data.get(i);    data.set(i, data.get(j));    data.set(j, temp);}private int compare(int a, int b\nQuicksort基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。\n func qsort(a []int, rng *rand.Rand) {    if len(a) &lt; 2 {        return    }    left, right := 0, len(a)-1    // 相当于分区    pivotIndex := rng.Intn(len(a))    // Move the pivot to the right    a[pivotIndex], a[right] = a[right], a[pivotIndex]    // Pile elements smaller than the pivot on the left    for i := range a {        if a[i] &lt; a[right] {            a[i], a[left] = a[left], a[i]            left++        }    }    // Place the pivot after the last smaller element    a[left], a[right] = a[right], a[left]    // Go down the rabbit hole    qsort(a[:left], rng)    qsort(a[left+1:], rng)}func main() {    rng := rand.New(rand.NewSource(time.Now().UnixNano()))    arr := []int{3, 6, 8, 10, 1, 2, 1}    qsort(arr, rng)    fmt.Println(arr) // Output: [1 1 2 3 6 8 10]}      //三路快排 当序列中有大量的重复元素，二路排序虽然会均衡的分配到两个序列中，但是重复元素仍然参与到分割序列中，带来无谓的性能损耗。 public void sort(int[] args, int l, int r) {        if (l &gt;= r) {            return;        }    int target = args[l];    int lt = l, gt = r + 1, i = l + 1;    while (i &lt; gt) {        if (args[i] &lt; target) {            swap(args, i, lt + 1);            lt++;            i++;        } else if (args[i] &gt; target) {            swap(args, i, gt - 1);            gt--;        } else {            i++;        }    }    swap(args, l, lt);    sort(args, l, lt - 1);    sort(args, gt, r);    }    //快速选择算法public int findKthLargest(int[] nums, int k) {        int left = 0;        int right = nums.length - 1;        int pivot = partition(nums, left, right);        // 从小到大排序，倒数第k个就是第k个最大元素        int targetIndex = nums.length - k;        while (true) {            if (pivot == targetIndex) {                return nums[pivot];            } else if (pivot &gt; targetIndex) {                right = pivot - 1;            } else {                left = pivot + 1;            }            pivot = partition(nums, left, right);        }    }    int partition(int[] nums, int left, int right) {        // 基准值索引        int pivot = left;        // 预放值索引        int index = left + 1;        // 将比基准值小的都紧挨着基准值        for (int i = index; i &lt;= right; i++) {            if (nums[i] &lt; nums[pivot]) {                swap(nums, i, index);                index++;            }        }        // 把基准值移动到比基准值小的数列的最右边        swap(nums, pivot, index - 1);        return index - 1;    }\n快速选择算法过一次遍历，确定某一个元素在排序以后的位置，这个算法叫「快速选择」。要理解「快速选择」算法，必须先理解「快速排序」的「partition」。 快速排序会递归处理划分的两边，而选择只处理划分一边。\n\nLRUcache\n解法\n1.LinkedHashmap。伪头部（dummy head）和伪尾部（dummy tail）标记界限，这样在添加节点和删除节点的时候就不需要检查相邻的节点是否存在。 当向缓存中添加新项时，如果达到容量限制，将删除链表头部（最少使用）的节点，并更新哈希表。\n使用双向链表加哈希表维护get 和 put  O(1) 平均时间复杂度\n2.自己实现LinkedHashmap\n\nLRU 的功能可以使用双向链表实现，访问到的节点移动到头部，超出容量的从尾部删除。\n要实现O(1)得使用HashMap，里面储存 key 与 链表节点即可，这样可以快速定位节点，然后删除它，将它移动到链表头部。\n\n3.O（n）解法 好写\n\n使用HashMap来存储键值对，其中键是缓存的键，值是对应的缓存项。\n使用ArrayList来维护最近访问的顺序，最近访问的项位于列表的末尾。\n当访问缓存项时，将其移到ArrayList的末尾，以表明它是最近访问的。\n当缓存达到容量限制时，淘汰最近最少使用的项（即ArrayList的头部项）。\n\n• ArrayList 中的移除和添加操作是线性时间复杂度 O(n)。但由于在缓存中存储的是键，因此在 ArrayList 中查找和移除元素的复杂度可以视为 O(n)。\nLFU 的缓存污染问题：\nLFU 通常根据缓存中条目的访问频率来替换最不经常使用的条目。但是，LFU 在某些情况下可能出现“缓存污染”问题：\n新数据问题： 当一些数据被频繁访问但实际上不是常用数据时，它们的频率计数会增加，导致 LFU 将其视为常用数据，长时间占据缓存空间。\n突发事件问题： 在短时间内，某些数据可能会因为特定事件而被频繁访问，这些数据的频率计数可能会暂时性地高于实际常用数据。\nLRU 的长环模式问题：\nLRU 根据最近最少使用的原则进行缓存替换，但存在一个“长环模式”问题：\n周期性访问模式： 如果存在一组数据被周期性地访问（例如，每隔一段时间访问一次），而且这组数据的访问顺序与 LRU 的缓存替换顺序一致，就会导致这些数据在缓存中形成长环。\n替换不及时： 长环中的数据虽然可能在某段时间内并不频繁使用，但由于周期性访问模式，它们的位置始终位于 LRU 缓存替换算法的末尾，因此不容易被替换出去。\n解决方法：\n针对这些问题，可以考虑使用一些改进的缓存算法或结合其他策略来提高缓存效率，如：\nLFU 的改进版本： 可以考虑采用 LFU 的变体算法，如动态调整频率计数的算法，以更准确地反映数据的实际热度。\nLRU 的改进版本： 引入时间衰减机制，使得长时间不被访问的数据在一定时间后被逐渐淘汰，而不仅仅依赖于访问顺序。\n混合替换策略： 使用两种或多种不同的缓存替换策略组合，根据具体情况动态选择合适的替换策略。\n在可计算性理论与计算复杂性理论中，所谓的归约是将某个计算问题变换为另一个问题的过程。可用归约法定义某些问题的复杂度类（因变换过程而异）。P NP\n以直觉观之，如果存在能有效解决问题B的算法，也可以作为解决问题A的子程序，则将问题A称为“可归约”到问题B，因此求解A并不会比求解B更困难。\nA(x) = B(f(x)), in other words, for x∈A，there’s f(x)∈B\n","categories":["计算机科学"],"tags":["编程","算法","CS基础"]},{"title":"操作系统 I/O 多路复用，select / poll / epoll","url":"/2024/09/15/IOandEpoll/Epoll/","content":"在现代计算机系统中，I/O 操作是影响性能的关键因素，尤其是在网络服务和本地存储密集型应用中。你是否曾经思考过网络 I/O 和本地 I/O 之间的性能差异？为什么网络传输总是比本地硬盘慢？\n函数select()、poll()和epoll()在网络编程和操作系统中用于监视多个文件描述符，以查看其中任何一个是否可进行 I/O（例如，套接字已准备好进行读/写）。这些主要用于事件驱动编程，特别是在服务器中同时处理多个连接时。那么，epoll 到底为什么如此高效？它如何超越传统的 select() 和 poll()？通过这篇讨论，你将深入了解为什么 epoll 是大规模网络 I/O 管理的首选，以及如何在实践中最大化其优势。\n在操作系统中，程序运行的空间分为内核空间和用户空间， 用户空间所有对io操作的代码（如文件的读写、socket的收发等）都会通过系统调用进入内核空间完成实际的操作。\n阻塞 I/O\n在阻塞 I/O 模式下，系统调用（如 read() 或 recvfrom()）会一直等待，直到内核把数据准备好并传输到用户空间。此期间，线程会被挂起，CPU将被释放去做其他事情，但这个线程无法执行任何其他任务，直到 I/O 操作完成。这种情况会导致线程阻塞，尤其在高并发环境下，每个 I/O 操作都可能耗费大量时间等待。在linux中，默认情况下所有的socket都是阻塞的，一个典型的读操作流程大概是这样：\n当用户进程调用了 read()/recvfrom() 等系统调用函数，它会进入内核空间中，当这个网络I/O没有数据的时候，内核就要等待数据的到来，而在用户进程这边，整个进程会被阻塞，直到内核空间返回数据。当内核空间的数据准备好了，它就会将数据从内核空间中拷贝到用户空间，此时用户进程才解除阻塞的的状态，重新运行起来。\n所以，阻塞I/O的特点就是在IO执行的两个阶段（用户空间与内核空间）都被阻塞了。\n非阻塞 I/O\n非阻塞 I/O 则允许程序发起 I/O 操作（例如 read() 或 recvfrom()），即使数据还没准备好，内核也不会让进程挂起，而是立即返回。此时用户进程得到的是一个错误，提示数据尚未准备好。用户进程需要不断轮询，重复调用 I/O 操作直到数据可用。这种方式避免了线程被阻塞的情况，但频繁的轮询可能会导致 CPU 使用效率低下。\n多路复用 I/O\n多路复用 I/O (select()，poll()，epoll()) 允许单个线程监控多个文件描述符（如 socket），从而在多个 I/O 操作之间进行复用。它的基本原理是通过轮询多个 socket，查看是否有数据到达，并在有数据到达时通知应用程序。相比非阻塞 I/O，这种机制更高效，特别适合于处理大量连接的服务器。\n本地 I/O 操作通常为阻塞模式，即在进行 I/O 操作时，进程会被挂起直到操作完成。然而，现代操作系统也提供了非阻塞本地 I/O 或异步 I/O（如 Linux 的 aio 或 Windows 的 IOCP），允许应用程序继续执行其他任务而不等待 I/O 完成。网络 I/O 同样可以是阻塞的或非阻塞的。在非阻塞网络 I/O 中，进程发起 I/O 操作时不需要等待数据返回，可以继续执行其他操作。网络 I/O 领域中，异步 I/O、多路复用 I/O（如 select()、poll()、epoll()）、以及事件驱动编程（如 libuv、libevent）等技术被广泛使用，以应对高延迟和并发问题。\n将协程与IO多路复用结合起来，可以实现高性能的并发IO程序。socket 是 Unix 中的术语。socket 可以用于同一台主机的不同进程间的通信，也可以用于不同主机间的通信。一个 socket 包含地址、类型和通信协议等信息，通过 socket() 函数创建：\nint socket(int domain, int type, int protocol)\n返回的就是这个 socket 对应的文件描述符 fd。操作系统将 socket 映射到进程的一个文件描述符上，进程就可以通过读写这个文件描述符来和远程主机通信。\n可以这样理解：socket 是进程间通信规则的高层抽象，而 fd 提供的是底层的具体实现。socket 与 fd 是一一对应的。通过 socket 通信，实际上就是通过文件描述符 fd 读写文件。这也符合 Unix“一切皆文件”的哲学。使用IO多路复用技术监听多个socket，当有socket可读或可写时，通过事件通知，激活相应的协程进行处理。\n多路复用 I/O和协程关系?\n这个问题问得好，实际上“多路复用 I/O（select/poll/epoll）”和“协程（coroutine）”是两个不同层次的机制，但它们在现代并发系统（尤其是 Go、Rust、Python async/await、libuv、Nginx 等）中是深度绑定的。我们可以从底层原理和调度模型两个维度看。协程（英语：coroutine）是计算机程序的一类组件，推广了协作式多任务的子例程，允许执行被挂起与被恢复,协程可以通过yield（取其“退让”之义而非“产生”）来调用其它协程，接下来的每次协程被调用时，从协程上次yield返回的位置接着执行，通过yield方式转移执行权的协程之间不是调用者与被调用者的关系，而是彼此对称、平等的。\n\n举个最直接的例子：Go 语言\nGo 的协程（goroutine）模型是一个典型代表：\nfunc handler(conn net.Conn) {    buf := make([]byte, 1024)    for {        n, err := conn.Read(buf)        if err != nil { return }        conn.Write(buf[:n])    }}func main() {    ln, _ := net.Listen(\"tcp\", \":8080\")    for {        conn, _ := ln.Accept()        go handler(conn)    }}\n表面上好像每个 conn 都是阻塞读写的，但实际上：\n\n\nGo runtime 在底层为每个 socket 注册到 epoll；\n\n\n当 goroutine 调用 Read() 时，如果没有数据可读，它会：\n\n把当前 goroutine 状态挂起；\n把 fd 注册到 epoll；\n调度其他 goroutine；\n当 epoll 返回“可读事件”，再恢复该 goroutine。\n\n\n\n整个过程完全用户态切换，线程没有被阻塞。\n所以你可以同时开 10w goroutine，因为系统线程数只维持在几十个（M:N 模型）。\nPython asyncio / Node.js / Rust tokio 同理\n\n\n\n框架\n底层 I/O 多路复用\n用户态调度机制\n\n\n\n\nPython asyncio\nepoll/kqueue/select via selectors 模块\nevent loop 调度 coroutine\n\n\nNode.js (libuv)\nepoll/kqueue/iocp\n单线程 event loop\n\n\nRust tokio\nepoll/kqueue/wepoll\nFuture + reactor 模型\n\n\nGo runtime\nepoll/kqueue/iocp\nM:N goroutine 调度器\n\n\n\n这些 runtime 全都在做同一件事：\n\n用 epoll 等来统一管理 I/O；\n当协程遇到 I/O 阻塞点时，自动 yield；\nI/O 就绪后再 resume。\n\n\n核心关系图（简化）\n   用户代码层┌──────────────────────────────┐│ async/await / goroutine / go ││ yield / await / resume       │└──────────────────────────────┘              │              ▼   runtime 调度器 (用户态)┌──────────────────────────────┐│ epoll_wait()                 ││ 回调、事件队列、任务队列     │└──────────────────────────────┘              │              ▼       内核 I/O 层┌──────────────────────────────┐│ socket, file, pipe, epoll_fd ││ 驱动中断、DMA、网卡队列      │└──────────────────────────────┘\n协程只是事件循环上的“任务单元”；\n多路复用是驱动事件循环的“信号源”。\n\n比如 Nginx 用 epoll+回调，Go/Netty 用 epoll+协程，性能上都是百万级连接可行。\n\n1.select()\n用法：允许监视多个文件描述符，以查看它们是否已准备好读取、写入或是否有错误。\n限制：\n具有可监控的最大文件描述符数量（通常为 1024）。此数量由 定义FD_SETSIZE，在高并发应用程序中可能会受到限制。\n当有大量文件描述符时效率低下，因为它每次都必须检查每个描述符。\n使用单一数据结构来保存读取、写入和错误描述符。\n例子：\nfd_set readfds; FD_ZERO(&amp;readfds); FD_SET(socket_fd, &amp;readfds); select(socket_fd + 1, &amp;readfds, NULL, NULL, &amp;timeout);\n2.poll()\npoll 和 select 几乎没有区别。poll 在用户态通过数组方式传递文件描述符，在内核会转为链表方式存储，没有最大数量的限制 int poll(struct pollfd *fds, nfds_t nfds, int timeout);\n好处：\nstruct pollfd使用比 使用的位掩码更具可扩展性的数组select()。引用 select 手册页：“select() 只能监视小于 FD_SETSIZE 的文件描述符数量；poll(2) 没有这个限制。”所以这可能是选择 poll 而不是 select 的一个原因。可以更轻松地处理大量文件描述符。\n例子：\nstruct pollfd fds[2]; fds[0].fd = socket_fd; fds[0].events = POLLIN; poll(fds, 2, timeout);\n3. epoll()\nepoll 是 Linux 2.5.44（2002年）引入的 I/O 多路复用机制，专为高并发设计。\nepoll 并非单一系统调用，而是由三个接口组成的高效事件通知机制：\n\nepoll_create1()：创建一个 epoll 实例，返回一个 epoll 文件描述符（epfd）；\nepoll_ctl()：向 epoll 实例中注册、修改或删除需要监听的 fd 及其关注的事件（如 EPOLLIN、EPOLLOUT）；\nepoll_wait()：阻塞等待，直到有 fd 就绪，返回就绪事件列表。\n\n3.1 为什么 epoll 能突破 select/poll 的性能瓶颈？\n（1）内核数据结构：红黑树 + 就绪链表\n\n\n红黑树（rb-tree）：用于存储所有通过 epoll_ctl(EPOLL_CTL_ADD) 注册的 fd。\n\n插入/删除复杂度为 O(log n)，高效支持百万级连接；\n避免了 select/poll 每次调用都要从用户态拷贝整个 fd 集合到内核的问题。\n\n\n\n就绪队列（ready list）：一个双向链表，由内核维护。\n\n当某个被监听的 fd 状态变为“可读”或“可写”时，内核会通过回调函数（callback）将该 fd 对应的 epitem 节点加入就绪队列；\nepoll_wait() 调用时，只需从该链表中取出就绪事件，无需遍历所有注册的 fd。\n\n\n\n\n这是 epoll 高效的核心：事件驱动 + 内核主动通知，而非用户态轮询。\n\n（2）避免全量 fd 拷贝\n\nselect/poll：每次调用都需将整个 fd 集合从用户空间复制到内核空间；\nepoll：fd 只需在 epoll_ctl() 时注册一次，后续 epoll_wait() 无需传递 fd 列表，极大减少系统调用开销。\n\n\n注：早期 epoll 实现曾使用 mmap 共享内存加速事件传递，但现代内核已优化为高效的 ring buffer 或直接拷贝，mmap 不再是必须。\n\n（3）时间复杂度：从 O(n) 到 O(k)\n\n\nselect/poll：O(n)，n 为监听的 fd 总数；这意味着即使只有少数文件描述符“就绪”（即数据可供读取/写入），内核也必须在每次调用该函数时检查每个描述符。这使得它们的性能与受监视的文件描述符数量成正比。\n例如，如果您正在监视 1,000 个文件描述符，但只有 1 个已准备就绪，select()则poll()仍将对所有 1,000 个进行迭代。\n\n\nepoll_wait()：时间复杂度从 O(n) 降到 O(1) or O(k)（k 为活跃连接数）；\n\n\n在 C1000K 场景下（100 万连接，仅 1000 个活跃），epoll 性能优势达 1000 倍以上。\n\n\n\n3.2 ET（边缘触发） vs LT（水平触发）\nepoll 支持两种事件触发模式，行为差异极大：\n\n\n\n模式\n行为\n适用场景\n\n\n\n\nLT（Level-Triggered，默认）\n只要 fd 处于“可读”状态（即内核接收缓冲区非空），每次调用 epoll_wait() 都会返回该 fd 的事件。\n编程简单，适合初学者；但可能重复触发，增加系统调用次数。\n\n\nET（Edge-Triggered）\n仅当 fd 状态从未就绪变为就绪时（如新数据到达），才触发一次事件。若未一次性读完数据，后续 epoll_wait() 不会再次通知。\n高性能场景；要求 fd 必须设为非阻塞模式，并在事件触发后循环读取直到返回 EAGAIN。\n\n\n\nET 模式使用示例（关键！）\n// 设置 socket 为非阻塞int flags = fcntl(sockfd, F_GETFL, 0);fcntl(sockfd, F_SETFL, flags | O_NONBLOCK);// 注册 ET 模式ev.events = EPOLLIN | EPOLLET;epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &amp;ev);// 事件处理void handle_event(int sockfd) {    char buf[4096];    ssize_t n;    while ((n = read(sockfd, buf, sizeof(buf))) &gt; 0) {        // 处理数据    }    if (n == -1 &amp;&amp; errno != EAGAIN) {        // 真正的错误        close(sockfd);    }    // 若 n == -1 &amp;&amp; errno == EAGAIN，说明数据已读完，退出循环}\n\n⚠️ ET 模式下，若未读完数据，将永远丢失后续通知！ 这是常见 bug 源头。\n\n\n四、Reactor 模式：epoll 的典型应用架构\n最直观的服务器处理多客户端访问的方式，就是为每个连接创建一个线程，或者更早期操作系统里甚至是一个进程。虽然线程比进程更轻量，切换成本也更低，但每连接一个线程在并发量大时仍会面临两个问题：\n线程创建/销毁有系统开销（malloc stack、context switch）\n线程是稀缺资源，系统线程数有上限（如 Linux 默认 1024~65535）\n所以，现代服务端架构更倾向于使用线程池来复用线程资源。线程池中每个线程从任务队列中取任务处理，避免了反复创建/销毁的开销。\n但线程池引入后面临另一个问题：I/O 阻塞会浪费线程资源。如果一个线程执行 read(socket) 被阻塞，而连接上没有数据，那么整个线程就\"卡住\"了，不能服务其他连接。\n为了避免这个问题，我们可以将 socket 设置为非阻塞模式。这样 read() 不会阻塞，而是立即返回，如果没有数据，就返回错误码 EAGAIN。线程就可以在用户空间\"轮询\"所有 socket。\n但这种方式会导致高 CPU 占用，尤其在连接数多时效率低。\nReactor（反应器）是一种事件驱动设计模式，其核心思想是：\n\n“一个或多个输入源 → 一个事件分发器（Reactor）→ 多个事件处理器”\n\n在 Linux 高并发服务中，epoll 就是 Reactor 的“事件分发器”。\n4.1 单 Reactor 单线程（如 Redis）\n\n结构：\n\n1 个线程运行 epoll 事件循环；\n所有 I/O 事件（accept、read、write）和业务逻辑均在此线程中串行处理。\n\n\n优点：\n\n无锁、无竞争，实现简单；\n上下文切换开销为零。\n\n\n缺点：\n\n业务逻辑不能阻塞（如不能执行慢查询、sleep）；\n无法利用多核 CPU。\n\n\n适用：I/O 密集、业务逻辑极轻的场景（如内存 KV 存储）。\n\n4.2 单 Reactor 多线程（如早期 Nginx）\n\n结构：\n\n1 个主线程运行 epoll，负责监听所有 socket；\n当连接可读/可写时，将整个连接（或请求）交给线程池处理；\n线程池执行业务逻辑（如解析 HTTP、访问磁盘）。\n\n\n优点：\n\n利用多核；\n业务逻辑可阻塞。\n\n\n缺点：\n\nI/O 事件分发仍由单线程处理，可能成为瓶颈；\n线程间数据传递需加锁或队列。\n\n\n\n4.3 主从 Reactor（如 Netty、Swoole）\n\n结构：\n\nMain Reactor（主 Reactor）：仅监听 listen fd，处理 accept()，将新连接分配给 Sub Reactor；\nSub Reactor（从 Reactor）：每个 Sub Reactor 运行独立的 epoll 循环，管理一组连接的 I/O（read/write）；\n业务逻辑可由 Sub Reactor 自己处理，或再交给线程池。\n\n\n优点：\n\n完全并行化：accept 与 I/O 分离，I/O 与 I/O 分离；\n无锁设计（每个连接只属于一个 Sub Reactor）；\n极致扩展性，轻松支撑百万连接。\n\n\n关键机制：\n\n使用 SO_REUSEPORT（Linux 3.9+）：允许多个进程/线程 bind 同一端口，内核自动负载均衡，避免“惊群”问题；\n每个 Sub Reactor 绑定到独立 CPU 核心，减少 cache miss。\n\n\n\n\n📌 Netty 的 EventLoopGroup 就是 Sub Reactor 池的实现。\n\n\n五、epoll 的局限与未来\n尽管 epoll 极其成功，但它仍有局限：\n\n仍是同步事件通知模型：用户需主动调用 read()/write()；\n系统调用开销仍存在（epoll_wait + read 至少两次 syscall）；\n对磁盘 I/O 支持弱（更适合网络 I/O）。\n\n下一代方案：io_uring（Linux 5.1+）\n\n用户提交 I/O 请求到 ring buffer，内核异步完成；\n支持 网络 + 文件 + epoll 替代；\n一次 syscall 完成提交 + 获取结果；\nRedis 7.0+、Nginx、Tokio 已开始集成。\n\n但目前，epoll + Reactor 仍是 Linux 高并发服务的事实标准。\n\n","categories":["计算机科学"],"tags":["计算机基础","操作系统"]},{"title":"The Roads I Couldn’t Take","url":"/2025/03/05/poem/index/","content":"tossed no books, yet lost my pages,\nWatched my past slip through the ages.\nA younger me once held them tight,\nBut let them go into the night.\nset my sights on distant lands,\nWith open heart and empty hands.\nLocked in walls I couldn’t break,\nDreams delayed—what path to take?\nThe rules shift, the borders change,\nPlans collapse, yet I remain.\nNot a thread but woven streams,\nNot one path but countless dreams.\nIf gates are closed, I’ll build my door,\nIf tides retreat, I’ll row the shore.\nNot bound by place, nor time, nor fate,\nI shape my world—it’s not too late.\nThough the past is cast in stone,\nThe future’s mine to carve alone.\nAll those moments will be lost in time\nlike tears in rain.\n","categories":["感想"]},{"title":"访问localhost的时候你的电脑发生了什么？","url":"/2025/02/28/Network/network/","content":"1. 环回地址（127.0.0.1）的请求流程\n(1) 协议栈的处理\n\n数据包仍然会走完整的网络协议栈（应用层 → 传输层 → 网络层 → 链路层），但链路层和物理层的处理是虚拟的。\n关键点：\n\n网络层（IP层）： 数据包的目标地址是 127.0.0.1，操作系统识别为环回地址，不会将数据包发送到物理网卡。\n链路层和物理层： 操作系统通过虚拟的环回接口（lo 或 Loopback Adapter）处理数据包，不会触发真实的硬件操作（如网卡收发数据）。\n传输层（TCP/UDP）： 仍然需要完成端口绑定、连接管理等逻辑（例如 TCP 的三次握手）。\n\n\n\n(2) 示例：Ping 127.0.0.1\n\n当执行 ping 127.0.0.1 时：\n当你在浏览器、cURL 或任何应用程序（如 ping 127.0.0.1）中输入 127.0.0.1：\n\n应用程序会尝试解析该地址，但 127.0.0.1 是一个 IP 地址，不需要 DNS 解析。\n它会选择协议（通常是 TCP 或 UDP），并调用系统 API（如 socket()、connect()）。\n2. 传输层（Transport Layer）\n如果使用 TCP（如 HTTP 请求），应用程序会创建一个 TCP 连接, 内核会生成 TCP 三次握手（SYN -&gt; SYN+ACK -&gt; ACK）。\n如果使用 UDP，则不会有连接建立的过程，而是直接发送数据报。\n3. 网络层（Network Layer）\n传输层将数据交给 网络层，指定目标地址 127.0.0.1。\n网络层检测到 127.0.0.1 属于 环回地址（Loopback Address）：\n这一地址不会被路由到外部网络。\n操作系统会将数据包 直接送回本机 处理，而不经过网卡。\na. ICMP 请求（网络层）生成并发送到环回接口。\nb. 操作系统直接在内核中将请求转发给接收逻辑，不经过物理网卡。\nc. 因此，能 ping 通，但数据包仅在内存中处理，不涉及硬件。\n\n\n127.0.0.1： 是一个 IPv4 地址,  网络层检测到 127.0.0.1 属于 环回地址（Loopback Address）\n\n\n在支持 IPv6 的系统中，localhost 可能同时映射到 127.0.0.1（IPv4）和 ::1（IPv6）。\n\n如果服务只监听 IPv4，使用 localhost 可能会导致连接失败。\n\n\n\n性能差异：\n\n在大多数情况下，localhost 和 127.0.0.1 的性能差异可以忽略不计。\n但在高并发或性能敏感的场景中，直接使用 127.0.0.1 可能更高效。\n\n\n\n\n2. localhost 的请求流程\n\nlocalhost 是主机名，默认解析为 127.0.0.1，因此其行为与直接使用 127.0.0.1 完全相同。\n请求流程：\n\n应用层发起请求（例如访问 http://localhost:8080）。\nDNS 解析将 localhost 转换为 127.0.0.1。\n\n\n\nlocalhost 的请求处理方式仍然依赖于网络协议栈, 如果 localhost 完全绕过网络协议栈，直接让应用层处理请求，那它确实更像 进程间通信（IPC），比如：\n\nUnix Domain Socket（UDS）\n共享内存、管道、消息队列\n\n但实际情况是：\n\nlocalhost 依然需要解析为 IP 地址并走 TCP/IP 协议栈。\n只不过数据不会经过物理网卡，而是在 内核 中完成回环处理。\n所以它比普通的网络请求快，但比 IPC 方式慢。\n\n\n3. localhost 请求 vs. 本地进程间通信（IPC）\n(1) 相似之处\n\n不依赖物理网络： 两者都不需要物理网卡或外部网络设备。\n高性能： 由于数据在内存中处理，延迟极低。\n\n(2) 关键区别\n\n\n\n特性\nlocalhost（环回接口）\n本地进程间通信（IPC）\n\n\n\n\n协议栈\n走完整的网络协议栈（TCP/IP）\n直接通过操作系统提供的 IPC 机制（如管道、共享内存）\n\n\n编程接口\n使用 Socket API（如 bind(), listen()）\n使用 IPC 专用 API（如 pipe(), shm_open()）\n\n\n兼容性\n与网络通信代码完全兼容\n需要专门设计 IPC 逻辑\n\n\n数据封装\n需要处理 TCP/UDP 包头、IP 包头等\n直接传输原始数据\n\n\n适用场景\n模拟网络服务、测试客户端-服务端逻辑\n高性能进程间数据交换\n\n\n\n(3) 为什么环回接口仍要走协议栈？\n\n兼容性： 开发者可以使用相同的网络编程代码（如 Socket API）测试本地服务，无需修改逻辑。\n隔离性： 通过端口号隔离不同服务，与远程通信的行为一致。\n安全性： 防火墙规则可以统一管理环回接口和物理接口的流量。\n\n\n4. 操作系统的实现优化\n\n内核优化： 操作系统会对环回接口的流量进行优化，例如：\n\n跳过物理网卡驱动和中断处理。\n减少数据拷贝次数（如从应用层直接传递到接收缓冲区）。\n\n\n性能对比：\n\n环回接口： 延迟通常在微秒级（μs），吞吐量可达数十 Gbps。\nIPC： 延迟可低至纳秒级（ns），吞吐量更高（取决于具体 IPC 机制）。\n\n\n\n\n5. 实验验证\n(1) 使用 Wireshark 抓包\n\n抓取环回接口（如 Loopback）的流量，可以看到完整的 TCP/IP 数据包（包括以太网帧头），但这些帧头是虚拟生成的，不会发送到物理网卡。\n\nInternet protocol\n在当今互联互通的世界中，TCP/IP（Transmission Control Protocol/Internet Protocol，传输控制协议/互联网协议）协议栈构成了现代网络通信的核心。无论是访问网页、发送电子邮件，还是进行视频通话，所有数据的传输都依赖于这一分层协议体系。TCP/IP 协议栈采用分层架构，通常分为四个层次：应用层、传输层、网络层和数据链路层，每一层都承担特定的功能，共同实现可靠、高效的数据传输。网络层（IP）负责寻址和路由，确保数据包能够跨越全球网络到达目标地址；传输层（TCP/UDP）提供数据传输的可靠性和效率，而应用层则承载 HTTP、FTP、DNS 等实际服务，使用户能够便捷地访问互联网资源。\n作为现代网络的基石，TCP/IP 协议栈不仅推动了全球互联网的发展，也成为企业网络、云计算、物联网（IoT）等技术的通信标准。它的开放性和可扩展性使其能够不断适应新的技术需求，持续引领数字时代的网络演进。\n\nProtocol defines format,order of messages was sent, received among network entities ,and  actions taken on transmission,receipt\nOSI\nMAC∈datalink LAN层\n常见的网络编程中的问题主要是怎么定位网络上的一台主机或多台主机，另一个是定位后如何进行数据的传输。对于前者，在网络层中主要负责网络主机的定位，数据传输的路由，由IP地址可以唯一地确定Internet上的一台主机。对于后者，在传输层则提供面向应用的可靠（tcp）的或非可靠（UDP）的数据传输机制。\n对于客户端/服务器（C/S）结构。 即通信双方一方作为服务器等待客户提出请求并予以响应。客户则在需要服务时向服务器提出申请。服务器一般作为守护进程始终运行，监听网络端口，一旦有客户请求，就会启动一个服务进程来响应该客户，同时自己继续监听服务端口，使后来的客户也能及时得到服务。\n对于浏览器/服务器（B/S）结构。 客户则在需要服务时向服务器进行请求。服务器响应后及时返回，不需要实时监听端口。\n二者的区别，取决于怎么看他们，如果使用浏览器，浏览器就是指“客户端”，“client/server” 和 “browser/server”两种体系结构没有真正的区别，没法比较\n\nHTTP TCP IP\nHTTP规定了每段数据以什么形式表达才是能够被另外一台计算机理解。而TCP所要规定的是数据应该怎么传输才能稳定且高效的传递与计算机之间。\nHTTP:\n200OK  请求成功  400 bad request\nIdempotent幂等性概念：幂等通俗来说是指不管进行多少次重复操作，都是实现相同的结果。\n2.REST请求中哪些是幂等操作\nGET，PUT，DELETE都是幂等操作，而POST不是，以下进行分析：\n首先GET请求很好理解，对资源做查询多次，此实现的结果都是一样的。    PUT请求的幂等性可以这样理解，将A修改为B\nSSL\n\nSSL(Secure Sockets Layer 安全套接层)是为网络通信提供安全及数据完整性的一种安全协议。SSL 是 “Secure Sockets Layer” 的缩写，中文意思为“安全套接层”，而 TLS 则是标准化之后的 SSL。\n\nTLS\n\n安全传输层协议（TLS：Transport Layer Security）用于在两个通信应用程序之间提供保密性和数据完整性。该协议由两层组成： TLS 记录协议（TLS Record）和 TLS 握手协议（TLS Handshake），是更新、更安全的SSL版本。 HTTPS use TLS1.3\n\nHTTP的核心概念\n除了HTTP存在于应用层之外，该协议还有5个特点。\n\n\nHTTP的标准建立在将两台计算机视为不同的角色：客户端和服务器。客户端会向服务器传送不同的请求(request)，而服务器会对应每个请求给出回应(response)。无连接\n\n\nHTTP属于无状态协议(Stateless)。这表示每一个请求之间是没有相关性的。在该协议的规则中服务器是不会记录任何客户端操作，每一次请求都是独立的。（记录用户浏览行为会通过其他技术实现）\n\n\n客户端的请求被定义在几个动词意义范围内。最长用到的是GET和POST，其他动词还包括DELETE, HEAD等等。\n[GET和POST两种http请求方法的区别]\nGET和POST是HTTP请求的两种基本方法, HTTP的底层是TCP/IP。所以GET和POST的底层也是TCP/IP，也就是说，GET/POST都是TCP链接。**GET请求没有body，只有url，请求数据放在url的querystring中；POST请求的数据在body中“。但这种情况仅限于浏览器发请求的场景。**GET和POST能做的事情是一样一样的。你要给GET加上request body，给POST带上url参数，技术上是完全行的通的。\nGET\n“读取“一个资源。比如Get到一个html文件。反复读取不应该对访问的数据有副作用。比如”GET一下，用户就下单了，返回订单已受理“，这是不可接受的。没有副作用被称为“幂等“（Idempotent)。\n因为GET因为是读取，就可以对GET请求的数据做缓存。这个缓存可以做到浏览器本身上（彻底避免浏览器发请求），也可以做到代理上（如nginx），或者做到server端（用Etag，至少可以减少带宽消耗）\n\n千万不要用GET方法传送密码等敏感信息！（发出的数据会在浏览器地址栏中显示出来）\n\nPOST\n在页面里 标签会定义一个表单。点击其中的submit元素会发出一个POST请求让服务器做一件事。这件事往往是有副作用的，不幂等的。\n不幂等也就意味着不能随意多次执行。因此也就不能缓存。比如通过POST下一个单，服务器创建了新的订单，然后返回订单成功的界面。这个页面不能被缓存。试想一下，如果POST请求被浏览器缓存了，那么下单请求就可以不向服务器发请求，而直接返回本地缓存的“下单成功界面”，却又没有真的在服务器下单。那是一件多么滑稽的事情。\n因为POST可能有副作用，所以浏览器实现为不能把POST请求保存为书签。想想，如果点一下书签就下一个单，是不是很恐怖？。\nPOST方法比GET方法更健壮、更安全，而且POST方法对数据大小没有限制。\n\n\n服务器的回应被定义在几个状态码之间：5开头表示服务器错误，4开头表示客户端错误，3开头表示需要做进一步处理，2开头表示成功，1开头表示在请求被接受处理的同时提供的额外信息。\n\n\n不管是客户端的请求信息还是服务器的回应，双方都拥有一块头部信息(Header)。头部信息是自定义，其用途在于传递额外信息（浏览器信息、请求的内容类型、相应的语言）。\n\n\n持久 or pipeline 连接\n**持久连接：**使用同一个TCP连接发送和接受 多个 http请求/应答；\n　　**非持久连接：**一个TCP连接只能发送和接受 一个 http请求/应答；\nstateful contains stateless filter, which is the most flexible but cost more.\nHTTP2.0 多路复用 (Multiplexing)\n多路复用允许同时通过单一的 HTTP/2 连接发起多重的请求-响应消息。\npipeline连接\nHTTP/1.1的新特性，允许在持久连接上可选地使用请求管道。在响应到达之前，可以将多条请求放入队列。当第一条请求通过网络流向服务器时，第二条和第三条请求也可以开始发送了。在髙时延网络条件下，这样做可以降低网络的环回时间，提高性能。\n管道化连接有如下几条限制：\n\n客户端必须确认是持久连接才能使用管道；\n\nTCP/IP 意味着 TCP 和 IP 在一起协同工作。TCP 负责应用软件（比如您的浏览器）和网络软件之间的通信。\nInternet 的传输层有两个主要协议，互为补充。无连接的是 UDP，它除了给应用程序发送数据包功能并允许它们在所需的层次上架构自己的协议之外，几乎没有做什么特别的事情。面向连接的是 TCP，该协议几乎做了所有的事情。\n使用无连接协议可以很方便地支持一对多和多对一通信，而面向连接协议通常都需要多个独立的连接才能做到。但更重要的是，无连接协议是构建面向连接协议的基础。TCP/IP 是基于一个４层的协议栈，如下图所示：\nIP 负责计算机之间的通信。TCP 负责将数据分割并装入 IP 包，然后在它们到达的时候重新组合它们。IP 负责将包发送至接受者。\nTransport layer\nTCP/IP 意味着 TCP 和 IP 在一起协同工作。TCP 负责应用软件（比如您的浏览器）和网络软件之间的通信。\nInternet 的传输层有两个主要协议，互为补充。无连接的是 UDP，它除了给应用程序发送数据包功能并允许它们在所需的层次上架构自己的协议之外，几乎没有做什么特别的事情。面向连接的是 TCP，该协议几乎做了所有的事情。\n\n使用无连接协议可以很方便地支持一对多和多对一通信，而面向连接协议通常都需要多个独立的连接才能做到。但更重要的是，无连接协议是构建面向连接协议的基础。\n\nIP 负责计算机之间的通信。TCP 负责将数据分割并装入 IP 包，然后在它们到达的时候重新组合它们。IP 负责将包发送至接受者。\nHere are some key differences between TCP and UDP:\n\nReliability: TCP is a reliable protocol, while UDP is unreliable. TCP provides guaranteed delivery of data packets, ensuring that they arrive at the destination in the correct order and without errors. UDP, on the other hand, does not have built-in mechanisms for error checking, retransmission of lost packets, or ensuring packet ordering. It is up to the application layer to handle these aspects if required.\nConnection-oriented vs. Connectionless: TCP is a connection-oriented protocol, which means it establishes a connection between the sender and receiver before data transmission. It performs a handshake process, establishes a reliable channel, and ensures a secure data transfer. UDP, on the other hand, is connectionless. It does not establish a dedicated connection before data transfer and simply sends packets to the destination without any handshake or setup.\nPacket overhead:开销 TCP has a higher packet overhead compared to UDP. TCP adds additional information to each packet, such as sequence numbers, acknowledgments, and other control flags, which increases the overall size of the transmitted data. UDP has a smaller packet overhead, resulting in less network congestion and lower latency.\nOrdering of packets: TCP guarantees the ordering of packets. If packets arrive out of order, TCP reorders them at the receiver’s end, ensuring that the application layer receives the data in the correct order. UDP does not provide any mechanisms for ordering packets, so the application layer must handle packet ordering if required.\nFlow control and congestion control: TCP implements flow control and congestion control mechanisms to manage the rate of data transmission and avoid network congestion. It dynamically adjusts the transmission rate based on network conditions and ensures that the receiver can handle the incoming data. UDP does not have built-in flow control or congestion control mechanisms, and it’s up to the application layer to manage these aspects if necessary.\nApplication suitability: TCP is commonly used for applications that require reliable and ordered data delivery, such as web browsing, email, file transfer, and remote desktop. UDP is suitable for applications that prioritize speed and efficiency over reliability, such as real-time streaming, online gaming, DNS (Domain Name System), and VoIP (Voice over IP).\n\n\n\nTCP实现细节\nhttps://blog.csdn.net/m0_46156900/article/details/113809699传输层概述、传输层服务、多路复用和解复用、无连接传输 UDP\nDEMUX\nconnectionless: 主机接收到UDP segment (dest port dest IP)\nconnection oriented:\n\n服务器能够在一个TCP端口上同时支持多个TCP套接字：\n 每个套接字由其四元组标识（有不同的源IP和源PORT）\n Web服务器对每个连接客户端有不同的套接字\n 非持久对每个请求有不同的套接字\n\n“服务器能够根据源IP地址和源端口号来区分来自不同客户机的报文段。\n但是套接字与进程之间并非总是有着一一对应的关系。\n事实上，Web服务器通常一个服务进程可以为每个新的客户机连接创建一个具有新连接套接字的线程。\n显然，对于这样的服务器，在任意给定的时间内都可能有很多套接字(具有不同的标识)连接到同一个进程。”\nrdt3.0：\n具有比特差错和分组丢失的信道\n新的假设：****下层信道可能会丢失分组（数据或ACK）(好比之前说的路由器队列排满了，新来的就被drop掉)\n\n 会死锁\n 机制还不够处理这种状况：\n•  rdt3.0可以工作，但链路容量比较大的情况下，性能很差 链路容量比较大，一次发一个PDU 的不能够充分利用链路的传输能力\npipelined protocol\n\n两种通用的流水线协议：回退N步(GBN)和选择重传(SR)\ngo-back-n的一个缺点 *是单个分组端差错能够引起大量分组端重传，许多分组其实没必要重传。*为了解决这个缺点， selective repeat来了\n滑动窗口          (slide window)协议\n\n 发送缓冲区\n 形式：内存中的一个区域，落入缓冲区的分组可以发送\n 功能：用于存放已发送，但是没有未经确认的分组\n 必要性：需要重发时可用\n发送缓冲区中的分组\n 未发送的：落入发送缓冲区的分组，可以连续发送出去；\n 已发送、等待确认的：发送缓冲区的分组只有得到确认才能删除\n\nselective repeat：\n!https://img-blog.csdnimg.cn/20190524220207716.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDAyNzg5Nw==,size_16,color_FFFFFF,t_70\n**sender 端：**接收上层端数据，为 分组 assign 序号，如果 在 window 范围内，则发送，同时为每一个分组起一个逻辑定时器，哪个分组端定时器超时，重传哪个分组。上图中， pkt 2 timeout，只重传 pkt2，对其他 package没有影响。\npkt3 发送完会有一个wait的动作，因为现在窗口满了，在收到回复之前不会发送下一个分组。\nTCP fast retransmit: if the receiver sends triple duplicate ACK for same  data,  sender resend unacked segment\nreceiver 端：确认每一个正确接收的分组，而不管是否乱序，乱序端分组被缓存，直到丢失分组（即序号更小的分组）全部接收，将这一批分组按序交付给上层。上图中，收到 pkt1，交付，收到 pkt2，交付，收到pkt3，缓存。\n需要注意的是，为什么sender 端在收到ack0 时会立即发送pk4，但是收到ack3时没有继续往外发送数据呢，原因是receiver端发送ACK3时，已经知道pkt2还未到到，所以在ACK3中带端 nextsequence 是pkt2，而此时发送端正在等待pkt2的超时（或者ACK2），所以在pkt2超时之前不会有新端package发出。\nTCP flow control\nreceiver controls the sender that sender will not overflow receiver by transmit too much\n!https://img-blog.csdnimg.cn/20210217120510616.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ2MTU2OTAw,size_16,color_FFFFFF,t_70\nMTU：\nMaximum Transmission Unit，最大数据传输单元，网络传输最大报文包；以太网MTU最大值是1500B\nMSS：\nMaximum Segment Size ，TCP提交给IP层最大分段大小，不包含TCP Header和 TCP Option，只包含TCP Payload ，MSS是TCP用来限制application层最大的发送字节数。如果底层物理接口MTU= 1500 byte，则 MSS = 1500- 20(IP Header) -20 (TCP Header) = 1460 byte，如果application 有2000 byte发送，需要两个segment才可以完成发送，第一个TCP segment = 1460，第二个TCP segment = 540.要根据MSS的大小进行切分和加TCP头部信息，变成TCP报文段（segment）\n·\n所以，（Message 转换成字节流，再被划分成一个个的MSS）\nMSS+TCP头部信息+IP头部信息 = MTU(比如以太网MTU1500B)，（“于是就存在一个分片的问题？”什么分片…）\nIP分片\n在TCP/IP分层中，数据链路层用MTU（Maximum Transmission Unit，最大传输单元）来限制所能传输的数据包大小，MTU是指一次传送的数据最大长度，不包括数据链路层数据帧的帧头。当发送的IP数据报的大小超过了MTU时，IP层就需要对数据进行分片，否则数据将无法发送成功。\n一个IP数据报的每个分片都具有自己的IP头部信息，它们都具有相同的标识值，但是具有不同的位偏移，且除了最后一个分片fragflag=0外，其他分片都将设置fragflag=1标志。此外，每个分片的IP头部的总长度字段将被设置为该分片的长度。\n例如，以太网帧的MTU是1500字节，因此它的数据部分最大为1480字节(IP头部占用20字节)。\nNagle算法\nA problem can occur when an application generates data very slowly.\n Consider, ssh or telnet that generate data only when a user types.\n This means (for ssh/telnet) one packet sent every time user hits key          ——&gt; Nagle’s algorithm\n\\1. 发送 TCP 发送它接收到的第一条数据 - 无论大小\n\n发送 TCP 在缓冲区中累积数据并等待以下之一，然后再发送该段： • 接收 TCP 发送确认• 数据已累积以填充最大大小的段\n重复步骤 2 注意：有时应关闭 Nagle 算法——例如当快速交互至关重要并且您希望发送小数据包时\n\n该算法的优越之处在于它是自适应的，确认到达的越快，数据也就发哦送的越快\nCongestion control\n\n 非正式的定义:“太多的数据需要网络传输，超过了网络的处理能力”\n 拥塞的表现:\n\n 分组丢失 (路由器缓冲区溢出)\n 分组经历比较长的延迟(在路由器的队列中排队)\n\n\n\n速率控制方法\n!https://img-blog.csdnimg.cn/20210218100930516.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ2MTU2OTAw,size_16,color_FFFFFF,t_70\n如何控制发送端发送的速率？\n (发送方)维持一个拥塞窗口的值：CongWin（CongestionWindow以字节为单位）\n它对一个TCP发送方能向网络中发送流量的速率进行了限制。\n 发送端限制已发送但是未确认的数据量（的上限）:\nLastByteSent - LastByteAcked &lt;= CongWin\n 从而粗略地控制发送方的往网络中注入的速率\nTCP reno : loss  indicated by 3 ACKs                  ;\ntcp Tahoe sets cwnd to 1\nrate = cwnd/RTT                          avg TCP throughput = 3Window size /4 RTT\n\n\n!https://img-blog.csdnimg.cn/20210216113823842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ2MTU2OTAw,size_16,color_FFFFFF,t_70\n在HTTP的规范内，两台计算机的交互被视为request和response的传递。而在实际的TCP操作中，信息传递会比单纯的传递request和response要复杂。通过TCP建立的通讯往往需要计算机之间多次的交换信息才能完成一次request或response。\nTCP的传输数据的核心是在于将数据分为若干段并将每段数据按顺序标记。标记后的顺序可以以不同的顺序被另一方接收并集成回完整的数据。计算机对每一段数据的成功接收都会做出相应，确保所有数据的完整性。\nTCP在传递数据时依赖于实现定义好的几个标记（Flags）去向另一方表态传达数据和连接的状态：\n\nF : FIN - 结束; 结束会话\nS : SYN - 同步; 表示开始会话请求\nR : RST - 复位;中断一个连接 P : PUSH - 推送; 数据包立即发送\nA : ACK - 应答 U : URG - 紧急 E : ECE - 显式拥塞提醒回应\nW : CWR - 拥塞窗口减少\n\nConnection建立&amp; close\n也是基于这些标志TCP可以实现三次（three ways handshake）和四次握手 (four ways tear down)。三次握手是初步建立连接的机制，而四次握手则是断开链接。两者之间大致操作是一样的，A发出建立链接(SYN)或者断开链接(FIN)的请求，B认可(A CK)其请求然后发出同样的请求给A并等待A的认可。在双方认可后，链接正式成立或者断开。\n!https://img2020.cnblogs.com/blog/2098532/202105/2098532-20210520103027590-619536232.png\n!https://img2020.cnblogs.com/blog/2098532/202105/2098532-20210520103048249-324869987.png\n1、为什么建立连接协议是三次握手，而关闭连接却是四次握手呢？\n建立连接时，ACK和SYN可以放在一个报文里来发送。而关闭连接时，被动关闭方可能还需要发送一些数据后，再发送FIN报文表示同意现在可以关闭连接了，所以它这里的ACK报文和FIN报文多数情况下都是分开发送的。\n2、为什么TIME_WAIT状态还需要等2MSL后才能返回到CLOSED状态？\n两个存在的理由：1、无法保证最后发送的ACK报文会一定被对方收到，所以需要重发可能丢失的ACK报文。2、关闭链接一段时间后可能会在相同的IP地址和端口建立新的连接，为了防止旧连接的重复分组在新连接已经终止后再现。2MSL足以让分组最多存活msl秒被丢弃。\n3、为什么必须是三次握手，不能用两次握手进行连接？\n记住服务器的资源宝贵不能浪费! 如果在断开连接后，第一次握手请求连接的包才到会使服务器打开连接，占用资源而且容易被恶意攻击！防止攻击的方法，缩短服务器等待时间。两次握手容易死锁。如果服务器的应答分组在传输中丢失，将不知道S建立什么样的序列号，C认为连接还未建立成功，将忽略S发来的任何数据分组，只等待连接确认应答分组。而S在发出的分组超时后，重复发送同样的分组。这样就形成了死锁。\nQUIC\n\nQUIC 的目标是几乎等同于 TCP 连接，但延迟大大降低。它主要通过依赖于对HTTP流量行为的理解的两项更改来实现这一点。[20]\n第一个变化是大大减少连接建立期间的开销。由于大多数 HTTP 连接都需要TLS，QUIC 将设置密钥和支持的协议的交换作为初始握手过程的一部分。当客户端打开连接时，响应数据包包含未来数据包使用加密所需的数据。这消除了建立 TCP 连接然后通过附加数据包协商安全协议的需要。其他协议可以以相同的方式提供服务，将多个步骤组合成一个请求-响应。然后，此数据可用于初始设置中的后续请求，以及将以其他方式协商为单独连接的未来请求。[20]\n第二个变化是使用UDP而不是 TCP 作为其基础，其中不包括丢失恢复。相反，每个 QUIC 流都是单独进行流量控制的，丢失的数据在 QUIC 级别而不是 UDP 级别重新传输。这意味着如果一个流中发生错误，如上面的图标示例，协议栈可以继续独立地为其他流提供服务。这对于提高容易出错的链接的性能非常有用，因为在大多数情况下，在 TCP 注意到数据包丢失或损坏之前可能会收到大量额外数据，并且在更正错误时所有这些数据都被阻止甚至刷新。在 QUIC 中，在修复单个多路复用流时，可以自由处理这些数据。[27]\nQUIC 还包括许多其他更改，这些更改也可以改善整体延迟和吞吐量。例如，数据包被单独加密，因此它们不会导致加密数据等待部分数据包。这在 TCP 下通常是不可能的，其中加密记录在字节流中，协议栈不知道该流中的高层边界。这些可以由运行在顶层的层协商，但 QUIC 的目标是在一次握手过程中完成所有这些。[8]\nQUIC 系统的另一个目标是提高网络切换事件期间的性能，例如当移动设备的用户从本地WiFi 热点移动到移动网络时发生的情况。当这种情况发生在 TCP 上时，一个漫长的过程开始，每个现有的连接一个接一个地超时，然后根据需要重新建立。为了解决这个问题，QUIC 包含一个连接标识符，它可以唯一标识与服务器的连接，而不管来源如何。这允许通过发送始终包含此 ID 的数据包简单地重新建立连接，因为即使用户的IP 地址更改，原始连接 ID 仍然有效。[28]\n!https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/HTTP-1.1_vs._HTTP-2_vs._HTTP-3_Protocol_Stack.svg/336px-HTTP-1.1_vs._HTTP-2_vs._HTTP-3_Protocol_Stack.svg.png\n!https://en.wikipedia.org/w/extensions/ImageMap/resources/desc-20.png?15600\nHTTP/3 与 HTTP/1.1 和 HTTP/2 对比的协议栈\nQUIC 可以在应用程序空间中实现，而不是在操作系统内核中。当数据在应用程序之间移动时，由于上下文切换，这通常会调用额外的开销。然而，就 QUIC 而言，协议栈旨在供单个应用程序使用，每个使用 QUIC 的应用程序都有自己的连接托管在 UDP 上。最终差异可能非常小，因为整个 HTTP/2 堆栈的大部分已经在应用程序中（或更常见的是它们的库）。将其余部分放在这些库中，本质上是纠错，对 HTTP/2 堆栈的大小或整体复杂性几乎没有影响。[8]\n这种组织允许更容易地进行未来的更改，因为它不需要更改内核来进行更新。QUIC 的长期目标之一是添加用于前向纠错(FEC) 和改进拥塞控制的新系统\nNetwork layer\n2functions: forwarding 转发: (是一种局部的概念/功能…数据平面的…依赖于路由表…)将分组从路由器的输入接口转发到合适的输出接口\nrouting: determine the\n1.router examines header fields in all IP datagram\n!https://img-blog.csdnimg.cn/20210218141705669.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ2MTU2OTAw,size_16,color_FFFFFF,t_70\n!https://img-blog.csdnimg.cn/20210218174446962.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ2MTU2OTAw,size_16,color_FFFFFF,t_70\n路由器\n功能\n\n运行路由算法/协议（RIP，OSPF，BGP）\n将数据报从入口链路转发到出口链路\n\nrouter structure:\n!https://img-blog.csdnimg.cn/2021021815334083.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ2MTU2OTAw,size_16,color_FFFFFF,t_70\nipv6 IP addr 128bits 16bytes\n\n实际中的路由器的Router input ports和 output ports 是同一个端口，\n只是为了方便输入输出端口的讲解才分成两个“独立的输入输出端口”\n\nIP\n!https://img-blog.csdnimg.cn/20210218184213704.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ2MTU2OTAw,size_16,color_FFFFFF,t_70\n局域网接入Internet的技术有三种https://blog.csdn.net/wenqiang1208/article/details/72403785：\n\n直接路由\n代理服务器(proxy)代理服务器（Proxy Server）是一种重要的服务器安全功能，它的工作主要在(OSI)模型的会话层，从而起到防火墙的作用。代理服务器大多被用来连接INTERNET（国际互联网）和Local Area Network（局域网）。\n代理分类：HTTP代理；socks代理；VPN代理；FTP代理\n网络地址转换(NAT)NAT将自动修改IP报文的源IP地址和目的IP地址，Ip地址校验则在NAT处理过程中自动完成。有些应用程序将源IP地址嵌入到IP报文的数据部分中，所以还需要同时对报文的数据部分进行修改，以匹配IP头中已经修改过的源IP地址。否则，在报文数据部分嵌入IP地址的应用程序就不能正常工作。\n\nNAT类型：\n\nNAT0（全锥形NAT）： 您的设备拥有一个真实的公网IP地址，可以直接与互联网上的其他设备通信，无需经过额外的转换。这是最理想的网络类型，适合需要高性能和公网访问的应用场景。\nNAT1（对称型NAT）： 您的设备可以通过端口映射与互联网上的其他设备通信，但需要手动配置端口映射规则。相对NAT0来说，性能稍差，但仍能满足大多数应用场景的需求。\nNAT2（端口限制型NAT或锥形NAT）： 您的设备只能与已建立连接的设备通信，无法主动发起连接。这会限制某些应用的使用，例如P2P下载、部分游戏等。\nNAT3（完全端口限制型NAT）： 您的设备只能与已建立连接的设备通信，且只能使用特定的端口。这是限制最严格的NAT类型，会对大部分应用造成影响。\n\nIP   (Fragmentation &amp; Reassembly)\n 网络链路有MTU(最大传输单元) – 链路层帧所携带的最大数据长度 不同的链路类型 不同的MTU\n如果IP层有一个数据报要传，而且数据帧的长度比链路层的MTU还大，\n那么IP层就需要进行分片( fragmentation)，即把数据报分成若干片，这样每一片就都小于MTU。\n 最后一个分片标记为0 “重组”只在最终的目标主机进行 IP头部的信息被用于标识，排序相关分片\nIP（专用）地址 只在局部网络中有意义，区分不同的设备\n 路由器不对目标地址是专用地址的分组进行转发\n 专用地址范围（A、B、C类都有）不用向isp申请\n!https://img-blog.csdnimg.cn/20210218205701933.png\n Class A 10.0.0.0-10.255.255.255 MASK 255.0.0.0\n Class B 172.16.0.0-172.31.255.255 MASK 255.255.0.0\n Class C 192.168.0.0-192.168.255.255 MASK 255.255.255.0\n分类寻址 (Classful Addressing)：将 IP 地址分为 ABCDE 五类，三个主要类为 ABC 三类，区别在于网络号和主机号的长度，网络号和主机号的划分线在八位字节边界，如 C 类地址前 24 bits 为网络号，后 8 bits 为主机号。\n子网分类寻址 (Subnetted Classful Addressing)：在子网寻址系统中，通过从主机号中拿取前端部分 bits 作为子网号，用于识别子网，这将原本的两层划分系统（网络号/主机号）被改为三层划分系统（网络号/子网号/主机号）。\n无类寻址 (Classless Addressing)：将原始的分类寻址抛开，网络号和主机号可以在任意点划分，不需要像分类寻之中那样划分在八位字节边界。这种方案更加灵活有弹性。\nCIDR: Classless InterDomain Routing（无类域间路由）\n\n网络前缀越短，\n其地址块所包含的地址数就越多。\n而在三级结构的IP地址中，划分子网是使网络前缀变长。\n\nVLSM(可变长子网掩码) 是为了有效的使用无类别域间路由（CIDR）和路由汇聚(route summary)来控制路由表的大小，它是网络管理员常用的IP寻址技术\nCIDR无类网络是一种相对于有类网络的网络，无类网络IP地址的掩码是变长的。\n在有类网络的基础上，拿出一部分主机ID作为子网ID。\n例如：\nIP地址为192.168.250.44 子网掩码不能是小于24位。\n因为这是一个C类地址（前3Bytes是网络号），子网掩码只能大于24位。\n而掩码255.255.248.0（21位）是不符合规定的。\n·\n如果一个网络中的主机有100台，\n那么，可以用子网掩码/25来划分这个C类网络（“192.168.250.0/24”）：\n划分成192.168.250.0/25 和192.168.250.128/25两个子网。\n—主机192.168.250.44/25 属于子网192.168.250.0/25。\nSubnet\n子网掩码不能单独存在，它必须结合IP地址一起使用。\n子网掩码只有一个作用，就是将某个IP地址划分成网络地址和主机地址两部分。\nIP地址分配中的几个特定类别，以下是对它们的解释：\n\n单播地址（Unicast）：\n\n单播地址用于一对一通信，表示数据包从一个发送者传输到一个特定的接收者。它是最常见的IP地址类型。\nA、B、C 类地址都是单播地址，分别用于大型网络、中等大小网络和小型网络。\n\n\n组播地址（Multicast）：\n\n组播地址用于一对多通信，表示数据包从一个发送者传输到一个特定的组中的多个接收者。\nD 类地址是组播地址，用于指示数据流向一组设备。多个主机可以通过加入一个特定的组播组来接收这些数据。\n\n\n广播地址（Broadcast）：\n\n广播地址用于一对所有通信，在一个子网或局域网中向所有设备发送数据包。\n在 IPv4 中，广播地址通常是特定的地址，例如全网广播地址为255.255.255.255，用于发送数据包给同一网络中的所有设备。\n\n\nE 类地址（Reserved）：\n\nE 类地址是保留地址，用于未来可能的用途。这个地址空间暂时没有分配给特定用途，被保留在IP地址分配方案中。\n\n\n\n路由器\n功能\n\n运行路由算法/协议（RIP，OSPF，BGP）\n将数据报从入口链路转发到出口链路\n\n结构\n输入端口(input port)：\n\n物理层：接收物理信号，将其转换为bit\n与数据链路层交互来执行数据链路层功能\n网络层：查找、转发、排队\n非集中式的交换：转发表的副本已发送给每个输入端口，因此，转发决策由输入端口在本地完成\n\n根据数据报的目的地址，在输入端口内存的转发表中寻找输出端口\n目标：以线缆一级的速度完成输入端口处理\n队列：如果数据报达到的速率大于处理速率则需要排队\n\n\n\nInput queuing caused  by input buffer overflow.\nOutput port:  datagram can be lost due to congestion,lack of buffers(arrive rate fabric &gt; transmission rate).\nscheduling discipline: priority scheduling- network neutrality\n交换结构fabric 3种：memory, bus,interconnection network\n交换结构将路由器的输入端口连接到它的输出端口。这种交换结构包含在路由之中，是一个路由器中的\"网络\"\n输出端口\n\n输出端口存储从交换结构接收到的分组，并在输出链路上传输这些分组。当一条链路是双向的时，输出端口常与该链路输入端口成对出现在同一线路卡上。\n\n缓冲区：当交换结构速率较快时，就需要缓存，否则，在遇到拥塞时，会导致分组丢失\n分组调度：先来先服务（FCFS）规则，加权公平排队规则（WFQ），遵循“网络中立”原则，为服务质量保障（QoS）起到关键作用\n\n数据平面、控制平面\ncontrol plane methods:\n1 traditional router algorithm\n2 software-defined networking (SDN): 在远程的服务器中实现\n（Compare to the traditional routing algorithm，SDN is much more flexible.\nrouter structure:\nipv6 IP addr 128bits 16bytes\n\n实际中的路由器的Router input ports和 output ports 是同一个端口，\n只是为了方便输入输出端口的讲解才分成两个“独立的输入输出端口”\n\nlongest prefix matching\nTCAM with high speed\nscheduling\nPriority scheduling\n\n\nRound Robin (RR) scheduling（本意环形丝带，其实叫轮询调度）:\nWeighted Fair Queuing (WFQ，加权公平队列):\n\n\nFIFO scheduling\n\n\nIP\n\n局域网接入Internet的技术有三种https://blog.csdn.net/wenqiang1208/article/details/72403785：\n\n\n直接路由\n\n\n代理服务器(proxy)代理服务器（Proxy Server）是一种重要的服务器安全功能，它的工作主要在(OSI)模型的会话层，从而起到防火墙的作用。代理服务器大多被用来连接INTERNET（国际互联网）和Local Area Network（局域网）。\n代理分类：HTTP代理；socks代理；VPN代理；FTP代理\n\n\n网络地址转换(NAT)NAT将自动修改IP报文的源IP地址和目的IP地址，Ip地址校验则在NAT处理过程中自动完成。有些应用程序将源IP地址嵌入到IP报文的数据部分中，所以还需要同时对报文的数据部分进行修改，以匹配IP头中已经修改过的源IP地址。否则，在报文数据部分嵌入IP地址的应用程序就不能正常工作。\n\n\nIP 分片和重组\n(Fragmentation &amp; Reassembly)\n 网络链路有MTU(最大传输单元) – 链路层帧所携带的最大数据长度 不同的链路类型 不同的MTU\n如果IP层有一个数据报要传，而且数据帧的长度比链路层的MTU还大，\n那么IP层就需要进行分片( fragmentation)，即把数据报分成若干片，这样每一片就都小于MTU。\n 最后一个分片标记为0 “重组”只在最终的目标主机进行 IP头部的信息被用于标识，排序相关分片\nIP（专用）地址 只在局部网络中有意义，区分不同的设备\n 路由器不对目标地址是专用地址的分组进行转发\n 专用地址范围（A、B、C类都有）不用向isp申请\n\n Class A 10.0.0.0-10.255.255.255 MASK 255.0.0.0\n Class B 172.16.0.0-172.31.255.255 MASK 255.255.0.0\n Class C 192.168.0.0-192.168.255.255 MASK 255.255.255.0\n分类寻址 (Classful Addressing)：将 IP 地址分为 ABCDE 五类，三个主要类为 ABC 三类，区别在于网络号和主机号的长度，网络号和主机号的划分线在八位字节边界，如 C 类地址前 24 bits 为网络号，后 8 bits 为主机号。\n子网分类寻址 (Subnetted Classful Addressing)：在子网寻址系统中，通过从主机号中拿取前端部分 bits 作为子网号，用于识别子网，这将原本的两层划分系统（网络号/主机号）被改为三层划分系统（网络号/子网号/主机号）。\n无类寻址 (Classless Addressing)：将原始的分类寻址抛开，网络号和主机号可以在任意点划分，不需要像分类寻之中那样划分在八位字节边界。这种方案更加灵活有弹性。\n编码CIDR\nCIDR: Classless InterDomain Routing（无类域间路由）\n\n网络前缀越短，\n其地址块所包含的地址数就越多。\n而在三级结构的IP地址中，划分子网是使网络前缀变长。\n\nVLSM(可变长子网掩码) 是为了有效的使用无类别域间路由（CIDR）和路由汇聚(route summary)来控制路由表的大小，它是网络管理员常用的IP寻址技术\nCIDR无类网络是一种相对于有类网络的网络，无类网络IP地址的掩码是变长的。\n在有类网络的基础上，拿出一部分主机ID作为子网ID。\n例如：\nIP地址为192.168.250.44 子网掩码不能是小于24位。\n因为这是一个C类地址（前3Bytes是网络号），子网掩码只能大于24位。\n而掩码255.255.248.0（21位）是不符合规定的。\n·\n如果一个网络中的主机有100台，\n那么，可以用子网掩码/25来划分这个C类网络（“192.168.250.0/24”）：\n划分成192.168.250.0/25 和192.168.250.128/25两个子网。\n—主机192.168.250.44/25 属于子网192.168.250.0/25。\nsubnet\n子网掩码不能单独存在，它必须结合IP地址一起使用。\n子网掩码只有一个作用，就是将某个IP地址划分成网络地址和主机地址两部分。\nA、B、C类的地址叫做单播地址（unicast）——“我到你”。而D 类地址叫组播地址（multicast）——“我到这个D类组的”…\n还以一种叫广播地址（broadcast）——“我到所有”，一般在局域网内部\nE 类地址预留的（Reserved for future use） （但存方寸地,留与子孙耕）\n在路由信息的通告的过程当中，还不断地做路由聚集。减少表项，路由减负。\n路由聚集的作用：\n\nDHCP\n第一步：Client通过广播（broadcast）发送DHCP Discover 报文，Look for服务器端\n第二步：Server通过单播（unicast）发送DHCP Offer 报文向客户端提供IP地址等网络信息\n第三步：Client通过广播DHCP Request 报文告知服务器端本地选择使用哪个IP地址\n第四步：Server通过DHCP Ack报文告知客户端IP地址是合法可用的\nDHCP返回：ip地址、网关（router、子网掩码、DNS\nNAT\nnetwork address translation\n动机（使用NAT的原因）: 本地网络只有一个有效IP地址:\n不需要从ISP分配一块地址，可用一个IP地址用于所有的（局域网）设备 –省钱\n可以在局域网改变设备的地址情况下而无须通知外界\n可以改变ISP（地址变化）而不需要改变内部的设备地址\n局域网内部的设备没有明确的地址，对外是不可见的 –安全\n\n如果流量从Inside端口进来，那么先执行路由，后执行NAT(本地 到 全局)。\n如果流量从Outside端口进来，那么先执行NAT(全局 到 本地)，后执行路由。\n\nData plane\na set of match-action rules send by a controller.\ntraditional: longest prefix match forwarding vs. SDN allows more flexibility\nopenFlow abstraction: match+action\n\n\nMatch，匹配数据包头部\n\n\nAction，根据匹配的结果，做相应的操作，比如修改MAC地址，ACL之类\n\n\nOpenFlow能够启动远程的控制器，经由网络交换器，决定网络数据包要由何种路径通过网络交换机。这个协议的发明者，将它当成软件定义网络（Software-defined networking）的启动器。 [1] OpenFlow允许从远程控制网络交换器的数据包转送表，透过新增、修改与移除数据包控制规则与行动，来改变数据包转送的路径。比起用访问控制表(ACLs) 和路由协议，允许更复杂的流量管理。同时，OpenFlow允许不同供应商用一个简单，开源的协议去远程管理交换机（通常提供专有的接口和描述语言）。\nOpenFlow协议用来描述控制器和交换机之间交互所用信息的标准，以及控制器和交换机的接口标准。协议的核心部分是用于OpenFlow协议信息结构的集合。OpenFlow协议支持三种信息类型：Controller-to-Switch，Asynchronous和Symmetric，每一个类型都有多个子类型。Controller-to-Switch信息由控制器发起并且直接用于检测交换机的状态。Asynchronous信息由交换机发起并通常用于更新控制器的网络事件和改变交换机的状态。Symmetric信息可以在没有请求的情况下由控制器或交换机发起。\n\n\nControl plane\nwhy logically centralized control plane?\n控制平面在物理上与转发平面分离，控制软件使用开放接口（例如OpenFlow）对转发平面（例如，交换机和路由器）进行编程。\n\ndata plane:  mainly forwarding . each router contains a flow table by a centralized routing controller\n\n\n\nrouting protocol\nlink state\n\n​                         贪心算法        global: all routers have complete topology\n\nDijkstra算法\ncondition：any 路径&gt;0  &amp; 单源 已知出发点\n/临近，标记！\n直到全部点标记结束，回溯找出路径\ndistance vector\n\n\n距离矢量算法(distance vector routing)                 动态规划  decentralized\t\n  每个路由器维护一张表（即一个矢量），表中列出了当前已知的到每个目标的最佳距离，以及所用的链路。这些表通过邻居之间交换信息而不断被更新，最终每个路由器都了解到达每个目的地的最佳链路。也叫分布式Bellman-Ford路由算法\n可以参考：距离矢量算法简介 和 距离矢量算法示例\n这个算法存在一个严重的缺陷：虽然它总是能够收敛到正确的答案，但速度可能非常慢。尤其是，它对于好消息的反应非常迅速，而对于坏消息的反应异常迟钝。这个算法存在无穷计数的问题。可扩展性很差，越大的网络收敛的越慢。而且，会占用比较大的网络带宽。\n\n\n相比于距离矢量算法，链路状态路由算法需要更多的内存和计算，在大型网络中运行这个算法依然是个问题。不过，在许多现实场合，链路状态路由算法工作得非常好，因为它没有慢收敛得问题。\nOSPF和IS-IS的出现，许多人认为RIP（distance vector）已经过时了。但事实上RIP也有它自己的优点。对于小型网络，RIP就所占带宽而言开销小，易于配置、管理和实现，并且RIP还在大量使用中。链路状态路由算法被广泛应用于实际网络中。IS-IS，Intermediate System-Intermediate System，被很多的ISP使用。后来它被ISO采纳用于OSI协议；然后它被修改多次以便能够处理多种协议，比如IP协议。OSPF，Open Shortest Path First，是另一个主流的链路状态协议。\nAS\nIntra-AS routing:         aka IGP\nOSPF(Open Shortest Path First开放式最短路径优先）是一个内部网关协议(Interior Gateway Protocol，简称IGP），用于在单一自治系统（autonomous system,AS）内决策路由。是对链路状态路由协议的一种实现，隶属内部网关协议（IGP），故运作于自治系统内部。著名的迪克斯彻（Dijkstra）算法被用来计算最短路径树。OSPF支持负载均衡和基于服务类型的选路，也支持多种路由形式，如特定主机路由和子网路由等\ninter-AS routing:\njob: learn which dests are reachable in other inter-AS system系统间传播(eBGP) ,\nand propagate the reachability info to all routers in AS1.内(iBGP)\nintra-AS: IGP             CONTAIN: RIP OSPF IGRP\nBGP inter domain routing protocol\n网关就是路由器的IP\nSDN control plane\nlogically centralized : easy management:avoid router mis configurations\nICMP\nICMP enables a router or destination host to communicate with a source host, for example,to report an error in datagram processing. (2 marks)Path MTU discovery uses the Type-3 Destination Unreachable ICMP messages as follows: TCP negotiates initial MTU size – usually TCP/IP sends datagram with Don’t Fragment flag set If datagram is too large an ICMP Destination Unreachable is received Reduce Maximum Transfer Unit.\nInternet控制报文协议。它是TCP/IP协议簇的一个子协议，用于在IP主机、路由器之间传递控制消息。控制消息是指网络通不通、主机是否可达、路由是否可用等网络本身的消息。这些控制消息虽然并不传输用户数据，但是对于用户数据的传递起着重要的作用。ICMP使用IP的基本支持，就像它是一个更高级别的协议，但是，ICMP in IP datagram.\nabove “IP” error reporting and echo request/reply\ntraceroute (Windows 系统下是tracert) 命令利用ICMP 协议定位您的计算机和目标计算机之间的所有路由器。TTL 值可以反映数据包经过的路由器或网关的数量，通过操纵独立ICMP 呼叫报文的TTL 值和观察该报文被抛弃的返回信息，traceroute命令能够遍历到数据包传输路径上的所有路由器。\nDatalink layer\nService\n组帧framing,链路接入 link access\n封装数据报构成数据帧，加首部和尾部帧同步\n如果是共享介质，需要解决信道接入\n帧首部的“MAC”地址，用于标识帧的源和目的\n\n不同于IP地址\n\n相邻结点间可靠交付\n\n在低误码率的有线链路很少采用\n\n流量控制\n\n协调相邻的发送结点和接收\n\n差错纠正\n\n接收端直接纠正比特差错\n\n全双工和半双工通信控制\n\n全双工：链路两端结点同时双向传输\n半双工：链路两端结点交替双向传输\n\nMAC地址（英语：Media Access Control Address），直译为媒体存取控制位址，也称为局域网地址（LAN Address），MAC位址，以太网地址（Ethernet Address）或物理地址（Physical Address），它是一个用来确认网络设备位置的位址。在OSI模型中，第三层网络层负责IP地址，第二层数据链路层则负责MAC位址 [1] 。MAC地址用于在网络中唯一标示一个网卡，一台设备若有一或多个网卡，则每个网卡都需要并会有一个唯一的MAC地址\n循环冗余检验CRC(Cyclic Redundancy Check)\nMultiple access protocol\n多路访问控制协议\n冲突(collision)：结点同时接收到两个或者多个信号→接收失败！\nMAC协议采用分布式算法决定结点如何共享信道，即决策结点何时可以传输数据。\n其必须基于信道本身，通信信道共享协调信息。无带外信道用于协调。\n\n信道划分(channel partitioning)MAC协议\n\nTDMA: time division multiple access\nTDM 将时间划分为时间帧(timeframe),并进一步划分每个时间帧为N个时隙(slot)\n每个站点在每个时间帧，占用固定长度的时隙(长度=分组传输时间)；未用时隙空闲(idle)\n如图：6站点LAN，134传输分组，256空闲\n\nFDMA: frequency division multiple access\n信道频谱划分为若干频带(frequency bands)\n每个站点分配一个固定的频带，不会冲突但信道利用率可能不高；无传输频带空闲\n如图：6站点LAN, 134频带传输数据，256频带空闲。\n\nRandom access protocol\n包括 ALOHA,CSMA,CSMA/CA,CSMA/CD             (transmit entire frame)\nSpecify 如何检测collision 如何恢复\nslotted ALOHA\n结点只能在时隙开始时刻发送帧，当结点有新的帧时在下一个时隙(slot)发送如果2个或2个以上结点在同一时隙发送帧，结点即检测到冲突.如果无冲突：该结点可以在下一个时隙继续发送新的帧.  如果冲突：该结点在下一个时隙以概率p重传该帧，直至成功.Slotted aloha reduces the number of collisions to half and doubles the efficiency of pure aloha.\n** pure ALOHA**\nno synchronization(CLOCK) but only half as efficient as slooted\n载波侦听多路访问协议 CSMA(carrier sense multiple access)协议\nListen before transmit\nCSMA/CD\neasy in wired LAN (ethernet)     reduce channel wastage, improve effiency\n避免冲突的载波侦听多路访问协议  CSMA/CA(CSMA with Collision Avoidance)协议\n802.11无线局域网中，不能像CSMA/CD那样，边发送、边检测冲突，原因为：\n（1）检测碰撞的能力要求站点具有同时发送（站点自己的信号）和接收（检测其他站点是否也在发送）的能力。因为在802. 11适配器上，接收信号的强度通常远远小于发送信号的强度，制造具有检测碰撞能力的硬件代价较大。\n802.11发送端：\n（1）如果监听到信道空闲了分布式帧间间隔DIFS后，则在发送整个帧（发送的同时不检测冲突）\n发送端首先利用CSMA向BS发送一个很短的请求发送（request-to-send，RTS）控制帧预约信道，而不是随机发送数据帧，利用小预约帧避免长数据帧的冲突。RTS帧仍然可能彼此冲突（但RTS帧很短）\nAP广播一个很短的允许发送（clear-to-send，CTS）控制帧作为对RTS的响应，CTS帧可以被所有结点接收，以消除隐藏站影响\n发送端可以发送数据帧，其他结点推迟发送\n（2）如果监听到信道忙，则选取随机回退值\n​       当信道空闲时，计时器倒计时;       当计时器超时时，发送帧\n（3）如果没有收到ACK，则增加随机退避间隔时间，重复（2）\n802.11接收端：\n如果正确接收帧，则在延迟短帧间间隔SIFS后，向发送端发送ACK（由于存在隐藏站问题）\nARP\nMAC(LAN,or physical or ethernet) address and ARP\n48 MAC address in NIC ROM\nMAC flat address portable (IP hierarchical not portable)\n地址解析协议，即ARP（Address Resolution Protocol），是根据IP地址获取物理地址的一个TCP/IP协议。主机发送信息时将包含目标IP地址的ARP请求广播到局域网络上的所有主机，并接收返回消息，以此确定目标的物理地址.  PLUG and PLAY\n802.11 all use CSMA/CA\nLAN\n\nswitch\nself learning plug and play\nframe forwarding, storing ; don’t need to be configured\nVLAN\n建立逻辑上独立的虚拟网络：于是交换机可以很方便实现虚拟局域网\n将连在Switch上的站点更具自己的喜好划分逻辑组：且与物理位置无关\nVLAN隔离广播域 ：于是也可以对网络安全隔离、控制广播风暴\n一般接终端设备使用access port 需要多VLAN透传的使用trunk port\nMPLS\n是一种在开放的通信网上利用标签引导数据高速、高效传输的新技术。多协议的含义是指MPLS不但可以支持多种网络层层面上的协\ngoal to fast lookup and borrow ideas from VC\nsecurity management\nFIrewall\nstateless pkt filter, application gateways\nstateful contains stateless filter, which is the most flexible but cost more.\n","tags":["计算机网络"]}]